const e="一流的嵌入、重新排序器、网络爬虫、深度搜索、小型 LM。用于多语言和多模式数据的搜索 AI。",n="您的搜索，如虎添翼！",a={approach:"我们的策略",approach_connect_dots:"绘图连线：从高级用户至企业用户",approach_connect_dots_description:"为何我们如此重视高级用户在整体策略中的地位？这是一个提前布局的考量。今日之投资，为了明日他们在企业中的影响力。这是我们的远见卓识，确保当这些高级用户掌握企业的话语权时，我们仍为他们心之所向。",approach_content1:"在人工智能的风云变幻中，策略必须灵活应变，洞察未来。尽管 Jina AI 以企业为核心，但AI的舞台已演变，吸引客户的策略亦当与时俱进。因此，以高级用户为突破口，不仅是我们的策略创新，更体现了我们对企业发展的坚韧不拔。",approach_content2:"在 Jina AI，我们不甘于守成，而是勇于开创。以高级用户为先导，确保我们把握当下，预见未来。对于企业，我们的决心坚如磐石；但在通向成功的路上，我们走的是一条既稳健又前瞻的新路。",approach_content4:'每个人都希望获得更好的搜索体验。在 Jina AI，我们通过提供 <span class="text-primary text-bold">搜索底座</span>（由向量模型、重排器、读取器 和 提示词工程组成）来实现更好的搜索体验。这些组件协同工作，彻底改变了我们搜索和理解数据的方式。',approach_miss_mark:"传统MLOps的短板",approach_miss_mark_description:"尽管高级用户日益壮大，但传统的MLOps工具往往步履蹒跚，难以满足他们的需求。它们宛如过时的老马，难以应对新时代的快节奏竞赛。新一代的开发者饥渴地寻求更为轻巧、直观的武器。",approach_new_paradigm:"提示词工程：AI开发之新潮",approach_new_paradigm_description:"2023年，AI世界迎来新篇章，提示词工程如日方升，实现了AI工具的大众化进程。即使是那些缺乏编程根基的高级用户，也能游刃有余地涉猎AI，无需再对如Pytorch、Docker或Kubernetes等深感畏难。此情此景，与个人计算的崛起历程颇为相似。当年，仅有的技术精英才可与计算机沟通，但随着更为亲和的界面的问世，大众也得以加入其行列。今日，随着提示词工程的普及，我们正在目睹AI领域的新一轮普及浪潮。",awards:"奖项与殊荣",berlin:"德国柏林（总部）",berlin_address:"Prinzessinnenstraße 19-20，10969 柏林，德国",berlin_address2:"工商注册地址: Leipzigerstr. 96, 10117 柏林, 德国",bj:"中国北京",bj_address:"中国北京市海淀区西大街48号6号楼5层",brochure_info:"我们公司的指南等待您的光临",description:"未来从这里开始。",download_brochure1:"下载手册",download_docarray_logo:"下载 DocArray的Logo",download_docarray_logo_desc:"获取 DocArray 徽标，这是一个由 Jina AI 发起的开源项目，并于 2022 年 12 月捐赠给了 Linux 基金会。提供浅色和深色模式、PNG 和 SVG 格式。",download_jina_logo:"下载Jina AI的Logo",download_jina_logo_desc:"获取浅色和深色模式下的 Jina AI 徽标，提供 PNG 和 SVG 格式。该标志是欧盟知识产权局（EUIPO）的注册商标。",download_logo:"下载Logo",employees:"当今员工",empower_developers:"开发者生态",fastApiCaption:"自 2021 年以来捐款超过 20,000 美元。",founded:"成立年份",founded_in:"成立于",investors:"我们的投资方",linuxFoundationCaption:"从 2022 年开始，每年捐款 10,000 美元。",many:"许多",media:{video:"视频采访"},mission:"我们的使命",mission_content1:"我们的关键技术包括快速调优、快速服务、模型调优和模型服务，体现了我们致力于使人工智能普及的承诺。通过我们的开源计划，我们努力促进创新、协作和透明度，确保提供可扩展、高效和强大的解决方案。Jina AI 不仅仅是一家公司；它是一个致力于帮助企业应对数字时代的动态挑战并在其领域蓬勃发展的社区。",mission_content2:"Jina AI 的核心理念在于我们的使命：成为从高级用户、开发者至各大企业的多模态 AI 通道。我们深信开源的魔力，并致力于为 AI 社区构建前沿且易于接入的工具。我们的关键技术，诸如提示词微调、模型向量调优部署等，展现了我们对 AI 普及的坚定信仰。借助我们的开源方案，我们旨在推动创新、合作与透明度，确保我们的方案既可扩展、高效又强大。Jina AI 不只是一个公司，它更是一个助企业应对数字化时代挑战并在其领域中持续领先的共同体。",mission_content3:"在 Jina AI，我们的使命是通过创新的向量模型和基于提示的技术，引领多模态AI的发展。我们特别关注自然语言处理、图片与视频分析以及跨模态数据交互等领域。我们的专业化致力于提供独特的解决方案，能够将复杂的多源数据转化为具有实际操作价值的洞察和创新应用。",mit_report_title:"多模态：人工智能的新前沿",mit_techreview:"麻省理工科技评论",numfocusCaption:"从 2022 年开始每月定期捐款。",office:"我们的办公室",otherProjectsCaption:"通过 Github 赞助捐赠了超过 3,000 美元。",our_answer:"不能同意更多，Yann。我们正在努力搭建通向多模态AI未来的桥梁！",pythonSoftwareFoundationCaption:"一次性捐赠 10,000 美元，并赞助了多项 PyCon 活动，包括德国、意大利、中国和美国的活动。",sectors:{ecommerceRetail:"下一代电商",ecommerceRetail_description:"电子商务和零售业领导者与 Jina AI 合作，提供精准的产品推荐和深入的搜索体验。我们的多语言向量和 AI 驱动的重新排序器有助于优化发现、提高转化率并缩短全球产品目录的洞察时间。",ecommerceRetail_short:"电商零售",financeConsulting:"咨询顾问和分析师",financeConsulting_description:"金融公司和咨询公司利用 Jina AI 的大规模数据清理和特定领域模型训练来获取实时洞察。我们的企业许可和安全的本地部署使他们能够保持机密性，同时受益于高级检索和分析。",financeConsulting_short:"咨询金融",media:"内容创作者",media_description:"媒体组织使用 Jina AI 将大量多媒体资产转化为可搜索的知识，简化内部研究并丰富用户体验。我们专业的读取器和重新排名服务可确保在文章、视频和档案中进行准确、情境感知的发现。",media_short:"媒体文创",misc:"弄潮儿",misc_description:"教育、农业、房地产等行业的组织利用 Jina AI 灵活的解决方案来大规模清理、提取和转换数据。通过利用我们尖端的神经检索堆栈，他们解锁了新的可能性并在各自的领域保持领先地位。",misc_short:"其他的",technology:"开拓创新的技术者",technology_description:"软件、云、AI 和数据领域的领导者依靠 Jina AI 的神经搜索解决方案来支持其基于大模型的搜索、RAG 和 AI 代理系统。我们的高级读取器、向量、重新排序器和小语言模型可帮助他们以前所未有的速度从试验品转向生产级。",technology_short:"软件科技"},sefo:{layer0:"面向用户的应用",layer1:"RAG/编排系统",layer3:"GPU/移动/边缘/本地计算"},segmentFaultCaption:"一次性捐款 6,000 美元。",show_position:"搜索底座在生态系统中的定位？",stats_1:"Jina AI 于 2020 年 2 月成立，短短 20 个月，已成为多模态AI技术的翘楚。这段时间里，我们成功筹集了 3750 万美元，确立了在 AI 行业的领军地位。在 GitHub 上，我们的革命性开源技术赋能超过 40,000 名开发者，使他们得以毫无障碍地构建与部署多模态应用。",stats_2:"到了 2023 年，我们在基于多模态技术的 AI 工具方面取得了跨越式的突破。这一创新已让超过 250,000 名用户受益，满足了各式各样的业务需求。无论是推动业务增长、提升运营效率还是优化成本，Jina AI 都致力于助力企业在多模态时代锐意进取。",stats_4:'Jina AI 成立于 2020 年，是一家领先的搜索 AI 公司。我们的 <span class="text-primary text-bold">搜索底座</span> 平台包含了向量模型、重排器和小语言模型，可帮助企业构建可靠且高质量的生成式AI和多模态的搜索应用。',stats_v1:"当搜索与加速主义碰撞",subtitle:"通过人工智能生成的解决方案彻底改变内容创建，释放无限可能性。塑造人工智能生成内容的未来并增强人类创造力。",sues_und_sauer:"酸甜苦辣",sues_und_sauer_tooltip:"Süß-Sauer 是德式中餐中一种流行的口味（地道的中餐里这种口味并不常见，属于德国人对中餐的刻板印象），意思是酸甜口。它隐喻着创业生涯的起起落落。",sunnyvale_address:"710 Lakeway Dr, Ste 200, 桑尼维尔, CA 94085, 美国",sz:"中国深圳",sz_address:"中国深圳市赋安科技大厦4楼402",team:"我们的团队",team_content1:"我们从全球各个角落构建 AI 的未来。我们独特的视角丰富了我们的工作，激发了创新。在这个门户网站中，我们拥抱个性，热情追求梦想。欢迎来到 AI 未来的门户网站。",team_join:"加入我们",team_size:"这些照片包括我们以前的同事和实习生——我们感谢他们每一个人。",technologies:"核心技术",title:"关于极纳科技",title0:"未来",title1:"起源",title2:"于此",title3:"始于斯",understand_our_strength:"了解我们的强项",understand_our_view2:"了解搜索底座",users:"注册用户",value:"我们的奖项",value_content1:"我们不会安于现状。我们不会妥协。我们追求卓越。",vision:"我们的任务",vision_content1:"启发于 Yann LeCun 对于AI的观点，“",vision_content3:'AI 的未来是<span class="text-primary text-bold">多模态</span>，而我们是其中的一部分。我们意识到企业在利用多模态数据方面面临挑战。为此，我们致力于<span class="text-primary text-bold">搜索底座</span>，以帮助企业和开发者更好地进行搜索，并利用多模态数据促进业务增长。',yannlecun_quote:"仅靠单词和句子训练的人工智能系统永远无法接近人类的理解力。"},i={answer1:"是的，同一个 API 密钥适用于 Jina AI 的所有搜索基础产品。这包括读取器、向量模型、重排器、分类器和微调模型 API，所有服务之间共享词元。",answer10:"这是因为我们的无服务器架构在使用率较低时会卸载某些模型。初始请求会激活或“预热”模型，这可能需要几秒钟。初始激活后，后续请求的处理速度会快得多。",answer12:"我们遵守严格的隐私政策，不会使用用户输入数据来训练我们的模型。我们还符合 SOC 2 类型 I 和类型 II 标准，确保高标准的安全性和隐私性。",answer3:"是的，您可以在“密钥和计费”选项卡中输入您的 API 密钥来查看词元最近的使用记录和剩余词元余额。如果您已登录 API 密钥控制面板，也可以在“管理 API 密钥”选项卡中查看这些详细信息。",answer4:"如果您遗失了充值密钥并希望找回，请使用您的注册电子邮件联系 support AT jina.ai 寻求帮助。建议登录以便于安全保存和便捷访问您的 API 密钥。",answer5:"不，我们的 API 密钥没有到期日期。但是，如果您怀疑您的密钥已被泄露并希望停用它，请联系我们的支持团队寻求帮助。您还可以在<a class='text-primary' href='https://jina.ai/api-dashboard'>API 密钥控制面板</a>中自助销毁您的密钥。",answer6:"是的，您可以将剩余的付费词元余额从一个高级密钥转移到另一个密钥。在<a class='text-primary' href='https://jina.ai/api-dashboard'>API 密钥控制面板</a>上登录您的帐户后，在该密钥的设置界面来转移所有剩余的付费词元余额。",answer7:"是的，如果您认为您的 API 密钥已被泄露，您可以销毁该密钥。销毁密钥将立即为所有存储该密钥的用户禁用该密钥，并且所有剩余词元余额和关联资产将永久不可用。如果您拥有高级密钥，您可以选择在销毁之前将剩余的已付款词元余额转移到另一个密钥。请注意，此操作无法撤消。要销毁密钥，请前往<a class='text-primary' href='https://jina.ai/api-dashboard'>API 密钥控制面板</a>中的密钥设置。",question1:"我可以对读取器、向量模型、重排器、分类器和微调模型 API 使用相同的 API 密钥吗？",question10:"为什么有些机型第一次请求比较慢？",question12:"用户输入数据是否用于训练您的模型？",question3:"我可以查看 API 密钥的词元使用情况吗？",question4:"如果我忘记了 API 密钥，该怎么办？",question5:"API 密钥会过期吗？",question6:"我可以在 API 密钥之间转移词元余额吗？",question7:"我可以销毁我的 API 密钥吗？",title:"API相关常见问题"},t={base_model:"微调底座模型",check_data:"下载合成数据",check_model:"下载微调模型",data_size:"生成合成数据",description:"获取您想要的任何域的微调向量模型。",description_long:"只需告诉我们您希望向量模型在哪个领域中表现出色，我们就会自动为该领域提供一个可立即使用的、经过微调的向量模型模型。",does_it_work_tho:"但它真的有效吗？",does_it_work_tho_explain:"自动微调具有神奇的自动效果，可以为您想要的任何域提供微调的向量。但它真的有效吗？这是一个相当合理的疑问。我们在各种领域和底座模型上对其进行了测试以找出答案。查看下面的精选和精选结果。",domain_instruction:"领域指令",embedding_provider:"选择基础向量模型",eval_evaluation:"验证",eval_map:"地图",eval_mrr:"月平均收入",eval_ndcg:"神经胶质细胞癌",eval_performance_before_after:"微调前后合成验证集上的表现",eval_syntheticDataSize:"全部的",eval_test:"真实数据测试",eval_training:"训练",faq_v1:{answer1:"此功能目前处于测试阶段，每个微调模型需要花费 100 万个词元。如果 Embedding/Reranker API 中有足够的词元，您可以使用现有的 API 密钥，也可以创建一个新的 API 密钥，其中包含 100 万个免费词元。",answer10:"目前没有。请注意，此功能仍处于测试阶段。将微调后的模型和合成数据公开存储在 Hugging Face 模型中心有助于我们和社区评估训练的质量。未来，我们计划提供私人存储选项。",answer11:"由于所有微调模型都已上传至 Hugging Face，因此您只需指定模型名称即可通过 SentenceTransformers 访问它们。",answer12:"请检查您的垃圾邮件文件夹。如果仍然找不到，请使用您提供的电子邮件地址联系我们的支持团队。",answer2:"您无需提供任何训练数据。只需用自然语言描述您的目标域（您希望优化微调向量模型的域），或使用 URL 作为参考，我们的系统就会生成合成数据来训练模型。",answer3:"大约 30 分钟。",answer4:"经过微调的模型和合成数据公开存储在 Hugging Face 模型中心。",answer5:"系统使用 Reader API 从 URL 中获取内容。然后分析内容以总结语气和领域，并以此作为生成合成数据的指导方针。因此，URL 应该是公开可访问的，并且代表目标域。",answer6:"是的，您可以针对非英语语言微调模型。系统会自动检测域指令的语言并相应地生成合成数据。我们还建议为目标语言选择合适的底座模型。例如，如果针对德语域，则应选择“jina-embeddings-v2-base-de”作为底座模型。",answer7:"不，我们的微调 API 仅支持 Jina v2 模型。",answer8:"在微调过程结束时，系统会使用保留的测试集评估模型并报告性能指标。您将收到一封电子邮件，详细说明此测试集的前后性能。我们还鼓励您在自己的测试集上评估模型以确保其质量。",answer9:"该系统通过将您提供的目标域指令与大模型智能体的推理相结合来生成合成数据。它会产生具有挑战的三元组，这对于训练高质量的向量模型模型至关重要。有关更多详细信息，请参阅我们即将在 Arxiv 上发表的研究论文。",question1:"微调 API 的费用是多少？",question10:"我可以对我的微调模型和合成数据保持私密吗？",question11:"如何使用微调模型？",question12:"我从未收到包含评估结果的电子邮件。我该怎么办？",question2:"我需要输入什么？我需要提供训练数据吗？",question3:"微调一个模型需要多长时间？",question4:"微调后的模型存储在哪里？",question5:"如果我提供一个参考 URL，系统将如何使用它？",question6:"我可以针对特定语言微调模型吗？",question7:"我可以微调非 Jina 向量模型吗，例如 bge-M3？",question8:"如何保证微调模型的质量？",question9:"如何生成合成数据？",title:"自微调相关常见问题"},find_on_hf:"列出微调模型",temporarily_unavailable:"暂时不可用。我们正在升级自动微调系统，以便为您提供更好的服务。请稍后再查看。",test_on:"已对来自 {_dataName} 的 {_dataSize} 个随机样本进行测试",test_performance_before_after:"微调前后在保留测试集上的表现",title:"自微调 API",total_improve:"平均改善",usage:"用法",what_is:"什么是自微调？",what_is_answer_long:"微调允许您采用预先训练的模型，并通过在新的数据集上进行训练来使其适应特定任务或领域。实际上，对于许多用户来说，找到有效的训练数据并不是一件容易的事。有效的训练不仅仅需要将原始 PDF、HTML 放入模型中；而且很难做到正确。自微调通过使用高级大模型智能体管道自动生成有效的训练数据来解决此问题；并在 ML 工作流中微调模型。您可以将其视为合成数据生成和 AutoML 的组合，因此您需要做的就是用自然语言描述您的目标域，然后让我们的系统完成剩下的工作。"},r={auth_required:"使用头像生成需要身份验证",classificationError:"对图片进行分类时出错。请重试。",clickToDownload:"点击下载 SVG",customize:"自定义功能",description:"生成具有可定制功能的独特头像",downloadError:"下载头像时出错",downloadSuccess:"头像下载成功",download_success:"头像下载成功",error_loading:"无法加载头像资源，请重试。",error_processing:"处理图片时出错",file_hint:"支持的格式：JPG、PNG、GIF、WebP",generate:"生成头像",how_does_it_work:"它是如何工作的？",noImageSelected:"请先选择图片",select_file:"选择肖像图片文件",title:"头像生成器",upload_description:"选择要转换为 base64 的图片（256x256）",upload_title:"上传图片",usage:"头像生成"},o={description:"从文章直接生成插图，无需提示词",example_description:"爱丽丝开始厌倦了坐在岸边姐姐身边，无所事事：有一两次她偷看了姐姐正在读的书，但里面没有图片或对话，“书有什么用，”爱丽丝想，“没有图片或对话？”于是，她正在心里思考（尽她所能，因为炎热的天气让她感到非常困倦和愚蠢），制作菊花链的乐趣是否值得费力起身去采摘雏菊，突然一只粉红色眼睛的白兔跑到她身边。",example_title:"爱丽丝梦游仙境 - 第 1 章"},s="试用版",_={answer10:"我们为新用户提供免费试用，其中包括 100 万个词元，可通过自动生成的 API 密钥与我们的任何模型一起使用。一旦达到免费词元限制，用户可以通过“购买词元”选项卡轻松为其 API 密钥购买额外的词元。",answer13:"不，失败的请求不会扣除词元。",answer14:"付款通过 Stripe 处理，支持多种付款方式，包括信用卡、Google Pay 和 PayPal，为您提供方便。",answer15:"是的，购买词元后，发票将发送到与您的 Stripe 帐户关联的电子邮件地址。",answer9:"我们的定价模型基于处理的词元总数，允许用户灵活地在任意数量的句子中分配这些词元，为不同的文本分析需求提供经济高效的解决方案。",question10:"新用户可以免费试用吗？",question13:"失败的请求是否会扣除词元？",question14:"接受哪些付款方式？",question15:"词元购买后可以开具发票吗？",question9:"API是根据句子的数量或请求的数量计费吗？",title:"与计费相关的常见问题"},d={all:"全部",events:"活动",featured:"甄选",insights:"观点","knowledge-base":"知识库",latest:"最新的",press:"新闻稿",releases:"软件更新","tech-blog":"技术文章"},l={caption:"探索《Re·Search》，由我们精心设计的年鉴，收录了我们2024年最具影响力的研究和搜索底座模型。",order_now:"立即订购"},c={api_free_trial:"免费 API 密钥",api_paid:"付费 API 密钥",api_paid_or_free:"您使用的是付费 API 密钥还是免费试用密钥？",are_you:"你是：",commercial_contact_sales:"此为商业性质。请联系我们的销售团队。",contact_sales_for_licensing:"联系我们的销售团队获取许可。",csp_user:"您是否在 AWS 和 Azure 上使用我们的官方模型？",educational_teaching:"教育机构用它来教学吗？",for_profit_internal_use:"盈利性公司在内部使用它吗？",free_use:"您可以自由使用这些模型。",government_public_services:"政府实体使用它来提供公共服务？",is_use_commercial:"您的用途是商业用途吗？",may_be_commercial_contact:"这可能是商业用途。请联系我们进行澄清。",no:"不",no1:"不",no2:"不",no3:"不",no_restrictions:"无限制。请按照当前协议使用。",no_restrictions_apply:"沒有限制。",non_commercial_free_use:"本模型非商业性质，您可以自由使用。",non_profit_ngo_mission:"非营利组织或非政府组织是否利用它来完成你的使命？",not_sure:"没有把握",personal_hobby_projects:"将其用于个人项目或者业余爱好项目？",product_service_sale:"在您销售的产品或服务中使用它吗？",title:"CC BY-NC 许可证自检",trial_key_restrictions:"免费试用密钥仅可用于非商业用途。如需商业用途，请购买付费套餐。",typically_non_commercial_check:"这通常是非商业性的，但如果不确定，请与我们联系。",typically_non_commercial_free_use:"这通常是非商业性的。您可以自由使用这些模型。",using_api_or_cloud:"您是否使用我们的官方 API 或在 Azure 或 AWS 上我们的官方镜像？",using_cc_by_nc_models:"你正在使用这些模型吗？",yes:"是的",yes1:"是的",yes2:"是的",yes3:"是的"},p={access:"公共访问",access_explain:"任何拥有 <code>classifier_id</code> 的人都可以使用公共分类器，并且它们的使用将消耗调用者的词元配额，而不是您的。私有分类器只能由您访问。",access_private:"私人的",access_public:"民众",api_delete:"删除分类器",api_delete_explain:"根据分类器ID删除分类器。",api_list:"列表分类器",api_list_explain:"列出您已创建的所有分类器。",classifier_id:"分类器 ID",classify_inputs:"要分类的输入",classify_inputs_explain:"对于文本，它可以是最多 8192 个词元的句子。对于图片，它可以是 URL 或 base64 编码的图片。",classify_labels:"候选标注",classify_labels_explain:"输入将被归类到这些类别中。最多可以有 256 个类别。使用带语义类别可获得更好的性能。",compare_table:{access_control:"访问控制",classifier_id_required:"必需分类器 ID",continuous_updates:"持续模型更新",default_solution:"一般分类问题的默认解决方案",feature:"特点",few_shot:"少量样本",image_multi_lingual_support:"多模态和多语言支持",labels_required_classify:"/classify 中需要的标注",labels_required_train:"/train 中需要的标注",max_classes:"最大类数",max_classifiers:"最大分类器",max_inputs_request:"每个请求的最大输入",max_token_length:"每个输入的最大词元长度",na:"不适用",no:"不",out_of_domain_solution:"对于 v3/clip-v1 域之外的数据或时间敏感的数据",primary_use_case:"主要用例",semantic_labels_required:"需要语义标注",state_management:"状态管理",stateful:"有状态",stateless:"无状态",token_count:"{count} 个词元",training_data_required:"需要训练数据",yes:"是的",zero_shot:"零样本"},create_classifier:"新的少样本分类器",create_classifier_explain:"创建一个新的小样本分类器并使用训练样本对其进行训练。",description:"图片和文本的零样本和少样本分类。",description_long:"试用我们的 API 沙盒实验场来了解我们的分类器如何工作。",description_long1:"针对多模态和多语言数据的高性能零样本和少样本分类器。",explain:"分类器是一种 API 服务，它使用向量模型 (<code>jina-embeddings-v3</code> 和 <code>jina-clip-v1</code>) 对文本和图片进行分类，支持无需训练数据的零样本分类和使用最少示例的少样本学习。",faq_v1:{answer1:"零样本分类需要语义标签，训练时不需要，而少样本分类则需要训练时需要标签，但分类时不需要。这意味着零样本分类更适合灵活、即时的分类需求，而少样本分类更适合固定的、特定领域的类别，这些类别可以随时间而演变。",answer10:"是的，您可以选择 <code>jina-embeddings-v3</code> 进行文本分类（特别适合多语言）和 <code>jina-clip-v1</code> 进行多模态分类。新模型（如 <code>jina-clip-v2</code>）将在发布时通过 API 自动提供。",answer2:"<code>num_iters</code> 控制训练强度 - 较高的值会强化重要示例，而较低的值会最大限度地减少不太可靠数据的影响。它可用于通过为近期示例提供更高的迭代次数来实现时间感知学习，这使其对于不断发展的数据模式很有价值。",answer3:"任何拥有 <code>classifier_id</code> 的人都可以使用公共分类器，并消耗自己的词元配额。用户无法访问训练数据或配置，也无法查看其他人的分类请求，从而实现安全的分类器共享。",answer4:"少量样本需要 200-400 个训练样本才能超越零样本分类。虽然它最终会实现更高的准确率，但需要这段预热期才能发挥作用。零样本无需训练数据即可立即提供一致的性能。",answer5:"是的 - API 支持使用 <code>jina-embeddings-v3</code> 进行多语言查询和使用 <code>jina-clip-v1</code> 进行多模态（文本/图片）分类，并支持在同一请求中对 URL 或 base64 编码的图片进行支持。",answer6:"Zero-shot 支持 256 个类别，没有分类器限制，而 few-shot 则限制为 16 个类别和 16 个分类器。两者均支持每个请求 1,024 个输入和每个输入 8,192 个词元。",answer7:"少量样本模式允许通过 <code>/train</code> 端点进行持续更新，以适应不断变化的数据模式。当数据分布发生变化时，您可以逐步添加新的示例或类别，而无需重建整个分类器。",answer8:"该 API 使用一次性在线学习 - 训练示例会更新分类器权重，但之后不会存储。这意味着您无法检索历史训练数据，但它可以确保隐私和资源效率。",answer9:"当您需要使用语义标签进行灵活分类时，请从零样本开始，以获得即时结果。当您有 200-400 个示例、需要更高的准确度或需要处理特定领域/时间敏感的数据时，请切换到少样本。",question1:"零样本和小样本的标签有何不同？",question10:"我可以针对不同的语言/任务使用不同的模型吗？",question2:"num_iters 有什么用处以及如何使用它？",question3:"公共分类器共享如何工作？",question4:"我需要多少数据才能使小样本研究发挥良好作用？",question5:"它能处理多种语言和文本/图片吗？",question6:"我应该了解哪些硬性限制？",question7:"我该如何处理随时间而发生的数据变化？",question8:"我发送训练数据后会发生什么情况？",question9:"零样本与小样本——何时使用哪个？",title:"分类器相关常见问题"},more:"更多",num_iters:"训练迭代",num_iters_explain:"控制训练强度 - 较高的值可提高当前示例的准确性，但会增加词元成本。默认值 10 通常效果很好。",read_notes:"读取发行说明",select_classifier_or_model:"选择分类器或向量模型",task_classify:"分类",task_classify_explain:"使用零样本或少样本分类器将文本或图片分类到定义的类别中。",task_manage:"管理",task_manage_explain:"列出或删除你的小样本分类器。",task_select:"选择任务",task_train:"训练",task_train_explain:"使用训练样本创建或更新少量分类器。",title:"分类器 API",train_inputs:"训练数据",train_inputs_explain:"用于训练的带有标签的文本或图片示例。您可以随着时间的推移使用新示例和标签逐步更新分类器。",train_label:"标签",what_is:"什么是分类器？",when_to_use_what:"何时使用零样本或小样本？",when_to_use_what_explain:"使用零样本分类作为默认解决方案，可以在最多 256 个类别的一般分类任务中获得即时结果，而少样本学习更适合处理向量模型知识之外的特定领域数据，或者需要处理需要持续模型更新的时间敏感数据。"},u={description:"使用 CLIP 将图片和文本向量到固定长度的向量中"},m={description:"多模态AI应用的云托管平台"},g={agreement:"提交即表示您确认同意 Jina AI 按照以下规定处理您的个人数据",anything_else:"告诉我们更多关于你的想法",cc_by_nc:"请求 CC BY-NC 模型的商业使用",cc_by_nc_description:"我们的最新模型通常采用 CC BY-NC 许可。如需商业使用，请通过我们的 API、Azure Marketplace 或 AWS SageMaker 访问它们。如需在这些渠道之外进行本地使用，请勾选此框。",company:"组织",company_size:"组织规模",company_website:"组织网站",company_website_placeholder:"您公司主页或 LinkedIn 个人资料的 URL",country:"国家",department:"部门",description:"与 Jina AI 一起拓展您的业务。",drop_area_for_image:"将图片拖放到此处",faq:"常见问题",feedback_sent:"已提交！我们将尽快回复您。",field_required:"字段为必填项",get_api_key:"如何获取我的 API 密钥？",image_upload:"附加图片",image_validate:"您最多可以附加 {_num} 张图片。仅限 JPG、JPEG、PNG、WEBP。",impact_snapshots:"实际案例",invalid_date_format:"日期格式无效。请使用 DD-MM-YYYY 格式。",invalid_email:"电子邮件无效",invalid_number:"无效数字。请再次输入",invalid_url:"网址无效",name:"姓名",nc_check:"我需要商业许可证吗？",other_questions:"其他问题",preferred_models:"您对哪些模型感兴趣？",preferred_products:"您对哪些产品感兴趣？",premium_key:"高级 API 密钥",pricing:"定价？",priority:"优先支持付费用户",private_statement:"隐私声明",rate_limit:"速率限制是多少？",role:"职业角色",self_check:"自检",sending_feedback:"正在发送...",shortcut:"捷径",standard_key:"标准 API 密钥",submit:"提交",submit_failed:"提交失败。请稍后再试。",submit_success:"感谢您提交。我们会尽快给您回复。",subtitle:"Jina AI 是多模态 AI 领域的领导者，擅长模型调优、模型服务、提示词调优部署。利用 Kubernetes 和无服务器架构等云原生技术，我们提供强大、可扩展且可立即投入生产的解决方案。凭借大语言模型、文本、图片、视频、音频理解、神经搜索和生成艺术方面的专业知识，我们提供创新、面向未来的策略来提升您的业务。",subtitle1:"Jina AI 是多模态 AI 领域的领导者，擅长大模型向量调优和部署、提示词调优和部署。利用 Kubernetes 和无服务器架构等云原生技术，我们提供强大、可扩展且可立即投入生产的解决方案。凭借大语言模型、文本、图片、视频、音频理解、神经搜索和生成式AI方面的专业知识，我们提供创新、面向未来的策略来提升您的业务。",subtitle2:"了解多模态AI最前沿的Jina AI。我们擅长向量化和提示词技术，利用 Kubernetes 等云原生解决方案来构建强大、可扩展的系统。我们专注于大型语言模型和媒体处理，利用先进的人工智能专业知识提供创新、面向未来的业务战略。",title:"联系销售",trusted_by:"我们值得信赖",turn_on_volume:"调高音量",work_email:"工作邮箱"},b="复制",A="已复制到剪贴板",h={description:"用于从文本创建高清图片的人机交互工作流程"},I={api_endpoint:"API 端口",api_key:"API 密钥",api_tagline:"与 OpenAI 的聊天 API 模式完全兼容，只需将 <code>api.openai.com</code> 与 <code>deepsearch.jina.ai</code> 交换即可开始。",api_title:"深度搜索 API",assistant_message:"助手",chat_ui:{clear_context_message:"您确定要清除上下文吗？这将重置对话。",clear_context_title:"新聊天？",description:"在简单的聊天界面中看看深度搜索地不地道。深度搜索最适合需要迭代推理、世界知识或最新信息的复杂问题。",example_q1:"OpenAI 的最新博客写的什么？",example_q2:"node-DeepResearch 项目背后的都动机是什么？",example_q3:"jina-colbert-v2 相对于 jina-colbert-v1 究竟有哪些改进？",input_cant_be_empty:"输入不能为空",input_placeholder:"在这里输入您的问题",keyman:"密钥管理器",move_to_new:"我们刚刚推出了一款全新的深度搜索UI，它速度快、简洁且免费。请访问 https://search.jina.ai 查看或单击下面的按钮尝试一下！",new_chat:"清除当前聊天并开始新对话",new_url:"访问新 UI",payment_required:"您的 API 密钥中剩余的词元额度不足。如果您有多个 API 密钥，您可以切换到密钥管理器中词元额度充足的密钥。否则，您可以充值 API 密钥以继续。",purchase:"充值API密钥",rate_limit_exceeded:"您已超出速率限制。请稍后重试，或使用您的 API 密钥获取更高的速率限制。",stay:"保留经典演示 UI",thinking:"正在思考...",thinking_done:"思维链",title:"深度搜索聊天",use_user_key:"使用您的 API 密钥来获取更高的速率限制。"},client_3p:"聊天客户端",client_3p_explain:"为了获得最佳体验，我们建议使用专业的聊天客户端。深度搜索 与 OpenAI 的聊天 API 架构完全兼容，因此可以轻松与任何兼容 OpenAI 的客户端一起使用。",comparison:{group1:{bestFor:"常识问题的快速答案",feature1:"答案完全由预先训练的知识生成，具有固定的截止日期",limitations:"无法获取实时或训练后的信息",timeCost:"约1秒",title:"大模型",tokenCost:"约 1000 个词元"},group2:{bestFor:"需要当前或特定领域信息的问题",feature1:"通过汇总单次搜索结果生成的答案",feature2:"能够获取训练截止时间以外的当前信息",limitations:"解决需要多跳推理的复杂问题",timeCost:"约 3 秒",title:"RAG范式和带搜索的大模型",tokenCost:"约 10,000 个词元"},group3:{bestFor:"需要深入研究和推理的复杂问题",feature1:"自主智能体，可反复搜索、读取和推理",feature2:"根据当前发现动态决定下一步行动",feature3:"在返回结果之前自我评估答案质量",feature4:"可以通过多次搜索和推理循环深入研究主题",limitations:"比简单的大模型或 RAG 方法花费的时间更长",timeCost:"约50秒",title:"深度搜索",tokenCost:"约 500,000 个词元"}},demo:"与深度搜索聊天",demo_description:"在简单的聊天界面里看看深度搜索地不地道。深度搜索最适合需要迭代推理、世界知识或最新信息的复杂问题。",description:"搜索、读取并推理直到找到最佳答案。",explain:"深度搜索结合了网络搜索、读取和推理，可进行全面调查。您可以将其视为一个代理，接受您的研究任务 - 它会进行广泛搜索并经过多次迭代，然后给出答案。",faq:{answer1:"深度搜索是一个大模型API，它执行迭代搜索、读取和推理，直到找到查询的准确答案或达到其词元预算限制。",answer10:"速率限制因 API 密钥层而异，范围从 10 RPM 到 30 RPM。对于查询量大的应用程序来说，这一点很重要。",answer11:"深度搜索将思考步骤包装在 XML 标签 <think>...</think> 中，然后提供最终答案，遵循 OpenAI 流格式，但使用这些特殊标记来表示思路链。",answer12:"是的。Jina Reader 用于网页搜索和读取，为系统提供高效访问和处理网页内容的能力。",answer14:"是的，深度搜索在复杂查询中的词元使用量可以说很高 - 平均为 70,000 个词元，而基本大模型响应为 500 个词元。这显示了研究的深度，但也影响了成本。",answer18:"该系统主要由词元预算而非步数控制。一旦超出词元预算，系统就会进入 Beast 模式以生成最终答案。查看 <code>reasoning_effort</code> 了解更多详情。",answer19:"参考文献非常重要，如果一个答案被认为是明确的，但缺乏参考文献，系统会继续搜索而不是接受该答案。",answer2:"与 OpenAI 和 Gemini 不同，深度搜索专注于通过迭代提供准确的答案，而不是生成长篇文章。它针对深度网络搜索的快速、精确答案进行了优化，而不是创建全面的报告。",answer20:"是的，但需要进行大量的研究。“谁将在 2028 年成为总统”的例子表明，它可以通过多次研究迭代来处理推测性问题，尽管这种预测的准确性无法得到保证。",answer3:"您需要 Jina API 密钥。我们为新 API 密钥提供 100 万个免费词元。",answer5:"它根据所有积累的知识生成最终答案，而不是仅仅放弃或返回不完整的答案。",answer6:"不是。虽然它使用迭代搜索过程来提高准确性，但评估显示它在测试题目上的通过率达到了 75％，明显优于 0％ 的基线（gemini-2.0-flash），但并不完美。",answer7:"它差别很大 - 根据评估数据，查询可能需要 1 到 42 步，平均需要 4 步。也就是 20 秒。简单的查询可能很快得到解决，而复杂的研究问题可能涉及多次迭代，最多需要 120 秒。",answer9:"是的，deepsearch.jina.ai/v1/chat/completions 上的官方深度搜索API 与 OpenAI API 架构完全兼容，使用“jina-deepsearch-v1”作为模型名称。因此，从 OpenAI 切换到深度搜索并与本地客户端或任何兼容 OpenAI 的客户端一起使用非常容易。我们强烈推荐 Chatwise 以获得无缝体验。",question1:"什么是深度搜索？",question10:"API 的速率限制是多少？",question11:"<think>标签里面的内容是什么？",question12:"深度搜索是否使用 Jina Reader 进行网页搜索和读取？",question14:"为什么深度搜索对我的查询使用这么多标记？",question18:"有没有办法控制或限制步数？",question19:"答案中的参考文献有多可靠？",question2:"深度搜索与 OpenAI 和 Gemini 的深度研究能力有何不同？",question20:"深度搜索能处理有关未来事件的问题吗？",question3:"我需要什么 API 密钥来使用 DeepResearch？",question5:"当深度搜索达到其词元预算时会发生什么？它会返回不完整的答案吗？",question6:"深度搜索能保证答案的准确性吗？",question7:"一次典型的深度搜索查询需要多长时间？",question9:"深度搜索可以与任何与 OpenAI 兼容的客户端（如 Chatwise、CherryStudio 或 ChatBox）配合使用吗？",title:"深度搜索相关常见问题"},last_chunk:"这是流的最后一部分，其中包含最终答案、访问过的 URL 和词元使用情况。单击上面的按钮可获取实时响应。",learn_more:"了解更多",message:"消息",message_type:"附加图片/文档",message_type_explain:"支持不同的消息类型（模态），如文本（.txt、.pdf）、图片（.png、.webp、.jpeg）。支持的文件最大为 10MB，并且必须预先编码为数据 URI。",messages:"消息",messages_explain:"用户和助手之间迄今为止的对话的消息列表。",model:"模型",model_explain:"要使用的模型的 ID。",model_name:"模型名称",open:"打开",parameters:{auth_token:"@:tokenizer.parameters.auth_token",auth_token_explain:"深度搜索API 可免费使用。通过提供您的 API 密钥，您可以访问更高的速率限制，并且不会向您的密钥收费。",bad_hostnames:"不良域名",bad_hostnames_explain:"严格排除在内容检索之外的域列表。通常用于过滤已知的垃圾、低质量或不相关的网站。",boost_hostnames:"优质域名",boost_hostnames_explain:"内容检索优先级较高的域列表。对于提供有价值内容的特定域的高质量来源很有用。",budget_tokens:"词元预算",budget_tokens_explain:"这决定了深度搜索流程允许使用的最大词元数。更大的预算可以通过对复杂查询启用更详尽的搜索来提高响应质量，尽管深度搜索可能不会使用分配的全部预算。这会覆盖 <code>reasoning_effort</code> 参数。",high_explain:"最大程度地推理和搜索复杂查询 (单次请求最多两百万词元)","jina-deepsearch-v1_explain":"通过迭代搜索给出简洁的答案",json_schema:"结构化输出",json_schema_explain:"这将启用结构化输出，确保模型的最终答案与您提供的 JSON 模式相匹配。",low_explain:"基本推理和搜索简单查询（单次请求最多五十万词元）",max_attempts:"最大尝试次数",max_attempts_explain:"深度搜索过程中解决问题（及其所有子问题）的最大重试次数。较大的值允许深度搜索使用不同的推理方法和解决策略重试解决问题。此参数会覆盖 <code>reasoning_effort</code> 参数。",max_returned_urls:"最大返回 URL",max_returned_urls_explain:"最终答案/块中包含的最大 URL 数量。URL 按相关性和其他重要因素排序。",medium_explain:"中等推理和搜索深度 (单次请求最多一百万词元)",model:"@:deepsearch.model",model_explain:"@:deepsearch.model_explain",no_direct_answer:"不要直接回答",no_direct_answer_explain:"开启后，即使用户的问题非常简单，也会强制模型采取进一步的思考/搜索步骤。仅在您已经非常确定输入的问题始终需要进行深度搜索的情况下使用，即您的输入不可能包含“1+1=？吃了么您?”之类的简单、无意义、脑筋急转弯似的问题时，开启此功能很有用。",only_hostnames:"仅允许域名",only_hostnames_explain:"仅在允许的域名列表中搜索和阅读。所有其他域都将被忽略。若给定域名无法访问或根本不包含答案则让深度搜索质量会显著下降。",reasoning_effort:"@:deepsearch.reasoning_effort",reasoning_effort_explain:"@:deepsearch.reasoning_effort_explain",stream:"@:deepsearch.stream",stream_explain:"@:deepsearch.stream_explain"},reasoning_effort:"推理强度",reasoning_effort_explain:"限制推理模型的推理工作量。当前支持的值有低、中和高。减少推理工作量可以加快响应速度，并减少响应中用于推理的标记。",stream:"流式返回",stream_explain:"通过服务器发送的事件传递事件发生时的信息，包括推理步骤和最终答案。我们<b>强烈建议</b>保持此选项处于启用状态，因为深度搜索请求可能需要很长时间才能完成。禁用流式传输可能会导致“524 超时”错误。",tagline:"@:deepsearch.description",title:"深度搜索",type_file:"附加文件",type_image:"附加图片",type_text:"纯文本消息",user_message:"用户",what_is:"什么是深度搜索？"},k={description:"用一行代码创建引人注目的 Disco Diffusion 艺术作品"},v={description:"多模态数据的数据结构"},P="下载 SOC 2 类型 1 证明",w={"11B tokens":"壹佰壹拾亿","11B tokens_intuition1":"类似于读取维基百科上的所有英文文章。","11B tokens_targetUser":"生产部署","1B tokens":"拾亿","1B tokens_intuition1":"大概和读完莎士比亚全集和整个《哈利波特》系列是一样的。","1B tokens_targetUser":"原型开发","1M tokens":"壹佰万","1M tokens_intuition1":"相当于读完了《霍比特人》和《了不起的盖茨比》的全部文本。","1M tokens_targetUser":"玩具实验","1M_free":"免费百万词元随心用","1M_free_description":"使用免费词元享受您的新 API 密钥，无需信用卡。","2_5B tokens":"2.5B 词元","2_5B tokens_intuition1":"相当于把电影《指环王》三部曲里说的每一个字都抄录1000遍。","3p_integration":"拥有 <b>{_numPartners}</b> 家第三方服务","3p_integration_desc":"将我们的搜索底座与您现有的服务相集成。我们的合作伙伴已经打通了与我们 API 的连接，让您可以轻松地在应用程序中使用我们的模型。","500M tokens":"500M 词元","500M tokens_intuition1":"类似于观看《辛普森一家》从第 1 季到第 30 季的每一集。","59B tokens":"59B 词元","59B tokens_intuition1":"相当于两天内全球发布的所有推文。","5_5B tokens":"5.5B 词元","5_5B tokens_intuition1":"相当于读取大英百科全书的全部内容。",Free1M:"100 万个词元","ReaderLM-v2_description":"用于将原始 HTML 转换为 markdown 或 JSON 的小型语言模型",add_pair:"新建",add_time_explain:"此模型被添加到搜索底座的时间。",api_integration_short:"在流行数据库、向量数据库、RAG 和 LLMOps 框架轻松使用我们的向量模型API。",api_integrations:"API集成",api_key_update_message:"通过替换旧 API 密钥，新密钥将在您每次访问 jina.ai 时显示在 UI 中。未来的充值将适用于此新密钥。您的旧密钥仍然有效，因此如果您打算再次使用它，请安全保存。",api_key_update_title:"更换 API 密钥",auto_recharge:"词元余额不足时自动充值",auto_recharge_confirm_message:"您确定要禁用自动充值吗？当您的词元余额不足时，这将停止自动充值，并可能中断您的服务或应用程序。",auto_recharge_confirm_title:"禁用自动充值",auto_recharge_description:"建议用于生产环境不间断的服务。当您的词元余额低于设定的阈值时，我们将使用您保存的支付方式自动为您充值上次购买的套餐，直到达到阈值。",auto_recharge_enable:"您已启用自动充值",auto_recharge_enable_message:"要启用自动充值，请在购买套餐时打开自动充值开关。",auto_recharge_enable_message2:"请选择自动充值时需要购买的套餐。",auto_recharge_enable_title:"启用自动充值",auto_request:"自动预览",auto_request_tooltip:"使用 API 密钥中的数百个词元，在更改模型时自动预览 API 响应。关闭后，单击“获取响应”即可手动发送请求。",autostart:"向量化将自动开始",base64_description:"向量以 base64 编码的字符串形式返回。传输效率更高。",batch_job:"批处理",batch_upload_hint:"我们将使用如下的 API 密钥和模型来批处理。","bge-base-en-v1_5_description":"一种强大的英语模型，平衡了性能和效率，适合多种用途。","bge-base-en_description":"平衡的英语模型，旨在实现可靠的性能。","bge-base-zh-v1_5_description":"全面平衡能力与效率的中国模式。","bge-base-zh_description":"集效率与强劲性能于一身的多功能中国车型。","bge-large-en-v1_5_description":"强大的英语模型，提供具有卓越品质的顶级向量。","bge-large-en_description":"专为优质向量而打造的顶级英语模型。","bge-large-zh-v1_5_description":"提供卓越且详细向量的高容量中文模型。","bge-large-zh_description":"针对顶级向量优化的高性能中文模型。","bge-m3_description":"一种多功能多语言模型，提供广泛的功能和高质量的向量。","bge-small-en-v1_5_description":"精简的英语模型，提供高效、高质量的向量。","bge-small-en_description":"一种高效的英语模型，用于简化和准确的向量。","bge-small-zh-v1_5_description":"紧凑的中国模型，提供灵活、精确的向量。","bge-small-zh_description":"一种敏捷的中国模型，用于高效、精确的向量。",binary_description:"向量被打包为 int8。存储、搜索和传输效率更高。",bulk:"批处理",bulk_embedding_failed:"创建批处理失败",buy_more_quota:"使用更多词元充值此 API 密钥",buy_poster:"购买海报",cancel_button:"取消",click_upload_btn_above:"单击上面的上传按钮即可开始。",clip_v2_description:"jina-clip-v2 是一个 0.9B CLIP 风格模型，它带来了三大进步：对 89 种语言的多语言支持、512x512 的高图片分辨率和用于截断向量的 Matryoshka 表示学习。",clip_v2_title:"clip-v2：多语言多模态向量",code:"代码",colbert_dimensions_explain:"每个词元向量的维度大小。",compatible:"兼容模式",compatible_explain:"遵循与我们的文本向量模型相同的请求格式。这允许您在不更改请求的情况下在模型之间切换。请注意，此模式下不支持图片输入。",contact_sales:"联系销售人员",contact_sales_description:"与我们的销售团队联系",cosine_similarity:"余弦相似度",debugging:"测试",delete_pair:"删除",description:"@:landing_page.embedding_desc1",dimensions:"输出维度",dimensions_error:"尺寸大小必须介于 1 到 1024 之间。",dimensions_explain:"较小的尺寸可以实现高效的存储和检索，并且由于采用了 Matryoshka 表示法，影响最小。",dimensions_warning:"为了提高性能，我们建议将尺寸大小保持在 {_minDimension} 以上。",document:"文档",download:"下载",edit_text1_text:"编辑左侧文字",edit_text2_text:"编辑右侧文字",embedding_done:"成功向量化{_Count}个句子。",embedding_none_description:"不要使用任何向量模型",example_inputs:"示例输入",faq:"@:contact_us_page.faq",faqs_v2:{answer0:"有关我们的训练过程、数据源和评估的详细信息，请参阅 arXiv 上提供的技术报告。",answer1:"Jina CLIP <code>jina-clip-v2</code> 是一种先进的多模态向量模型，支持文本-文本、文本-图片、图片-图片和图片-文本检索任务。与在文本-文本搜索方面表现不佳的原始 OpenAI CLIP 不同，Jina CLIP 在文本检索方面表现出色。<code>jina-clip-v2</code> 在文本-图片和文本-文本检索任务中比 <code>jina-clip-v1</code> 的性能提高了 3%，支持 89 种语言进行多语言图片检索，处理更高分辨率的图片（512x512），并通过 Matryoshka 表示降低存储要求。您可以在我们的技术报告中读取更多相关信息。",answer17:"是的，<code>jina-clip-v2</code> 和 <code>jina-clip-v1</code> 可以支持图片和文本。更多模态上的向量模型将很快公布！",answer18:"有关使用特定数据微调我们的模型的疑问，请联系我们讨论您的要求。我们愿意探索如何调整我们的模型来满足您的需求。",answer19:"是的，我们的服务在 AWS、Azure 和 GCP 市场上可用。如果您有特定要求，请通过 sales AT jina.ai 联系我们。",answer3:"截至 2024 年 9 月 18 日发布，<code>jina-embeddings-v3</code> 是最好的多语言模型，在参数少于 10 亿的模型的 MTEB 英语排行榜上排名第二。v3 共支持 89 种语言，包括性能最佳的前 30 种语言：阿拉伯语、孟加拉语、中文、丹麦语、荷兰语、英语、芬兰语、法语、格鲁吉亚语、德语、希腊语、印地语、印尼语、意大利语、日语、韩语、拉脱维亚语、挪威语、波兰语、葡萄牙语、罗马尼亚语、俄语、斯洛伐克语、西班牙语、瑞典语、泰语、土耳其语、乌克兰语、乌尔都语和越南语。有关更多详细信息，请参阅 <code>jina-embeddings-v3</code> 技术报告。",answer4:"我们的模型允许输入长度高达 8192 个词元，这比大多数其他模型高得多。词元的范围可以从单个字符（如“a”）到整个单词（如“apple”）。可以输入的字符总数取决于所用单词的长度和复杂性。这种扩展的输入功能使我们的 <code>jina-embeddings-v3</code> 和 <code>jina-clip</code> 模型能够执行更全面的文本分析，并在上下文理解方面实现更高的准确性，尤其是对于大量文本数据。",answer5:"一次 API 调用最多可以处理 2048 个句子或文本，从而有助于在一次请求中进行广泛的文本分析。",answer6:"您可以在 API 请求的 <code>input</code> 字段中使用 <code>url</code> 或 <code>bytes</code>。对于 <code>url</code>，请提供要处理的图片的 URL。对于 <code>bytes</code>，请以 base64 格式对图片进行编码并将其包含在请求中。模型将在结果中返回图片的向量。",answer7:"在 MTEB 英语、多语言和 LongEmbed 基准的评估中，<code>jina-embeddings-v3</code> 在英语任务上的表现优于 OpenAI 和 Cohere 的最新专有向量模型，并在所有多语言任务中超越 <code>multilingual-e5-large-instruct</code>。由于集成了 Matryoshka 表示学习 (MRL)，默认输出维度为 1024，用户可以将向量维度截断为 32，而不会影响性能。",answer8:"迁移过程十分顺畅，因为<a class='text-primary' href='https://api.jina.ai/v1/embeddings'>我们的 API 端点</a>与 OpenAI 的 <code>text-embedding-3-large</code> 模型的输入和输出 JSON 架构相匹配。这种兼容性确保用户在使用 OpenAI 端点时可以轻松地将 OpenAI 模型替换为我们的模型。",answer9:`词元是根据文本长度和图片大小计算的。对于请求中的文本，词元以标准方式计算。对于图片，执行以下步骤：

1. 图块大小：每个图片被分成图块。对于 <code>jina-clip-v2</code>，图块为 512x512 像素，而对于 <code>jina-clip-v1</code>，图块为 224x224 像素。
2. 覆盖率：计算覆盖输入图片所需的图块数量。即使图片尺寸不能被图块大小完全整除，部分图块也会被视为完整图块。
3. 总图块数：覆盖图片的图块总数决定了成本。例如，600x600 像素的图片在 v2 中将被 2x2 图块（4 个图块）覆盖，在 v1 中将被 3x3 图块（9 个图块）覆盖。
4. 成本计算：对于 <code>jina-clip-v2</code>，每个图块的成本为 4000 个词元，而对于 <code>jina-clip-v1</code>，每个图块的成本为 1000 个词元。

示例：
对于尺寸为 600x600 像素的图片：

• 使用 <code>jina-clip-v2</code>
• 图片被分成 512x512 像素图块。
• 所需的图块总数为 2（水平）x 2（垂直）= 4 个图块。
• <code>jina-clip-v2</code> 的成本为 4*4000 = 16000 个词元。

• 使用 <code>jina-clip-v1</code>
• 图片被分成 224x224 像素图块。
• 所需的图块总数为 3（水平）x 3（垂直）= 9 块图块。
• jina-clip-v1 的成本为 9*1000 = 9000 个词元。`,question0:"jina-embeddings-v3 模型是如何训练的？",question1:"jina-clip 模型是什么？我可以使用它们进行文本和图片搜索吗？",question17:"你们提供向量模型图片或音频的模型吗？",question18:"Jina 向量模型模型可以使用私人或公司数据进行微调吗？",question19:"您的服务可以在 AWS、Azure 或 GCP 上私有化部署吗？",question3:"你们的模型支持哪些语言？",question4:"单个句子输入的最大长度是多少？",question5:"单个请求中最多可以包含多少个句子？",question6:"如何将图片发送给 jina-clip 模型？",question7:"Jina Embeddings 模型与 OpenAI 和 Cohere 的最新向量模型相比如何？",question8:"如何从 OpenAI 的 text-embedding-3-large 迁移到 Jina Embeddings 模型？",question9:"使用 jina-clip 模型时如何计算 token？",title:"向量模型相关的常见问题"},feature_8k1:"8192长度",feature_8k_description1:"世界首发的 8192 个词元长度的开源向量模型，能够把一整版人民日报压缩成一个向量。",feature_cheap:"降本 50 倍",feature_cheap_v1:"5 倍降本",feature_cheap_v1_description1:"从免费试用开始，简单的定价结构，快捷支付。只需 OpenAI 20% 的成本即可获得强大的向量模型。",feature_multilingual:"提供德英、中英等双语模型，出海企业适合跨语言应用必备。",feature_on_premises:"隐私第一",feature_on_premises_description1:"直接在您的虚拟私有云 (VPC) 中无缝部署我们的向量模型。轻松在 AWS Sagemaker上部署，即将与微软Azure 和谷歌 Cloud Platform 集成。如需定制 Kubernetes 部署，请联系我们的销售团队寻求专业帮助。",feature_on_premises_description2:"您可以轻松在 AWS Sagemaker上部署我们的向量模型，我们很快将在微软Azure和谷歌Cloud Services中提供支持。如需定制 Kubernetes 部署，请联系我们的销售团队寻求专业帮助。",feature_on_premises_description3:"在 AWS Sagemaker 和 Microsoft Azure 中部署 Jina Embeddings 模型，并很快在 Google Cloud Services 中部署，或者联系我们的销售团队，为您的虚拟私有云和本地服务器获取定制的 Kubernetes 部署。",feature_on_premises_description4:"使用 AWS SageMaker、Microsoft Azure 或 Google Cloud Services 在本地部署 Jina Embedding 和 Reranker 模型，确保您的数据安全地处于您的控制之下。",feature_solid:"出类拔萃",feature_solid_description1:"基于我们实打实的AI科研工作，并与同类模型进行了严格对比测试，以确保模型的最佳性能。",feature_top_perform1:"无缝集成",feature_top_perform_description1:"与OpenAI的API完全兼容。轻松与 10 多个向量数据库和 RAG 系统集成，提供流畅的开发者体验。",file_required:"请上传文件",file_size_exceed:"文件大小超过上限 {_size}",file_type_not_supported:"文件类型不支持",fill_example:"填写示例",float_description:"向量以浮点数列表的形式返回。最常见且易于使用。",free:"自由的",generate_api_key_error:"API 密钥生成失败。",generating_visualization:"生成可视化...",get_new_key_button:"获取新密钥",get_new_key_button_explain:"选择新密钥将导致与旧密钥相关的使用历史记录丢失。",get_new_key_survey:"填写调查问卷，帮助我们了解您的使用情况，并免费获得新的 API 密钥！",includes:"购得词元可在以下产品中使用：",index_and_search:"先索引再查询",index_and_search1:"索引和搜索",input:"请求",input_api_key_error1:"您的 API 密钥无效！",input_length:"输入长度",input_type:"向量化文档/查询",input_type_explain:"根据搜索角色，相同的输入可以作为查询或文档向量。",integrate:"集成","jina-clip-v1_description":"图片和英文文本的多模态向量模型","jina-clip-v2_description":"文本和图片的多语言多模态向量模型","jina-colbert-v1-en_description":"改进版的ColBERT模型支持8K长度的上下文，可用于向量化和重排任务","jina-colbert-v2_description":"最新的多语言ColBERT，在向量化和重排方面具有顶级性能","jina-embedding-b-en-v1_description":"Jina 向量模型的第一个版本，传说中的OG。","jina-embeddings-v2-base-code_description":"针对代码和技术文档搜索的向量模型","jina-embeddings-v2-base-de_description":"支持德英双语的 8K 最佳向量模型","jina-embeddings-v2-base-en_description":"与 OpenAI 的 text-embedding-ada002旗鼓相当","jina-embeddings-v2-base-es_description":"支持西英双语的 8K 最佳向量模型","jina-embeddings-v2-base-zh_description":"支持中英双语的 8K 最佳向量模型","jina-embeddings-v2-small-en_description":"针对低延迟和小内存进行了优化","jina-embeddings-v3_description":"最新、最好的向量化模型，在文本和代码上均具有最佳性能","jina-reranker-v1-base-en_description":"我们的第一个重排器最大化搜索和 RAG 相关性","jina-reranker-v1-tiny-en_description":"最快的重排器，适合对大量文档进行可靠的排序","jina-reranker-v1-turbo-en_description":"速度与准确率的最佳权衡选择","jina-reranker-v2-base-multilingual_description":"最先进的多语言文档和查询重排器，具有最佳的准确性和速度性能",key:"API密钥",key_enter_placeholder:"请输入您的 API 密钥",key_enter_placeholder_to_topup:"输入您要充值的 API 密钥",key_to_top_up:"有其他 API 密钥需要充值？粘贴上述内容并点击“保存”。",key_warn:"请确保将您的 API 密钥存储在安全的地方。否则您将需要生成一个新密钥",key_warn_v2:"这是您的专属密钥。请安全保存！",language_explain:"该模型对{_language}语言有最好的支持。",last_7_days:"用法",late_chunking:"后期分块",late_chunking_explain:"应用后期分块技术来利用模型的长上下文功能来生成上下文块向量化。",learn_more:"了解更多",learn_poster:"了解海报",learning1:"学习向量模型",learning1_description:"什么是向量，为什么要向量化？我们已经为您提供了一些入门文章。通过我们的综合指南从头开始了解向量模型。",length:"长度",manage_billing:"管理发票",manage_billing_tip:"管理您的账单信息、获取发票并设置自动充值。",manage_quota1:"密钥和计费",max_file_size:"最大允许的尺寸：{_maxSize}。",maximize_tooltip:"使用 Shift+1 最大化此面板",mistake_contact:"如果您认为这是一个错误，请联系我们。",mminput_placeholder:"文本、图片URL、图片base64字符串",model_required:"请选择模型",more_models:"其他 {_numMore} 个模型",more_than_two2:"请输入两个以上的文档，即两行以上。",multi_embedding:"多载体",multi_embedding_explain:"该模型会给一个输入返回一组向量。输入句子中的每个词元都被映射到单独的一个向量。",multilingual:"多语言支持",multimodal:"多式联运",multimodal_explain:"该模型可以对文本和图片输入进行编码，使其成为多模态搜索任务的理想选择。",new:"新模型",no_data1:"添加一对句子来计算相似度",none:"没有任何",normalized:"L2 规范化",normalized_explain:"归一化向量，使其欧几里得 (L2) 范数变为 1，保持方向。当下游涉及点积、分类、可视化时很有用。",oncsp:"关于 CSP",onprem:"私有化部署",open_tensorboard:"打开可视化工具",opensource:"开源",opensource_explain:"该模型是开源的，可从 Hugging Face 下载使用。单击此按钮可查看 Hugging Face 上的模型。",original_documents:"向量化的句子",original_documents_hint:"在此处输入句子。每个新行将被视为一个单独的句子/文档。",output:"响应",output_dim:"输出维度",output_dim_explain:"该模型输出的向量维度为 {_outputDim}。",output_dimension:"输出维度",pairwise_test:"成对测试",per_k:"每千个词元",per_m:"每百万词元",please_fill_docs_first:"在搜索之前，请先在下面输入一些句子。",please_select_model:"请选择向量模型或重排器模型",poster:"向量模型70年",poster_description:"在您的办公空间或起居室内悬挂一张我们精心制作的海报，在1950年以来文本向量模型的进化和演变中寻找下一个灵感。",pricing:"API价格表",pricing_desc:"API 定价基于词元使用情况。一个 API 密钥即可访问所有搜索基础产品。",protectData1:"您向我们发送的数据和文档都不会用于模型训练。",protectData2:"数据在传输过程中加密 (TLS 1.2+) ，同时静态数据也会被加密 (AES-GCM 256)。",protectData3:"SOC 2 和 GDPR 合规。",protect_data:"保护您的数据",public_cloud_integration:"与 <b>{_numPartners}</b> 个云服务提供商合作",public_cloud_integration_desc:"您的公司是否在使用 AWS 或 Azure？那么请直接在贵公司的这些平台上私有化部署我们的搜索底座模型，这样您的数据就能保持安全且合规。",query:"查询句",raise_issue:"问题反馈",rank_none_description:"不要使用任何重排模型",read_api_docs:"API 规范",read_release_note:"读取发行说明","reader-lm-05b_description":"用于将原始 HTML 转换为 Markdown 的小型语言模型","reader-lm-15b_description":"用于将原始 HTML 转换为 Markdown 的小型语言模型",recharge_threshold:"充值门槛",refresh:"刷新",refresh_key_tooltip1:"免费获取新的 API 密钥",refresh_token_count1:"刷新以获取当前 API 密钥的可用词元",regenerate:"生成新的密钥",remaining:"剩余词元额度",remaining_left:"您在下面的 API 密钥中还剩余 <b>{_leftTokens}</b> 个词元。",request_number:"请求次数",request_path:"请求端点",results_as_final_result:"最终结果的文档数",results_fed_to_reranker:"提交给重排器的文档数",retry:"重试",return_base64:"Base64（字符串）",return_binary:"二进制（打包为 int8）",return_float:"默认（浮点型）",return_format:"向量格式",return_format_explain:"除了浮点数之外，您还可以要求它以二进制形式返回，以便更快地进行矢量检索，或者以 base64 编码形式返回，以便更快地进行传输。",return_format_title:"返回数据类型",return_ubinary:"二进制（打包为 uint8）",right_api_key_to_charge:"请输入正确的API密钥进行充值",running:"运行中",score:"分数",search:"搜索",search_hint:"在下列句子中输入要搜索的内容",select_classify_model:"选择分类器",select_embedding_model:"选择向量模型",select_rerank_model:"选择重排器",show_api_key:"显示 API 密钥",size:"参数",size_explain:"模型中的参数数量为{_size}，请注意，这不代表模型的文件大小。",sleeping:"休眠中",start_batch:"开始批处理",start_embedding:"开始索引",status_explain:"我们的无服务器架构可能根据使用率即时从内存中卸载一些当下无人使用的模型。对于活跃模型，响应是立即的。而休眠中的模型在收到第一次请求时才会加载，这一过程可能会持续几十秒。模型被唤醒后，后续请求的处理速度会更快。",task_type:"下游任务",task_type_classification:"分类",task_type_classification_explain:"文本分类。",task_type_explain:"选择将使用向量模型的下游任务。模型将返回针对该任务优化的向量。",task_type_none_explain:"不使用适配器。将返回通用向量，可用于调试或黑客攻击。",task_type_retrieval_passage:"检索通道",task_type_retrieval_passage_explain:"将文档向量化查询文档检索任务中。",task_type_retrieval_query:"检索查询",task_type_retrieval_query_explain:"在查询文档检索任务中向量化查询。",task_type_separation:"分离",task_type_separation_explain:"聚类文档，可视化语料库。","task_type_text-matching":"文本匹配","task_type_text-matching_explain":"语义文本相似度、广义对称检索、推荐、查找相似项、去重。",tax_may_apply:"根据您所在的位置，您可能需要支付美元、欧元或其他货币的费用。可能需缴纳税费。",text1:"左",text2:"右",three_ways:"三种购买方式",three_ways_desc:"订阅我们的 API、通过云提供商购买或为您的组织获取商业许可证。",title:"向量模型API",token_example:"一条微博大约有 20 个词元，一篇人民日报新闻大约有 1000 个词元，而查尔斯·狄更斯的小说《双城记》有超过一百万个词元。",token_length_explain:"此模型所支持的输入词元的最大长度为 {_tokenLength}。",tokens:"词元",tools:"工具",top_up_button:"给旧密钥充值",top_up_button_explain:"集成此 API 密钥可提供更专业的解决方案，无需频繁更改密钥。使用数据将被保留并可随时访问。",top_up_warning_message1:"当前 API 密钥还剩 {_remainedTokens} 词元，将被具有 {_freeTokens} 词元的新密钥替换。如果您已妥善保管旧密钥，则可以继续使用或充值旧密钥。您想如何进行？",top_up_warning_title:"是否替换旧密钥？",total_documents:"向量化进度：{_Processed}/{_Count} 个句子。",truncate:"在最大上下文长度处截断",truncate_explain:"启用后，模型将自动删除超出模型允许的最大上下文长度的尾部，而不是抛出错误。",tuning:"微调",turnstile_error:"我们无法生成 API 密钥，因为我们无法验证您是否是人类。",turnstile_unsupported:"由于您的浏览器不受支持，我们无法生成 API 密钥。",ubinary_description:"向量被打包为 uint8。存储、搜索和传输效率更高。",upload:"上传",upload_file:"单击此处上传文件",usage:"用法",usage_amount:"词元",usage_history:"过去 7 天的使用情况",usage_history_explain:"数据并不是实时的，可能会延迟几分钟。",usage_reason:"描述",usage_reason_consume:"用过的",usage_reason_purchase:"已购买",usage_reason_transfer_in:"转入",usage_reason_transfer_out:"转出",usage_reason_trial:"每个新 API 密钥中都有免费令牌",usage_rerank:"用法",usage_time:"时间日期",v3_description:"<code>jina-embeddings-v3</code> 是一种前沿多语言文本向量模型，具有 570M 个参数和 8192 个词元长度，在 MTEB 上的表现优于 OpenAI 和 Cohere 的最新专有向量模型。请读取下面的博客文章和研究论文。",v3_title:"v3：顶级多语言向量模型",vector_database_integration1:"集成",vector_database_integration2:"在流行数据库、向量数据库、RAG 和 LLMOps 框架轻松使用我们的向量模型API。首先，只需将您的 API 密钥复制到以下任意集成中即可快速使用我们的模型。",vector_database_integration3:"我们的 Embedding & Reranker API 与各种知名数据库、向量存储、RAG 和 LLMOps 框架原生集成。首先，只需将您的 API 密钥复制并粘贴到任何列出的集成中即可快速无缝地启动。",vector_database_integration_description:"将 Jina Embeddings API 与以下任何向量数据库、大模型编排框架和 RAG 应用程序无缝、轻松地集成。我们的教程将向您展示如何操作。",view_details:"查看详情",visualization_example:"将本节中的所有句子映射到 3D 向量空间",visualization_example_you_can:"使用我们下面的API，你也可以做到！",visualize:"可视化",visualize_done:"可视化已完成，您现在可以单击顶部按钮打开可视化工具。",wait_for_processing:"您的请求正在处理中。",wait_stripe:"正在开通Stripe支付，请稍候",what_are_embedding:"什么是向量化？",what_are_embedding_answer:`想象一下教计算机掌握单词和短语的细微含义。传统方法依赖于严格的基于规则的系统，由于语言过于复杂和流动，因此无法达到预期效果。输入文本向量：一种强大的解决方案，可将文本转换为数字语言 - 具体来说，转换为高维空间中的向量。

考虑短语“晴朗的天气”和“晴朗的天空”。对我们来说，它们描绘了相似的画面。通过向量的视角，这些短语被转换成数字向量，它们在这个多维空间中彼此靠近，捕捉它们的语义亲缘关系。向量空间中的这种接近性不仅仅是单词或短语相似；它还涉及理解上下文、情感，甚至含义中的细微差别。

为什么这一突破很重要？首先，它弥合了人类语言的丰富性和算法的计算效率之间的差距。算法擅长处理数字，而不是解释文本。通过将文本转换为向量，向量使这些算法能够以以前无法实现的方式“理解”和处理语言。

实际应用范围广泛且多种多样。无论是推荐符合您兴趣的内容，还是为感觉非常人性化的对话式人工智能提供支持，甚至是在大量文本中检测微妙的模式，向量都是关键。它们使机器能够执行情绪分析、语言翻译等任务，并且对语言的理解越来越细致入微和精细化。`,what_is_a_token:"文本处理中的词元是一个单元，通常是一个单词。例如，“Jina AI 太棒了！”变成五个词元，包括标点符号。",why_do_you_need:"选择最适合的向量模型",why_do_you_need_after:"通过深度学习和先进的语言处理技术，我们的向量模型能够将复杂的多模态数据转换为简化的格式，这不仅提升了机器的理解力，也为实现更复杂的AI应用提供了可能，包括但不限于提高数据解析能力、增加用户互动、消除语言障碍和改进开发流程。",why_do_you_need_before:"我们的向量模型旨在涵盖各种搜索和 GenAI 应用。",why_need_1_description:"我们依托JinaBERT打造的向量基座模型，面向各种场景设计。它在解析长篇文本方面表现出色，适合用于语义搜索、内容分類及深度语言分析等任务。这种多功能性使其成为开发情绪分析工具、文本摘要和个性化推荐系统的理想选择。",why_need_1_title:"通用向量",why_need_2_description:"我们的双语模型旨在打破语言壁垒，提升多语言平台的沟通效率、全球客户支持和跨语言内容的发现能力。专注于德语-英语和中文-英语的双向翻译，让不同语言的用户能够更容易地理解和交流。",why_need_2_title:"双语向量",why_need_3_description:"专为开发者设计的代码向量模型，能够简化代码摘要编写、代码生成和自动代码审查等任务。通过深入分析代码结构并提供改进建议，显著提升了开发效率，是开发高级IDE插件、自动生成文档和创新调试工具的关键。",why_need_3_title:"代码向量",why_need_4_description:"Jina CLIP 是我们最新的图片和文本多模态向量模型。与 OpenAI CLIP 相比，一个很大的改进是，这个单一模型可以用于文本-文本检索，以及文本-图片、图片-文本和图片-图片检索任务！因此，一个模型，两种模态，四个搜索方向！",why_need_4_title:"多模态向量",write_email_here:"请输入您希望在完成后接收下载链接的电子邮件。",you_can_leave:"您可以离开此页面，完成后我们将向您发送下载链接。"},y={description:"世界一流的多模态多语言向量模型。"},f={contractType:{department:"子部门许可证",poc:"概念验证（3-6个月）",standard:"标准企业许可证",title:"合同类型"},department:{businessSponsor:"业务单位内荐人",executionModel:"执行模型",growth:{high:"明确整个企业的潜力",highDesc:"计划在全公司范围内采用的战略举措",limited:"限部门",limitedDesc:"通过标准支持关注单个部门的需求",steady:"其他部门的潜力",steadyDesc:"计划在 12 个月内扩展到 2-3 个部门"},growthTitle:"增长轨迹",sponsorDescription:"专门的执行发起人，倡导实施，提供战略方向，并确保资源得到分配"},descriptions:{contractType:{department:"单一部门部署，后期可灵活扩展",poc:"试验部署以在特定环境和用例中测试模型",standard:"全面企业部署，模型使用不受限制"},department:{growth:"计划扩大模型在其他部门的使用",sponsorship:"表明创收部门是否支持部署"},features:{csm:"个人客户成功经理提供战略指导和支持",priority:"保证对关键问题做出快速响应",training:"帮助您针对特定数据进行预训练或微调模型"},models:{clip:"为多模式应用处理图片和文本",colbert:"高精度文档检索专用模型",embeddings:"用于语义搜索和文本相似度的文本向量模型",reader:"将 HTML 内容转换为干净的 markdown 格式",reranker:"微调搜索结果以提高相关性"},payment:{annual:"每年一次的简化会计付款",quarterly:"每三个月定期付款"},poc:{duration:"在您的环境中测试和验证模型性能的时间表",metrics:"跟踪关键绩效指标和模型有效性"},support:{enterprise:"最高优先级的全面支持覆盖",premium:"额外的咨询时间和更快的响应时间",standard:"基础技术支持与实施指导"},usage:{business:"有多少不同的企业将使用由我们的模型支持的应用程序",consumer:"每月有多少终端用户将与我们的模型进行交互"}},features:{csm:"专门的客户经理",priority:"优先响应 SLA（4 小时内）",title:"其他功能",training:"自定义模型训练支持"},interests:"我对此配置的商业许可 (${_Price}) 感兴趣",labels:{basePrice:"基价",custom:"联系销售人员了解定制价格",discountApplied:"已享受折扣",included:"包含在基础价格中",learnMore:"了解更多",priceQuarterly:"每季度价格",selectAll:"选择所有型号",selectSupport:"选择支持层",totalPrice:"总价",upTo:"最多 {count} 个"},messaging:{additionalFeatures:"附加功能包括：",baseModelIncluded:"包含基本向量模型 (jina-embeddings-v3)",deptIncludes:"部门执照包括：",deptReviews:"季度业务审查会议",deptRoadmap:"企业扩张路线图规划",deptSponsor:"执行发起人协调会议",deptWorkshops:"跨部门协作研讨会",enterpriseAlert:"您的使用水平表明这是一个企业范围内的机会。让我们安排一次通话来讨论定制企业协议。",noModelsSelected:"未选择其他模型。使用基础向量模型。",pocCheckins:"每两周与技术团队进行一次检查",pocIncludes:"POC 包包括：",pocMetrics:"成功指标跟踪仪表板",pocMigration:"支持迁移至完整许可证",pocTemplate:"POC 结果文档模板",selectedModels:"选定的模型：",standardFeatures:"标准许可证功能：",supportTierIncluded:"{tier} 支持层包含 {hours}",usageTierBusiness:"商业使用层级：最多 {count} 个商业帐户",usageTierConsumer:"消费者使用层级：每月最多 {count} 个活跃用户"},models:{clip:"jina-clip-v2",colbert:"jina-colbert-v2",description:"选择要包含在您的商业套餐中的模型",embeddings:"jina-embeddings-v3",lm:"reader-lm",reranker:"jina-reranker-v2",title:"选择型号"},payment:{annual:"年度结算（10% 折扣）",features:"包含的功能",quarterly:"按季度计费",title:"付款条款"},poc:{description:"POC 包括成功指标跟踪和完整许可升级路径",duration:"POC 持续时间（月）"},pricing:{annual:"年",cta:"与我们的销售人员联系",disclaimer:"此定价计算器提供估算。您的最终价格可能因具体要求、数量承诺和自定义配置而异。请联系我们的销售团队获取详细报价。",frequency:"${price} / {frequency}",oneTime:"${price} 一次性",pocTotal:"{months} 月 POC 价格：${price}",quarterly:"季度",title:"预计价格"},short_title:"配置许可证",subtitle:"为 Jina AI 模型配置企业许可证",support:{enterprise:"高级的",hoursQuarter:"{hours} 小时/季度",premium:"标准的",standard:"轻量的",title:"支持层"},title:"企业许可证配置器",tooltips:{annualDiscount:"按年付款可节省 10%",businessSponsor:"拥有业务单位赞助商可享受额外折扣",pocDuration:"选择概念验证期的持续时间",supportTier:"选择最适合您需求的支持级别",usageLimit:"如果超出这些限制，请联系我们获取定制价格"},usage:{business:"B2B（企业账户）",businessCount:"企业账户数量",businessDescription:"使用我们的模型的不同商业账户数量",consumer:"B2C（终端用户）",consumerCount:"每月活跃用户",consumerDescription:"所有应用程序的每月活跃终端用户数量",title:"使用配置"}},x={answer1:"Jina AI 专注于多模态AI技术，包括大模型向量的调优和部署、提示词的调优和部署。我们利用 Kubernetes 和无服务器架构等先进工具来创建强大、可扩展且可立即投入生产的解决方案。",answer10:"我们根据项目的性质和客户的需求提供不同的许可选项。详细条款可以与我们的销售团队讨论。",answer11:"我们在全球范围内提供服务，总部位于欧洲柏林，并在北京和深圳设有办公室。",answer12:"是的，我们提供现场支持，特别是对于位于柏林、北京和深圳办公室附近的客户。对于其他地点，我们努力提供最好的远程支持，并在必要时安排现场支持。",answer2:"我们的专业知识涵盖广泛，包括大型语言模型、文本、图片、视频、音频理解、神经搜索和生成式AI。",answer3:"是的，我们的解决方案设计为可扩展并可用于生产。我们使用云原生技术构建解决方案，从而在生产环境中实现高效扩展和可靠的性能。",answer4:"我们的服务用途广泛且适应性强，适用于多种行业，包括电子商务、法律技术、数字营销、游戏、医疗保健、金融等。",answer5:"您可以通过此页面上的联系表与我们的销售团队取得联系。我们很乐意讨论您的项目需求以及我们的解决方案如何帮助您的业务。",answer6:"我们提供持续的支持，以确保我们的解决方案顺利运行。这包括根据您的反馈和需求进行故障排除、定期更新和改进。",answer7:"项目持续时间根据项目的复杂性和范围而有所不同。了解您的要求后，我们可以提供更准确的估算。",answer8:"数据安全是我们的首要任务。我们遵守严格的数据保护政策和法规，以确保您的数据安全和保密。",answer9:"定价取决于项目的复杂性和要求。我们提供基于项目的定价模型和保留定价模型。请联系我们的销售团队了解更多信息。",question1:"Jina AI 擅长什么？",question10:"你们的解决方案的许可条款是什么？",question11:"您的服务区域是什么？",question12:"你们提供现场支持吗？",question2:"Jina AI 适用于哪些类型的人工智能？",question3:"您的解决方案可扩展且可投入生产吗？",question4:"哪些行业可以从 Jina AI 的解决方案中受益？",question5:"我们如何使用 Jina AI 启动一个项目？",question6:"实施解决方案后您提供哪些支持？",question7:"项目的典型持续时间是多长？",question8:"Jina AI 如何保护我的数据？",question9:"你们的服务的定价结构是怎样的？"},L="常见问题",R={text:"江湖再见。",toggle_btn:"下次访问时保持此面板打开",warning_message:"当您访问 jina.ai 时，此面板将自动打开。您需要关闭它才能查看网站内容。启用此设置吗？",warning_title:"启动时显示"},M={description:"调优特定领域数据的大模型向量以获得更好的搜索质量",intro:"你的公司、你的数据、你的模型"},q={description:"为您的企业提供本地调优解决方案"},C={api_key:"输入您的 API 密钥。",back:"后退",base_model_selected:"选择底座模型",click_start:"同意条款并开始微调。",confirm_title:"确认微调工作",confirm_your_email:"重新输入您的电子邮件地址以确认微调工作。更新和下载链接将发送到此电子邮件。",consent0:"我同意根据我的指示生成用于模型微调的合成数据。",consent1:"我承认最终模型和合成数据将在 Hugging Face 上公开。",consent2:"我了解此功能处于测试阶段，Jina AI 不提供任何保证。定价和用户体验可能会发生变化。",continue:"继续",cost_1m_token:"每个微调作业消耗 1M 词元。请确保您有足够的词元或充值。您也可以生成新的 API 密钥。每个 API 密钥附带 1M 免费词元。",doc_explain:"描述匹配的文档应该是什么样的。",domain_explain:"详细描述如何使用微调向量模型。这对于生成高质量的合成数据（可提高向量模型的性能）至关重要。",domain_explain2:"有三种方式可以指定您的需求：一般说明、URL 或查询文档描述。请选择一种。",domain_hint:"描述您希望微调的域。",email_not_match:"电子邮件地址不匹配。请验证。",failed_job:"微调请求失败。请参阅下面的原因。",find_on_huggingface:"在 Hugging Face 上查找结果",general_instruction:"或者，一般说明",general_instruction_caption:"提供有关如何使用微调向量的详细描述。",general_instruction_explain:"以自由格式的文本描述您的域名。您可以将其想象为 ChatGPT 中的“提示”。",how_it_works:"了解微调过程。",job_acknowledged:"您的微调作业已排队。作业开始时，您将收到一封电子邮件。整个过程通常需要 20 分钟才能完成。",new_key:"获取新密钥",not_enough_token:"此 API 密钥中词元不足。请充值或使用其他 API 密钥。",placeholder:"汽车保险索赔",preview:"预览",query_doc:"查询文档描述",query_doc_caption:"描述查询是什么样的以及匹配的文档在您的域中是什么样的。",query_explain:"描述查询的样子。",reset:"重来",select_base_model:"选择一个基础向量模型进行微调。",select_base_model_explain:"选择一个底座模型作为微调的起点。通常，base-en 是一个不错的选择，但对于其他语言的任务，请考虑使用双语模型。",start_tuning:"开始微调",url:"或者，网页网址",url_caption:"参考URL中的内容进行微调。",url_explain:"包含您要微调的内容的网页的公共 URL。",use_url:"使用 URL 代替。启用该选项意味着我们将根据该 URL 的页面内容生成合成数据以进行微调。",wait_for_processing:"请稍候，我们正在处理您的请求...",which_domain:"微调域",write_email_explain:"微调需要时间。我们将通过电子邮件告知您微调工作的开始、进度、完成情况和任何问题，以及微调模型和训练数据集的详细信息。"},j={address_beijing:"中国北京",address_berlin:"德国柏林（总部）",address_shenzhen:"中国深圳",address_sunnyvale:"加利福尼亚州桑尼维尔",all_rights_reserved:"版权所有",api_documentation:"API 文档",company:"公司",developers:"为开发者",docs:"文档",enterprise:"为企业",get_api_key:"获取 Jina API 密钥",offices:"办公室",power_users:"为高级用户",privacy:"隐私",privacy_policy:"隐私政策",privacy_settings:"管理 Cookie",security:"安全",sefo:"搜索底座",soc2:"我们符合美国注册会计师协会 (AICPA) 的 SOC 2 Type 1 和 Type 2 标准。",status:"API 状态",status_short:"服务状态",tc:"条款及条件",tc1:"条款"},S="获取 API 密钥",T={stars:"Stars"},J={description:"运用网络知识进行地面陈述",title:"事实核查",usage:"接地使用"},B={about_us:"关于我们",api_docs:"API 文档",api_docs_explain:"为您的AI 编程助手 IDE 或大模型自动生成代码",company:"公司",contact_us:"联系销售",developers_others:"更多开发者工具",enterprise_others:"更多的",for_developers:"为开发者赋能",for_developers_description:"专为开发人员打造的的多模态开源技术栈。",for_enterprise:"为企业赋能",for_enterprise_description:"探索为企业定制的多模态解决方案。",for_power_users:"为高级用户赋能",for_power_users_description:"利用我们多模态工具来提高您的日常生产力。",internship1:"实习生计划",jobs:"加入我们",join_discord:"加入我们的 Discord 社区",logos:"下载Logo",maximize:"⇧1",maximize_btn:"最大化",news:"新闻",open_day:"开放日",open_in_full:"在新窗口中显示所有企业产品",power_users_others:"更多高级用户工具",products:"产品"},E={description:"分享和发现多模态AI应用程序的构建模块"},G={sentence_similarity:"句向量模型",updated_about:"更新了关于"},U={project1:"使用点云信息在 3D 网格数据中实现高精度搜索。",project10:"利用计算机视觉提高政府网站的数字可访问性。",project11:"为一家咨询公司优化财务数据分析的调优大规模语言模型。",project12:"通过调优文本到图片模型以进行风格转换来实现高级营销策略。",project2:"设计了一个基于内容的动画短片搜索引擎。",project3:"通过调优向量模型来提高电子商务转化率。",project4:"为一家商业咨询公司进行及时调整以提高效率。",project5:"为一家领先的游戏企业开创了游戏场景理解和自动标注的先河。",project6:"为一家聊天机器人公司实现了实时输入扩展，增强了用户体验。",project7:"通过在冗长的法律文档中实现高效搜索。",project8:"支持大规模运营的高吞吐量生成艺术服务。",project9:"使用高级语言模型进行流程挖掘和建模。"},z={description:"最先进的多模态推理模型"},O="由于您的 API 密钥中剩余词元不足，因此请求失败，请充值后继续。",D={copy_full_prompt:"复制完整提示词",embedding:"向量模型",how_to_use_meta_prompt:"如何使用",meta_prompt:"使用元提示词进行代码生成",meta_prompt_description:"元提示词可以让大模型（如 ChatGPT 和 Claude）知晓我们所有的API使用方法，从而使代码生成更容易、质量更高。",reranker:"重排器",which_to_go:"哪一个与{_vendor}集成？"},H={answer1:"本科、硕士、博士我们鼓励来自世界各地对研究、工程、营销和销售等领域感兴趣的学生申请。我们还欢迎营销、销售、行政助理等领域的非技术实习机会。我们正在寻找热情的个人，准备与我们一起开拓多模态AI。",answer10:"是的，我们的实习计划提供有竞争力的薪酬。",answer11:"作为 Jina AI 实习生，您将获得从事具有挑战性的项目的实践经验，向行业专家学习，成为充满活力的社区的一部分，并有机会为我们在多模态 AI 领域的开创性工作做出真正的贡献。",answer2:"实习必须在我们位于柏林、北京和深圳的办事处之一现场进行。",answer3:"是的，Jina AI 在签证流程中为成功申请者提供合理的帮助。",answer4:"是的，Jina AI 为实习生在实习期间提供合理的生活费保障。",answer5:"是的，您可以在 Jina AI 实习期间完成硕士论文，这通常适用于德国大学的学生。但是，您必须事先征得您所在大学主管的沟通并同意。请注意，我们不帮助学生寻找顾问。",answer6:"申请过程包括提交您的申请表、简历、表达您的兴趣和动机的求职信以及任何相关的专业链接，例如 GitHub 或 LinkedIn。我们根据候选人在面试中的表现以及他们在大学的表现来评估他们。",answer7:"是的，成功的实习生可能会在实习结束时收到一封由我们首席执行官签署的推荐信。",answer8:"实习的持续时间根据角色和项目而有所不同。然而，它通常为三到六个月。",answer9:"是的，我们欢迎所有学术背景的申请。我们重视您对学习的热情和承诺，就像您以前的经验一样。",question1:"哪些人可以申请 Jina AI 实习项目？",question10:"这是带薪实习吗？",question11:"作为 Jina AI 实习生，我将获得哪些机会？",question2:"实习将在哪里进行？",question3:"Jina AI 是否协助办理签证流程？",question4:"Jina AI 是否为实习生提供任何津贴或福利？",question5:"在 Jina AI 实习期间可以写硕士论文吗？",question6:"申请流程涉及哪些内容？",question7:"Jina AI 实习后有推荐信吗？",question8:"实习期限是多长？",question9:"如果我没有人工智能方面的经验，我可以申请吗？"},N={about_internship_program:"关于实习计划",about_internship_program_desc1:"我们很高兴为有才华的人提供这个独特的机会加入我们充满活力的团队，并为人工智能领域的突破性项目做出贡献。该实习旨在为您提供宝贵的实践经验、指导和接触正在塑造人工智能未来的尖端技术。",about_internship_program_desc2:"在 Jina AI，我们深知培养和利用年轻人才的重要性。我们认识到实习生带来了新的视角、热情和创造力，用新的想法和方法激励我们的团队。通过提供实习机会，我们的目标是促进人工智能行业未来领导者的成长，同时在支持性和挑战性的环境中为他们提供实际经验。",alumni:"校友",alumni_network:"我们蓬勃发展的校友网络",application:"应用",application_desc:"与 Jina AI 一起踏上变革之旅。我们全面的实习计划邀请所有渴望塑造人工智能未来的充满激情的伙伴。加入我们，获得现实世界的经验，从事具有挑战性的项目，并与人工智能行业中一些最聪明的人做同事。",apply:"现在申请",autumn:"秋季",description:"全球招募学生：研究、工程、营销、销售等专业的实习。",dev_rel_intern:"开发者关系实习生",enthusiastic:"热情的",explore_stories_from_our_interns:"探索我们实习生的故事",explore_stories_from_our_interns1:"从我们的实习生的旅程中获得灵感",innovative:"创新的",intern_work1:"微调大模型模型以实现更好的句向量模型",intern_work2:"探索检索增强生成(RAG)算法的潜力",intern_work3:"发表了一篇关于句子向量的论文",intern_work4:"为团队注入源源不断的年轻活力",intern_work5:"压缩大模型的基准量化技术",intern_work6:"为 PromptPerfect 创建和推广引人注目的活动",intern_work7:"快速开发和改进 JinaColBERT V2",recruiting_and_administrative_intern:"招聘及行政实习生",researcher_intern:"实习研究员",self_motivated:"自我激励",software_engineer_intern:"软件工程师实习生",spring:"春季",submit_application:"与 Jina AI 一起开始你的冒险",subtitle:"我们的全日制实习计划通过精心设计的广泛范围的实习项目提供实践工作经验。",subtitle1:"全球范围内招募学生：研究、工程、营销、销售等领域的实习生，共同开创多模态AI。",summer:"夏季",title:"实习生计划",who_do_we_look_for:"我们要找谁？",who_do_we_look_for_desc:"我们重视多样性，并鼓励来自不同背景和背景的申请人加入我们的实习计划。多个部门提供实习机会，包括工程、设计、产品管理、销售和客户管理、营销和社区管理。",winter:"冬季"},F={description:"将一个本地项目部署为云服务。极其简单，没有令人不快的意外。"},W={description:"开源的大模型的实验性调优器"},K={description:"在云端构建多模态AI应用"},Q={description:"更多模态、更长记忆、更低成本",example_1:"你是谁？",example_2:"我是 Jina AI 制作的大模型聊天服务"},X={add:"添加密钥",add_key_explain:"向您的帐户添加另一个 API 密钥。可以随时管理、充值或删除添加的密钥。",add_key_labels_explain:"给您的钥匙添加一些标签，以帮助您有效地管理多个钥匙。",add_key_labels_input_explain:"允许多个标签。使用“Enter”键添加新标签。",add_shared_key:"添加到我的钥匙",add_success:"已成功添加密钥 {_key}。",advance_settings:"打开高级设置",advanced_feature:"高级功能仅适用于高级密钥。",auto_recharge_enable_success:"已成功为密钥 {_key} 开启自动充值。",auto_recharge_title:"启用自动充值吗？",auto_reminder:"低余额提醒",auto_reminder_cancel_message:"您确实要取消此密钥的自动提醒吗？",auto_reminder_cancel_title:"取消自动提醒",auto_reminder_description:"当您的词元余额低于设定的阈值时，您会收到自动电子邮件提醒。您最多可以配置三个阈值。",auto_reminder_email:"提醒电子邮件地址",auto_reminder_info:"当词元余额低于 {_threshold} 时，邮件提醒将发送到 {_email}。",auto_reminder_threshold:"提醒阈值",auto_reminder_threshold_error:"阈值必须介于 1 到 1T 之间。",auto_reminder_toggle:"开关自动提醒，请注意只有高级密钥才能启用此功能。",available_resources:"可用词元",balance:"可用词元",balance_primary_key:"主密钥余额",cancel:"取消",confirm:"确认",copy:"复制密钥",copy_share_link:"复制链接",customized_tags:"给你的钥匙贴上标签",description:"管理所有 Jina AI 服务的 API 密钥 — Embeddings、Reader、Reranker 等。",do_it_later:"稍后再做",email:"电子邮件",existing_key:"现有密钥",filter_by:"按密钥过滤",free_key:"免费密钥",generate_new_key:"生成新密钥",generate_new_key_tooltip:"生成一个新的 API 密钥，其中的余额为空。您可以稍后充值。",generate_success:"成功生成新密钥 {_key}。",get_free_key:"创建 API 密钥",ignore:"忽略",invalid_email:"邮箱无效",invalid_key:"无效密钥",is_primary:"您的主密钥。登录后您可以更改它。",key_labels:"标签",labels_updated:"已成功更新标签。",last_used:"最后使用",last_used_at:"上次活动",login:"登录",login_explain:"管理多个 API 密钥并跟踪使用情况——全部在一个帐户中完成。",login_explain_long:"登录以安全地存储和管理您的 API 密钥。跟踪使用历史记录、管理多个密钥，并且永远不会失去对您的凭据的访问权限。",login_required:"请先登录，然后添加共享密钥。",login_via:"通过 {_provider} 登录",logout:"登出",logout_message:"您的 API 密钥安全地存储在您的帐户中。您可以随时登录来管理它们。",logout_success:"已成功登出",no_key_title:"需要 API 密钥吗？",no_key_with_login:"您尚未创建 API 密钥。立即生成一个并获取免费词元以开始使用。",no_key_without_login:"已有帐户？登录以访问您的 API 密钥，或单击“{_button}”创建一个新密钥。",no_transferable_keys:"没有其他可供转移的密钥，请先添加新密钥。",normal_key:"普通密钥",ok:"好的",primary_key:"设置为主密钥",primary_key_set:"成功将 {_apiKey} 设置为您的主密钥。",primary_key_set_caption:"此密钥将用于 jina.ai 上的所有演示、示例和沙盒实验场。",purchase:"购买词元",readonly:"限制自己对此密钥的只读访问权限。",readonly_key:"只读密钥",recharge_threshold_confirm_message:"您确定要将自动充值阈值更改为{_threshold}个词元吗？",recharge_threshold_confirm_title:"更改自动充值阈值",remove:"移除密钥",remove_explain:"从列表中移除密钥不会影响依赖该密钥的软件和服务,或存储该密钥的其他用户。该密钥仍然有效，并且可以随时重新添加。",remove_message:"您确定要移除密钥吗？此密钥仍然有效，可以随时重新添加。",remove_primary_key:"移除当前主密钥之前，请将另一个密钥设置为主密钥。",remove_success:"已成功移除密钥 {_key}。",remove_title:"移除密钥",revert_changes:"恢复更改",revoke:"销毁密钥",revoke_error:"您输入的密钥与您尝试销毁的密钥不匹配。",revoke_explain:"销毁密钥将立即对所有储存它的用户永久失效，并且该密钥所有剩余词元余额和关联资产将永久不可用。此操作无法撤消。",revoke_label:"请在下面输入此密钥以确认销毁",revoke_message:"您确定要销毁此密钥吗？一旦销毁，此密钥将对所有存储它的用户永久失效。该密钥所有剩余词元余额和关联资产将永久不可用。此操作无法撤消。",revoke_success:"已成功销毁密钥 {_key}。",revoke_title:"销毁密钥",save:"保存",save_as:"保存为",settings:"设置",share:"分享密钥",share_key_confirm_message:`接收者将能够查看、管理和充值此密钥的余额。您将保留相同的权限。
请注意，该链接将在 24 小时后过期。`,share_key_confirm_title:"分享 API 密钥",share_key_expired_at:"分享链接将于{_time}过期！",share_key_expired_message:"密钥的分享链接已过期，请让密钥拥有者重新分享。",share_key_expired_title:"分享链接已过期",share_key_message:"{_user} 已与您共享 API 密钥。添加它以管理密钥及其余额。",share_link_copied:"分享链接已复制",shared_from:"密钥由 {_user} 共享",shared_key:"共享密钥",subscribed_key:"高级密钥",title:"Jina 搜索底座 API",to_dashboard:"管理密钥",top_up:"充值",total_keys:"密钥数",transfer_before_revoke:"在销毁密钥之前转移剩余的已付费词元余额。",transfer_explain:"将您全部剩余的付费词元余额转移到另一个密钥，以获得更大的灵活性并增强管理资源的安全性。",transfer_label:"转移到",transfer_message:"您确定要将全部剩余的付费词元余额 {_tokens} 从 {_source} 转移到 {_target} 吗？",transfer_success:"已成功将付费词元余额从 {_source} 转移到 {_target}。",transfer_title:"转移词元",update_label:"更新标签",usage_history:"使用历史记录",usage_summary:"过去 7 天：{_usage} 个词元"},Y={GlobalQA:{description:"在任意页面上按“/”键即可打开提问。输入您的查询并按“Enter”得到页面内容相关的答案。此功能由 PromptPerfect 提供支持。",title:"页上RAG"},Recommender:{description:"使用“Shift+2”打开任何新闻页面上的推荐框。选择重排模型以发现与该新闻页面相关的前 5 篇文章。此功能由我们的 Reranker API 提供实时支持。",title:"相关文章"},SceneXplainTooltip:{description:"将鼠标悬停在新闻页面或我们的新闻室目录中的任何图片上，以显示该图片的描述。描述由 SceneXplain 预先计算并向量化在图片的 ALT 属性中。",title:"图片标注"},explain:"探索我们网站上的隐藏功能"},V={also_available_on:"也可在应用市场上使用",also_available_on1:"通过应用市场一键部署到您的企业云上",ask_how_your_question:"请描述您的问题",autotune:"自微调",avatar:"头像生成器",badge:{"clip-v2":"clip-v2 发布！","readerlm-v2":"ReaderLM-v2 发布！",v2:"第二版已发布！",v3:"v3 发布！"},browser_info_title:"浏览器信息",build_js:"使用 JavaScript 开发",build_python:"使用 Python 开发",ccbync:"此模型为 CC BY-NC 4.0 许可。通过 API 或我们的官方 AWS/Azure 映像使用它；或联系销售人员进行本地部署。",checkout_our_solution_for_you:"了解我们为您量身定制的解决方案",classifier:"分类器",coming_soon:"敬请期待",contact_sales:"联系我们",copied_to_clipboard:"已复制到剪贴板",copy:"复制",developers:"开发者",developers_desc:"通过尖端的云原生技术和开源基础设施，释放多模态AI的全部力量。",download_pdf:"下载 PDF",embedding:"向量",embedding_desc1:"适用于搜索、RAG、智能体应用程序的性能最佳的多模态多语言长上下文向量模型。",embedding_paper_desc:"Jina Embeddings 构成了一组高性能文本向量模型，擅长将各种文本输入转换为数字表示，从而捕获文本的语义本质。虽然这些模型并非专门为文本生成而设计，但它们在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了 Jina Embeddings 的开发，从创建高质量的成对和三元组数据集开始。它强调了数据清理在数据集准备中的关键作用，深入了解了模型训练过程，并使用大规模文本向量基准 (MTEB) 进行了全面的性能评估。",embedding_paper_title:"Jina Embeddings：一套新颖的高性能文本向量模型",embeddings:"向量模型",enterprise:"企业",enterprise_desc:"通过可扩展、安全和定制的多模态AI解决方案促进您的业务。",enterprise_desc_v2:"用我们世界一流的向量模型来改进您的搜索和 RAG 系统。从免费试用开始！",enterprise_desc_v3:"我们的前沿模型构成了高质量企业搜索和 RAG 系统的搜索底座。",error:"出现了问题：{message}",find_your_portal:"探索您的专属传送门",finding_faq:"根据以下常见问题解答知识生成答案",for:"为",for_better_search:"为了更好的搜索",for_developers:"为开发者",for_enterprise:"为企业",for_power_users:"为高级用户",get_api_now:"API",get_started:"开始使用",go_to_product_homepage:"转至产品主页",grounding:"接地",how_to:"如何",include_experiment:"在解决方案中提及我们还在实验中和已归档的项目。",join_community:"社区",key_manager:"管理 API 密钥",learn_more_embeddings:"了解向量模型",learn_more_reader:"详细了解读取器",learn_more_reranker:"了解重排器",llm:"大模型向量模型",llm_desc:"我们提供了一系列高性能文本向量模型，拥有 3500 万到 60 亿个参数。它们非常适合增强神经搜索、重排器、句子相似性、推荐等。准备好提升您的 AI 体验！",mentioned_products:"提及产品：",mmstack:"多模态技术栈",mmstack_desc:"多年来，我们开发了各种开源软件，帮助开发人员构建更好的 GenAI 和更快地搜索应用程序。",models:"模型",more:"更多的",multimodal:"多模态",multimodal_ai:"多模态AI",new:"新的",newsroom:"新闻",num_publications:"共计 {_total} 篇出版物。","on-prem-deploy":"私有化部署","on-premises":"本地",opensource:"开源",our_customer:"我们的客户",our_customer_explain:"大大小小的企业都信任 Jina AI 的搜索底座技术来构建他们的工具和产品——您也可以信任我们。",our_publications:"我们的论文",parameters:"参数",podcast:"播客",power_users:"高级用户",power_users_desc:"自动提示词工程，提高您的日常工作效率。",powered_by_promptperfect:"由 PromptPerfect 的“提示词优化”和“提示即服务”功能提供支持",pricing:"价格表",proposing_solution:"基于 Jina AI 产品线的构思解决方案...",read_more:"更多新闻",reader:"读取器",refresh_page:"刷新",require_full_question:"请更详细地描述您的问题。",reranker:"重排器",researcher_desc:"了解我们的前沿搜索模型是如何从头开始训练的，查看我们的最新出版物。在 EMNLP、SIGIR、ICLR、NeurIPS 和 ICML 与我们的团队见面！",researchers:"研究人员",sdk:"软件开发工具包",sdk_desc:"想要使用 PromptPerfect、SceneXplain、BestBanner、JinaChat、Rationale API 构建高级 AIGC 应用程序？我们为您服务！尝试我们易于使用的 SDK，几分钟内即可开始使用。",sdk_docs:"读取文档",sdk_example:"例子",search_foundation:"搜索底座",source_code:"源代码",starter_kit:"新人包",supercharged1:"如虎添翼！",tokenizer:"切分器",trusted_by:"我们值得信赖",try_it_for_free:"立即开始——无需信用卡或注册！",try_our_saas:"尝试我们托管的的云原生推理方案，它是OpenAI API的1:1替代品。",version_notify:"您正在查看该网站的旧版本。如需最新功能，请转到 {_link}",view_browser_info:"查看浏览器信息",your_portal_to:"邀您通往",your_search_foundation1:"您的搜索底座"},$={description:"使用 Jina 和 FastAPI 制作生产级别的 Langchain 应用程序"},Z={description:"关于Jina AI产品和服务的法律信息、服务条款、隐私政策和其他重要文件。",download_type1:"下载 SOC 2 I型证明",download_type2:"下载 SOC 2 II型证明",request_audit:"索取审计报告",title:"法律信息"},ee={api:"LLM-as-SERP API",description:"大型语言模型作为搜索结果页面",faq_v1:{answer1:"SERP 代表搜索引擎结果页面，是搜索引擎响应用户查询而显示的页面。它包含结果列表，通常还包含其他信息，例如 URL、摘要、域名。",answer2:"是的，LLM-as-SERP 使用大型语言模型来生成搜索结果。",answer3:"出现幻觉。",answer4:"它们可能在模型的训练数据中。",answer5:"是的，您可以使用 LLM-as-SERP API 来生成搜索结果。点击下面的按钮。",answer6:"我们在试验深度搜索的搜索基础时开发了它。点击按钮读取我们的博客文章。",answer7:"是的，代码是开源的，可以在 GitHub 上获取。",question1:"什么是 SERP？",question2:"那么所有搜索结果都是由 LLM 生成的吗？",question3:"为什么搜索结果中有些 URL 损坏并导致 404 页面？",question4:"那么为什么有些 URL 是真实的并且会引导至实际的页面？",question5:"我可以通过 API 调用它吗？",question6:"但是如果所有搜索结果都是假的，那么这个演示或 API 有什么意义呢？",question7:"它是开源的吗？",title:"LLM-as-SERP 常见问题"},okay_but_why:"玩儿归玩儿，它有什么用？",parameters:{auth_token:"@:tokenizer.parameters.auth_token",auth_token_explain:"LLM 作为 SERP API 可免费使用。通过提供您的 API 密钥，您可以访问更高的速率限制，并且不会向您的密钥收费。",country:"首选国家",country_explain:"用于搜索的国家/地区。这是两个字母的国家/地区代码。",language:"首选语言",language_explain:"搜索所用的语言。这是一个双字母的语言代码。",location:"首选地点",location_explain:"您希望搜索查询源自何处。建议指定城市级别的位置，以模拟真实用户的搜索。",number:"结果数量",number_explain:"返回的最大结果数。使用 num 可能会导致延迟，和/或阻止包含专门的结果类型。结果不保证具有 num 中指定的结果数。",page:"分页",page_explain:"结果偏移量。它会跳过给定数量的结果。它用于分页。",query:"询问",query_explain:"您要搜索的查询。您可以使用常规搜索中使用的任何内容。例如 inurl:、site:、intitle:。"},query_label:"您的查询",title:"LLM 作为 SERP"},ne={api:"Jina AI的API",browse_catalog:"浏览目录",contact_sales_about_it:"联系销售人员了解详情",deploy_it_on:"部署于",description:"从第一天起，我们就一直在推动搜索模型的发展。请查看下面的模型演变过程 — 将鼠标悬停或单击即可发现每个里程碑。",find_on_hf:"在 HuggingFace 上找到它",search_for:"在我们的网站上搜索",search_models:"按模型名称过滤",title:"我们的搜索底座模型",use_it_via:"通过使用"},ae={back_to_models:"返回模型",comparison:{btn:"比较",select_models:"选择要比较的模型"},error:"加载模型失败",input_type:{"3d":"3D",audio:"声音的",code:"代码",document:"文档",graph:"图形",image:"图片","multi-vector":"多向量",other:"其他",ranking:"排名",tabular:"表格",text:"文本","text (code)":"文本（代码）","text (document)":"文本（文档）","text (html)":"文本（HTML）","text (json)":"文本（JSON）","text (markdown)":"文本（Markdown）","text (query)":"文本（查询）",timeseries:"时间序列",vector:"向量",video:"视频"},loading:"正在加载模型详细信息...",metadata:{api_link:"Jina API",arxiv:"ArXiv 论文",aws_link:"亚马逊云",azure_link:"微软云",deprecated_by:"已弃用",gcp_link:"谷歌云",huggingface_link:"抱抱脸",input_type:"输入",license:"许可证",license_link:"商业许可证",output_type:"输出","reader-api_link":"Jina API",related_models:"相关模型",release_blog:"发行说明",release_date:"发布日期"},search:{no_results:"未找到与“{query}”匹配的模型",placeholder:"按名称、标签或类型搜索..."},sections:{availability:"可用性",blogs:"提及此模型的博客",external_links:"外部链接和资源",guidance:{"ReaderLM-v2":"该模型可通过 Google Colab 笔记本访问，该笔记本演示了 HTML 到 Markdown 的转换、JSON 提取和指令遵循。对于 HTML 到 Markdown 任务，用户可以输入没有前缀指令的原始 HTML，而 JSON 提取则需要特定的架构格式。create_prompt 辅助函数有助于轻松为这两个任务创建提示。虽然该模型可以在 Colab 的免费 T4 GPU 层上运行（需要 vllm 和 triton），但如果不支持 bfloat16 或 flash Attention 2，则存在局限性。建议将 RTX 3090/4090 用于生产用途。该模型将在 AWS SageMaker、Azure 和 GCP 市场上提供，根据 CC BY-NC 4.0 许可用于非商业用途。","jina-clip-v1":"为了有效部署 Jina CLIP v1，团队应同时考虑其功能和资源需求。该模型以 224x224 像素图块的形式处理图片，每个图块消耗 1,000 个词元的处理能力。为了获得最佳性能，请实施有效的图片预处理以匹配这些尺寸。虽然该模型在短文本和长文本处理方面都表现出色，但目前仅支持英语输入。团队应仔细考虑词元的使用：文本每个单词大约需要 1.1 个词元，而图片以图块的形式处理（例如，750x500 像素的图片需要 12 个图块，消耗 12,000 个词元）。该模型可通过 Jina Embeddings API 和 Apache 2.0 许可下的 Hugging Face 上的开源版本获得，提供灵活的部署选项。对于生产环境，请考虑使用 AWS Marketplace 或 Azure 部署选项，它们提供优化的基础设施设置。","jina-clip-v2":"为了实现最佳部署，用户应考虑几个关键因素。该模型需要支持 CUDA 的硬件才能高效处理，内存需求会根据批次大小和图片分辨率进行调整。为了优化 API 成本和性能，请在处理之前将图片大小调整为 512x512 像素 - 较大的图片会自动平铺，从而增加词元使用量和处理时间。该模型擅长跨语言匹配带有描述性文本的图片，但可能难以处理抽象概念或高度专业化的特定领域内容。它对于电子商务产品搜索、内容推荐系统和视觉搜索应用程序特别有效，但可能不适合需要细粒度视觉细节分析或高度专业化领域专业知识的任务。使用 Matryoshka 表示功能时，请考虑降维和性能之间的权衡 - 虽然 64 维向量保持了强大的性能，但关键应用程序可能会受益于更高的维度。","jina-colbert-v1-en":"为了有效部署 Jina-ColBERT-v1-en，团队应考虑几个实际方面。该模型需要具有 CUDA 功能的 GPU 才能获得最佳性能，尽管开发过程中可以使用 CPU 推理。对于文档处理，8,192 个词元限制相当于大约 6,000 个单词，使其适用于大多数文档类型，包括学术论文、技术文档和长篇内容。团队应实施有效的文档预处理来处理词元限制，并考虑对大规模索引进行批处理。虽然该模型擅长处理英语内容，但它并非为多语言应用程序或跨语言检索而设计的。对于生产部署，请实施适当的文档分块策略，并考虑使用向量相似性索引（如 FAISS）进行有效检索。当使用 RAGatouille 等框架将该模型集成到 RAG 管道中时，该模型特别有效，这简化了复杂检索模式的实现。","jina-colbert-v2":"为了有效部署 Jina-ColBERT-v2，团队应考虑几个实际方面。该模型需要支持 CUDA 的硬件才能获得最佳性能，并支持最多 8,192 个词元（可扩展至 12,288 个）的文档长度，同时将查询限制为 32 个词元。对于生产部署，该模型可通过 Jina Search Foundation API、AWS 市场和 Azure 获得，非商业版本可通过 Hugging Face 访问。在实施时，团队应指定他们是向量查询还是文档，因为该模型使用非对称编码。该模型并非专为在没有适当索引的情况下实时处理极大的文档集合而设计，虽然它在多语言检索方面表现出色，但与针对这些特定领域进行微调的模型相比，它在专门的特定领域任务上的性能可能会略低。","jina-embedding-b-en-v1":"为了实现最佳部署，该模型需要具有 CUDA 功能的 GPU，尽管其适中的大小允许在标准硬件上进行高效推理。该模型接受长度最多为 512 个词元的输入序列，特别适合一致、可靠的向量生成至关重要的生产环境。它在英语内容上表现最佳，是语义搜索、文档相似性比较和内容推荐系统等应用的理想选择。团队应考虑在新项目中使用较新的 v2 或 v3 版本，因为它们提供了更好的性能和更广泛的语言支持。不建议将该模型用于需要多语言理解或一般英语文本之外的专门领域知识的任务。","jina-embeddings-v2-base-code":"为了有效部署 Jina Embeddings v2 基础代码，团队应考虑几个实际方面。该模型与 MongoDB、Qdrant 和 Weaviate 等流行的矢量数据库无缝集成，从而可以轻松构建可扩展的代码搜索系统。为了获得最佳性能，请实施适当的代码预处理以处理 8,192 个词元限制，这通常可以容纳大多数函数和类定义。虽然该模型支持 30 种编程语言，但它在六种核心语言中表现出最强的性能：Python、JavaScript、Java、PHP、Go 和 Ruby。团队应考虑使用批处理进行大规模代码索引以优化性能。该模型的 RAG 兼容性使其对于自动文档生成和代码理解任务特别有效，但团队应该为非常大的代码库实施适当的分块策略。对于生产部署，请考虑使用 AWS SageMaker 端点进行托管推理，并实施适当的缓存策略以优化查询性能。","jina-embeddings-v2-base-de":"为了有效部署 Jina Embeddings v2 Base German，组织应考虑几个实际方面。该模型与 MongoDB、Qdrant 和 Weaviate 等流行的矢量数据库无缝集成，使构建可扩展的双语搜索系统变得简单。为了获得最佳性能，请实施适当的文本预处理以有效处理 8,192 个词元限制 - 这通常可容纳大约 15-20 页文本。虽然该模型在德语和英语内容方面都表现出色，但在用于查询和文档语言可能不同的跨语言检索任务时尤其有效。组织应考虑为经常访问的内容实施缓存策略，并使用批处理进行大规模文档索引。该模型的 AWS SageMaker 集成提供了一条可靠的生产部署路径，但团队应该监控词元使用情况并为高流量应用程序实施适当的速率限制。当将该模型用于 RAG 应用程序时，请考虑实施语言检测以根据输入语言优化提示构造。","jina-embeddings-v2-base-en":"为了有效部署 Jina Embeddings v2 Base English，团队应考虑几个实际方面。该模型需要支持 CUDA 的硬件才能获得最佳性能，但其高效的架构意味着它可以在消费级 GPU 上运行。它可通过多种渠道获得：直接从 Hugging Face 下载、AWS Marketplace 部署或带有 100 万个免费词元的 Jina AI API。对于生产部署，us-east-1 区域中的 AWS SageMaker 提供了最具可扩展性的解决方案。该模型擅长通用文本分析，但对于未经微调的高度专业化的科学术语或领域特定术语，可能不是最佳选择。处理长文档时，请考虑将它们分解为有意义的语义块，而不是任意拆分以保持上下文完整性。为获得最佳结果，请实施适当的文本预处理并确保输入数据干净、格式良好。","jina-embeddings-v2-base-es":"为了有效利用该模型，组织应确保能够访问支持 CUDA 的 GPU 基础架构以获得最佳性能。该模型与主要的矢量数据库和 RAG 框架（包括 MongoDB、Qdrant、Weaviate 和 Haystack）无缝集成，使其可轻松部署到生产环境中。它在双语文档搜索、内容推荐系统和跨语言文档分析等应用中表现出色。虽然该模型表现出色，但它特别针对西班牙语-英语双语场景进行了优化，可能不是单语应用或涉及其他语言对的场景的最佳选择。为了获得最佳效果，输入文本应以西班牙语或英语正确格式化，但该模型可以有效处理混合语言内容。该模型支持针对特定领域的应用程序进行微调，但应仔细考虑训练数据的质量和分布。","jina-embeddings-v2-base-zh":"该模型需要 322MB 的存储空间，可通过多种渠道部署，包括 AWS SageMaker（us-east-1 区域）和 Jina AI API。虽然 GPU 加速不是强制性的，但它可以显著提高生产工作负载的处理速度。该模型在文档分析、多语言搜索和跨语言信息检索等各种应用中表现出色，但用户应注意，它专门针对中英双语场景进行了优化。为了获得最佳效果，输入文本应正确分段，虽然该模型最多可以处理 8,192 个词元，但建议将极长的文档分解为具有语义意义的块以获得更好的性能。该模型可能不适合需要实时处理非常短的文本的任务，在这些任务中，低延迟的专用模型可能更合适。","jina-embeddings-v3":"为了有效部署 Jina Embeddings v3，团队应考虑其特定用例以选择适当的任务适配器：搜索应用程序使用 retrieval.query 和 retrieval.passage，聚类任务使用分离，分类使用分类，语义相似性使用文本匹配。该模型需要具有 CUDA 功能的硬件才能获得最佳性能，但其高效的架构意味着它所需的 GPU 内存比更大的替代方案少得多。对于生产部署，AWS SageMaker 集成提供了一条简化的可扩展性路径。该模型在多语言应用程序中表现出色，但对于资源匮乏的语言可能需要额外的评估。虽然它支持多达 8,192 个词元的长文档，但对于非常长的文本，使用后期分块功能可实现最佳性能。团队应避免将该模型用于需要实时生成或复杂推理的任务 - 它是为向量和检索而设计的，而不是文本生成或直接问答。","jina-reranker-v1-base-en":"该模型需要支持 CUDA 的硬件才能实现最佳性能，并且可通过 API 端点和 AWS SageMaker 部署选项访问。虽然它可以处理极长的序列，但用户应该考虑上下文长度和处理时间之间的权衡——模型的延迟会随着文档的延长而显着增加，从 256 个词元的 156 毫秒到 4096 个词元的 7068 毫秒（512 个词元查询）。对于生产部署，建议实施两阶段管道，其中向量搜索提供重新排名的初始候选。该模型专门针对英语内容进行了优化，在多语言或代码密集型文档上可能无法达到最佳效果。与 RAG 系统集成时，用户应根据其延迟要求仔细调整发送以进行重新排名的文档数量，100-200 个文档通常可以在质量和性能之间提供良好的平衡。","jina-reranker-v1-tiny-en":"为了有效部署此模型，组织应优先考虑处理速度和资源效率是关键考虑因素的场景。该模型特别适合边缘计算部署、移动应用程序和对延迟要求严格的高吞吐量搜索系统。虽然它在大多数重新排名任务中表现非常出色，但需要注意的是，对于需要绝对最高排名精度的应用程序，基本模型可能仍然是首选。该模型需要具有 CUDA 功能的 GPU 基础设施才能获得最佳性能，但其高效的架构意味着它可以在功能较弱的硬件上有效运行，而其大型同类产品则不然。对于部署，该模型与主要的矢量数据库和 RAG 框架无缝集成，并且可通过 Reranker API 和 AWS SageMaker 获得。在针对特定域进行微调时，用户应仔细平衡训练数据质量和模型的紧凑架构，以保持其性能特征。","jina-reranker-v1-turbo-en":"该模型需要支持 CUDA 的硬件才能实现最佳性能，并且可以通过 AWS SageMaker 部署或通过 API 端点访问。对于生产部署，组织应实施两阶段管道，其中向量搜索提供重新排名的初始候选。虽然该模型支持 8,192 个词元，但用户应考虑较长序列的延迟影响——处理时间会随着文档长度的增加而增加。大多数应用程序的最佳点是每个查询重新排名 100-200 个候选，以平衡质量和速度。该模型专门针对英语内容进行了优化，在多语言文档上可能无法达到最佳效果。内存要求明显低于基本模型，通常只需要 150MB 的 GPU 内存，而 550MB 则需要，使其适合部署在较小的实例上，并在云环境中节省大量成本。","jina-reranker-v2-base-multilingual":"为了实现最佳部署，该模型需要具有 CUDA 功能的 GPU，并且可以通过多种渠道访问，包括 Reranker API、主要的 RAG 框架（如 Haystack 和 LangChain），或通过云市场私下部署。该模型在需要跨语言障碍和数据类型进行精确理解的场景中表现出色，使其成为处理多语言内容、API 文档或代码存储库的全球企业的理想选择。其广泛的 524,288 个词元上下文窗口支持一次性处理大型文档或整个代码库。当团队需要提高跨语言搜索准确性、需要代理 RAG 系统的函数调用功能或想要改进跨多语言代码库的代码搜索功能时，应考虑使用此模型。该模型与矢量搜索系统结合使用时特别有效，可以显著提高检索到的文档的最终排名。","reader-lm-05b":"为了有效部署 Reader LM 0.5B，组织应确保其基础设施能够满足模型的 CUDA 要求，尽管其高效的架构意味着它可以在消费级 GPU 上运行。该模型最适合原始 HTML 输入，不需要特殊的前缀或指令。为获得最佳性能，请实施提供的重复检测机制以防止在输出生成中出现潜在的词元循环。虽然该模型支持多种语言和各种 HTML 结构，但它专为内容提取和 markdown 转换而设计 - 不应用于文本生成、摘要或直接问答等任务。该模型可通过 AWS SageMaker 进行生产部署，并提供 Google Colab 笔记本供测试和实验。团队应该注意，虽然该模型可以处理多达 256K 个词元的超长文档，但处理如此大的输入可能需要额外的内存管理策略。","reader-lm-15b":"为了有效部署 Reader LM 1.5B，组织应专注于涉及复杂 HTML 文档处理的场景，其中准确性和效率至关重要。该模型需要具有 CUDA 功能的 GPU 基础设施才能获得最佳性能，但其高效的架构意味着与更大的替代方案相比，它可以在更适中的硬件上有效运行。对于生产部署，该模型可通过 AWS SageMaker 和 Azure Marketplace 获得，提供灵活的集成选项。虽然该模型在 HTML 到 markdown 的转换方面表现出色，但需要注意的是，它专门针对此任务进行了优化，可能不适合通用文本生成或其他 NLP 任务。在处理极长的文档（接近 512K 个词元）时，用户应注意性能可能会下降，因为这超出了模型的训练参数。为获得最佳结果，请实施提供的重复检测机制，并考虑在推理过程中使用对比搜索以保持输出质量。",title:"最佳实践"},image_size:"输入图片大小",language:"语言支持",methods:{"ReaderLM-v2":"ReaderLM-v2 以 Qwen2.5-1.5B-Instruction 为基础，其训练涉及 html-markdown-1m 数据集，其中包含一百万个 HTML 文档，平均每个文档有 56,000 个词元。训练过程包括：1) 使用 ring-zag 注意力和 RoPE 进行长上下文预训练，将上下文从 32K 扩展到 256K 个词元，2) 使用精炼数据集进行监督微调，3) 直接偏好优化以实现输出对齐，以及 4) 自我游戏强化调整。数据准备遵循由 Qwen2.5-32B-Instruction 提供支持的三步流程（草稿-精炼-批评），使用针对特定任务训练的专用模型，然后通过线性参数插值进行合并。","jina-clip-v1":"该模型的架构代表了多模态 AI 设计的重大创新，将经过调整的 Jina BERT v2 文本编码器与北京人工智能研究院的尖端 EVA-02 图片编码器相结合。文本编码器支持最多 12,288 个词元的序列 - 比原始 CLIP 的 77 个词元限制长 100 多倍 - 而图片编码器可以高效处理 16 个补丁词元。训练过程遵循一种新颖的三步方法：首先，通过交错文本对训练对齐图片-标题对，同时保持文本理解；其次，结合 AI 生成的较长的图片文本描述；最后，使用硬负文本三元组来增强语义区分能力。这种独特的训练方法使模型能够在短标题和详细文本描述中保持高性能，同时保留强大的视觉理解力。","jina-clip-v2":"Jina CLIP v2 的核心是采用复杂的双编码器架构，将 Jina XLM-RoBERTa 文本编码器（561M 参数）与 EVA02-L14 视觉编码器（304M 参数）相结合。文本编码器使用 696,320 个词元的海量上下文窗口处理 89 种语言的内容，而视觉编码器则处理高达 512x512 像素的高分辨率图片。该模型引入了创新的 Matryoshka 表示学习，可在保持性能的同时实现动态向量维度从 1024 维到 64 维的调整。该架构通过各自的编码器处理文本和图片，将它们投射到共享语义空间中，无论其原始模态或语言如何，相似的概念都可以对齐。","jina-colbert-v1-en":"该模型采用了创新的后期交互架构，从根本上改变了文档检索的工作方式。它不是一次性比较整个文档，而是使用 ColBERT 方法的改编版本，独立处理查询和文档，直到最终匹配阶段。该架构结合了两个关键组件：一个文档编码器，可处理多达 8,192 个词元（比标准Transformer长 16 倍以上）的文本，以及一个查询编码器，可创建精确的词元级表示。查询和文档中的每个词元都有自己的 128 维向量，从而保留了单向量模型中会丢失的细粒度语义信息。然后，后期交互机制可以在查询和文档之间实现高效的逐个词元匹配，使用最大池化和求和操作来计算最终相关性分数，而无需进行昂贵的全部比较。","jina-colbert-v2":"该模型以 ColBERT 架构为基础，引入了一种复杂的后期交互机制，从根本上改变了查询和文档的匹配方式。其核心是使用经过修改的 XLM-RoBERTa 主干，具有 5.6 亿个参数，通过旋转位置向量增强，并通过闪存注意进行优化。训练过程涉及两个关键阶段：使用来自各种语言的各种弱监督数据进行初始预训练，然后使用词元三元组数据进行微调和监督蒸馏。这种方法的独特之处在于实现了 Matryoshka 表示学习，这使模型能够从单个训练过程中生成多个维度（128、96 或 64）的向量，从而允许动态存储优化而无需重新训练。","jina-embedding-b-en-v1":"该模型采用基于 T5 编码器的架构，并通过均值池化增强来生成固定长度的表示。该模型在精心策划的 Linnaeus-Clean 数据集上进行训练，该数据集包含从最初的 16 亿对句子中筛选出的 3.85 亿对高质量句子对，该模型经历了两个阶段的训练过程。第一阶段利用对比学习对文本对进行 InfoNCE 损失，而第二阶段则采用三重训练来提高模型区分相似和不同内容的能力。这种创新的训练方法与严格的数据过滤（包括语言检测和一致性检查）相结合，使模型能够有效捕捉细微的语义关系。","jina-embeddings-v2-base-code":"该模型通过专门为代码理解而设计的专用架构实现了令人印象深刻的性能。其核心是使用基于 Transformer 的神经网络，该网络具有 1.61 亿个参数，在各种编程语言数据集上进行训练，重点是六种主要语言：Python、JavaScript、Java、PHP、Go 和 Ruby。该架构的独特之处在于其扩展的上下文窗口为 8,192 个词元，使其能够同时处理整个函数或多个文件，同时保持语义理解。该模型生成密集的 768 维向量，可捕获代码的句法结构和语义含义，使其能够理解不同代码段之间的关系，即使它们使用不同的编程模式或语法来实现相同的目标。","jina-embeddings-v2-base-de":"该模型通过创新架构实现了令人印象深刻的双语能力，该架构在统一的 768 维向量空间中处理德语和英语文本。其核心是采用基于 Transformer 的神经网络，该网络具有 1.61 亿个参数，经过精心训练，可以理解两种语言之间的语义关系。这种架构特别有效的原因是其偏差最小化方法，专门设计用于避免偏向英语语法结构的常见陷阱 - 这是最近多语言模型研究中发现的一个问题。该模型的扩展上下文窗口为 8,192 个词元，使其能够一次性处理整个文档或多页文本，从而保持两种语言长篇内容的语义一致性。","jina-embeddings-v2-base-en":"该模型的架构将 BERT Small 主干与创新的对称双向 ALiBi（具有线性偏差的注意力机制）机制相结合，消除了对传统位置向量的需求。这种架构选择使模型能够推断出远远超出其 512 个词元的训练长度，处理多达 8,192 个词元的序列而不会降低性能。训练过程涉及两个关键阶段：在 C4 数据集上进行初始预训练，然后在 Jina AI 精选的 40 多个专业数据集上进行细化。这些多样化的训练数据（包括具有挑战性的负面示例和不同的句子对）确保了在不同领域和用例中的稳健性能。该模型生成 768 维密集向量，可捕捉细微的语义关系，使用相对适中的 137M 个参数实现。","jina-embeddings-v2-base-es":"该模型的核心是基于对称双向 ALiBi（具有线性偏差的注意力机制）的创新架构，这是一种复杂的方法，无需传统的位置向量即可处理多达 8,192 个词元的序列。该模型采用具有 161M 个参数的改进的 BERT 架构，结合了门控线性单元 (GLU) 和专门的层规范化技术。训练遵循三个阶段的过程：首先在海量文本语料库上进行预训练，然后使用精心挑选的文本对进行微调，最后进行硬负训练以增强对相似但语义不同的内容的区分。这种方法与 768 维向量相结合，使模型能够捕捉细微的语义关系，同时保持计算效率。","jina-embeddings-v2-base-zh":"该模型的架构将基于 BERT 的主干与对称双向 ALiBi（具有线性偏差的注意力机制）相结合，从而能够高效处理长序列，而不受传统 512 个 token 的限制。训练过程遵循精心策划的三阶段方法：首先在高质量双语数据上进行预训练，然后进行主要和次要微调阶段。这种有条不紊的训练策略，加上模型的 161M 参数和 768 维输出，实现了卓越的效率，同时保持了两种语言的平衡性能。对称双向 ALiBi 机制代表了一项重大创新，使模型能够处理长度高达 8,192 个 token 的文档——这一功能以前仅限于专有解决方案。","jina-embeddings-v3":"该模型的架构代表了向量技术的重大创新，它建立在具有 24 层的 jina-XLM-RoBERTa 基础上，并通过特定于任务的低秩自适应 (LoRA) 适配器进行了增强。LoRA 适配器是专门的神经网络组件，可针对不同的任务（如检索、分类或聚类）优化模型，而不会显着增加参数数量 - 它们使总参数增加不到 3%。该模型结合了 Matryoshka 表示学习 (MRL)，允许将向量从 1024 维灵活地减少到 32 维，同时保持性能。训练涉及三个阶段：对来自 89 种语言的多语言文本进行初始预训练，对成对文本进行微调以提高向量质量，以及专门的适配器训练以优化任务。该模型通过旋转位置向量 (RoPE) 支持高达 8,192 个词元的上下文长度，并采用创新的基频调整技术来提高短文本和长文本的性能。","jina-reranker-v1-base-en":"该模型采用基于 BERT 的交叉注意架构，与传统的基于向量的方法有着根本区别。它不是比较预先计算的文档向量，而是在查询和文档之间执行动态词元级交互，从而能够捕捉简单的相似性指标所遗漏的上下文细微差别。该架构的 1.37 亿个参数经过精心设计，可在保持计算效率的同时实现深度语义理解。一个突出的创新是它能够处理多达 262,144 个词元的序列（远远超出了典型的模型限制），这是通过复杂的优化技术实现的，尽管上下文窗口增加了，但仍能保持快速的推理速度。","jina-reranker-v1-tiny-en":"该模型采用基于 JinaBERT 的精简四层架构，具有对称双向 ALiBi（具有线性偏差的注意力机制），可高效处理长序列。其开发利用了一种先进的知识蒸馏方法，其中更大的高性能教师模型 (jina-reranker-v1-base-en) 指导训练过程，使较小的模型无需大量现实世界的训练数据即可学习最佳排名行为。这种创新的训练方法与减少隐藏层和高效注意力机制等架构优化相结合，使模型能够保持高质量的排名，同时显著降低计算要求。结果是模型实现了卓越的效率，同时又不损害其理解复杂文档关系的能力。","jina-reranker-v1-turbo-en":"该模型通过创新的六层架构实现了其效率，该架构将其较大模型的复杂重新排序功能压缩为仅 3780 万个参数 - 与基础模型的 1.37 亿个参数相比大幅减少。这种精简的设计采用知识蒸馏，其中较大的基础模型充当老师，训练 turbo 变体以匹配其行为，同时使用更少的资源。该架构保留了核心的基于 BERT 的交叉注意机制，用于查询和文档之间的词元级交互，但通过减少层数和有效的参数分配来优化其速度。该模型支持多达 8,192 个词元的序列，通过复杂的优化技术实现全面的文档分析，同时保持快速的推理速度。","jina-reranker-v2-base-multilingual":"该模型采用了通过 Flash Attention 2 增强的交叉编码器架构，可以直接比较查询和文档，从而更准确地评估相关性。该模型经过四阶段的训练，首先建立英语语言能力，然后逐步整合跨语言和多语言数据，最后使用硬负样本进行细化。这种创新的训练方法与 Flash Attention 2 实现相结合，使该模型能够处理多达 524,288 个词元的序列，同时保持出色的速度。该架构的效率使其能够处理跨多种语言的复杂重新排序任务，吞吐量比其前代产品高出 6 倍，同时通过直接查询文档交互确保准确的相关性评估。","reader-lm-05b":"该模型采用创新的“浅而宽”架构，专门针对选择性复制操作而非创造性文本生成进行优化。该模型建立在仅解码器的基础上，具有 24 层和 896 个隐藏维度，使用具有 14 个查询头和 2 个键值头的专门注意机制来高效处理输入序列。训练过程涉及两个不同的阶段：首先使用更短、更简单的 HTML（32K 个词元）来学习基本的转换模式，然后使用复杂的真实 HTML（128K 个词元）来处理具有挑战性的情况。该模型在训练期间结合了对比搜索，并实施了重复检测机制，以防止出现词元循环等退化问题。其架构的一个独特方面是锯齿环注意机制，这使模型能够处理高达 256K 个词元的极长序列，同时保持稳定的性能。","reader-lm-15b":"该模型采用了一种创新的“浅而宽”架构，挑战了语言模型设计中的传统扩展方法。其核心是 28 个Transformer层，配置了 12 个查询头和 2 个键值头，从而创建了一种独特的平衡，可优化选择性复制操作，同时保持深度语义理解。该架构的隐藏大小为 1536，中间大小为 8960，经过精心调整，可处理最多 256K 个词元的序列。训练过程涉及两个不同的阶段：首先专注于具有 32K 个词元序列的短而简单的 HTML，然后推进到具有 128K 个词元的长而难的 HTML，实现锯齿状环注意以实现高效处理。这种方法与对比搜索和专门的重复检测机制相结合，使模型能够避免处理复杂文档处理任务的小型语言模型中通常存在的退化和死循环等常见问题。",title:"方法"},model_comparison:"模型比较",model_details:"模型详细信息",model_io_graph:"I/O 图 {_number}",model_name:"名称",output_dimension:"输出维度",overview:{"ReaderLM-v2":"ReaderLM-v2 是一个 1.5B 参数语言模型，可将原始 HTML 转换为 markdown 或 JSON，处理最多 512K 个词元组合输入/输出长度，支持 29 种语言。与将 HTML 到 markdown 视为“选择性复制”任务的前身不同，v2 将其视为翻译过程，从而能够出色地处理代码围栏、嵌套列表、表格和 LaTeX 方程式等复杂元素。该模型在不同的上下文长度下保持一致的性能，并引入了具有预定义架构的直接 HTML 到 JSON 生成功能。","jina-clip-v1":"Jina CLIP v1 是第一个在文本转文本和文本转图片检索任务中表现优异的模型，它彻底改变了多模态 AI。与在纯文本场景中表现不佳的传统 CLIP 模型不同，该模型在所有检索组合中都实现了最先进的性能，同时保持了非常紧凑的 223M 参数大小。该模型解决了一个关键的行业挑战，它消除了对用于文本和图片处理的单独模型的需求，从而降低了系统复杂性和计算开销。对于构建搜索系统、推荐引擎或内容分析工具的团队，Jina CLIP v1 提供了一个单一、高效的解决方案，可以以极高的准确性处理文本和视觉内容。","jina-clip-v2":"Jina CLIP v2 彻底改变了多模态 AI，它弥合了 89 种语言中视觉和文本理解之间的差距。该模型通过实现准确的图片文本匹配，解决了全球电子商务、内容管理和跨文化交流中的关键挑战，不受语言障碍的影响。对于在国际上扩张或管理多语言内容的企业来说，它消除了对每种语言单独使用模型或复杂翻译流程的需求。该模型在需要跨语言边界进行精确视觉搜索的场景中尤其出色，例如全球市场产品发现或多语言数字资产管理。","jina-colbert-v1-en":"Jina-ColBERT-v1-en 通过解决信息检索中的一个关键挑战，彻底改变了文本搜索：在不牺牲计算效率的情况下实现高精度。与将整个文档压缩为单个向量的传统模型不同，此模型在仅需要 1.37 亿个参数的情况下保持了精确的词元级理解。对于构建搜索应用程序、推荐系统或内容发现平台的团队来说，Jina-ColBERT-v1-en 消除了搜索质量和系统性能之间的传统权衡。该模型在细致入微的文本理解至关重要的场景中尤其出色，例如技术文档搜索、学术论文检索或任何捕捉微妙的语义关系可能会在找到正确信息和遗漏关键内容之间产生差异的应用程序。","jina-colbert-v2":"Jina-ColBERT-v2 是一种突破性的多语言信息检索模型，解决了跨多种语言进行高效、高质量搜索的关键挑战。作为第一个生成紧凑向量的多语言 ColBERT 类模型，它满足了全球应用中对可扩展、经济高效的多语言搜索解决方案日益增长的需求。从电子商务平台到内容管理系统，处理多语言内容的组织可以利用此模型提供 89 种语言的准确搜索结果，同时通过其创新的降维功能显着降低存储和计算成本。","jina-embedding-b-en-v1":"Jina Embedding B v1 是一种专门的文本向量模型，旨在将英文文本转换为高维数字表示，同时保持语义含义。该模型满足了生产环境中对高效、准确的文本向量的关键需求，对于需要在计算效率和向量质量之间取得平衡的组织尤其有价值。凭借其 1.1 亿个参数生成 768 维向量，它可作为团队实施语义搜索、文档聚类或内容推荐系统的实用解决方案，而无需大量计算资源。","jina-embeddings-v2-base-code":"Jina Embeddings v2 基础代码解决了现代软件开发中的一个关键挑战：高效导航和理解大型代码库。对于在代码发现和文档方面遇到困难的开发团队，此模型通过跨 30 种编程语言启用自然语言搜索，改变了开发人员与代码交互的方式。与依赖精确模式匹配的传统代码搜索工具不同，此模型理解代码背后的语义含义，允许开发人员使用简单的英语描述找到相关的代码片段。此功能对于维护大型遗留代码库的团队、加入新项目的开发人员或希望改进代码重用和文档实践的组织尤其有价值。","jina-embeddings-v2-base-de":"Jina Embeddings v2 Base German 解决了国际业务中的一个关键挑战：弥合德语和英语市场之间的语言差距。对于向英语地区扩张的德国公司来说，准确的双语理解至关重要，因为英语地区三分之一的企业的全球销售额占比超过 20%。该模型通过实现德语和英语的无缝文本理解和检索，改变了组织处理跨语言内容的方式，这对于实施国际文档系统、客户支持平台或内容管理解决方案的公司来说非常有用。与传统的基于翻译的方法不同，该模型将两种语言的等效含义直接映射到相同的向量空间，从而实现更准确、更高效的双语操作。","jina-embeddings-v2-base-en":"Jina Embeddings v2 Base English 是一种突破性的开源文本向量模型，它解决了处理长文档同时保持高准确率的关键挑战。那些难以分析大量法律文件、研究论文或财务报告的组织会发现这个模型特别有价值。它以处理长度高达 8,192 个词元的文档而脱颖而出——比传统模型长 16 倍——同时性能与 OpenAI 的专有解决方案相匹配。它体积小巧，仅为 0.27GB，资源利用率高，为寻求实施高级文档分析而无需过多计算开销的团队提供了一个可访问的解决方案。","jina-embeddings-v2-base-es":"Jina Embeddings v2 Base Spanish 是一种突破性的双语文本向量模型，可解决西班牙语和英语内容之间的跨语言信息检索和分析这一关键挑战。与通常偏向特定语言的传统多语言模型不同，该模型在西班牙语和英语之间实现了真正平衡的性能，对于在西班牙语市场运营或处理双语内容的组织来说，它是必不可少的。该模型最引人注目的特点是它能够生成几何对齐的向量 - 当西班牙语和英语文本表达相同的含义时，它们的向量表示会自然地聚集在向量空间中，从而实现无缝的跨语言搜索和分析。","jina-embeddings-v2-base-zh":"Jina Embeddings v2 Base Chinese 开创了先河，成为第一个无缝处理中文和英文文本的开源模型，其上下文长度达到前所未有的 8,192 个 token。这个强大的双语模型解决了全球商业面临的一个关键挑战：需要准确、长篇文档处理中文和英文内容。与传统模型难以进行跨语言理解或需要为每种语言建立单独的模型不同，该模型将两种语言的等效含义映射到同一个向量空间，这对于在全球范围内扩张或管理多语言内容的组织来说非常有价值。","jina-embeddings-v3":"Jina Embeddings v3 是一种突破性的多语言文本向量模型，它改变了组织处理跨语言文本理解和检索的方式。从本质上讲，它解决了在多种语言和任务中保持高性能，同时保持计算要求可控的关键挑战。该模型在效率至关重要的生产环境中尤其出色 - 它仅用 5.7 亿个参数就实现了最先进的性能，这使得无法承担较大模型计算开销的团队也可以使用它。需要构建可扩展的多语言搜索系统或跨语言障碍分析内容的组织会发现这个模型特别有价值。","jina-reranker-v1-base-en":"Jina Reranker v1 Base English 彻底改变了搜索结果优化，解决了传统向量搜索系统的一个关键限制：无法捕捉查询和文档之间的细微关系。虽然采用余弦相似度的向量搜索可以快速提供初始结果，但它通常会错过人类用户直观理解的细微相关性信号。此重新排序器通过对查询和文档执行复杂的词元级分析来弥补这一差距，将搜索准确率提高了 20%。对于在搜索精度方面遇到困难或实施 RAG 系统的组织，此模型提供了一种强大的解决方案，可显著提高结果质量，而无需彻底改造现有的搜索基础设施。","jina-reranker-v1-tiny-en":"Jina Reranker v1 Tiny English 代表了高效搜索优化的突破，专为需要在资源受限环境中进行高性能重新排序的组织而设计。该模型解决了保持搜索质量的关键挑战，同时显著降低了计算开销和部署成本。它仅使用 33M 个参数（典型重新排序器大小的一小部分），通过创新的知识提炼技术提供了极具竞争力的性能。该模型最令人惊讶的功能是它能够以比基础模型快近五倍的速度处理文档，同时保持 92% 以上的准确率，使企业级搜索优化可供计算资源非常宝贵的应用程序使用。","jina-reranker-v1-turbo-en":"Jina Reranker v1 Turbo English 解决了生产搜索系统中的一个关键挑战：结果质量和计算效率之间的权衡。虽然传统的重新排序器提供了更高的搜索准确度，但它们的计算需求往往使它们不适用于实时应用。该模型突破了这一障碍，它提供了基本模型 95% 的准确度，同时处理文档的速度提高了三倍，使用的内存减少了 75%。对于在搜索延迟或计算成本方面苦苦挣扎的组织，该模型提供了一个引人注目的解决方案，既能保持高质量的搜索优化，又能显著降低基础设施要求和运营成本。","jina-reranker-v2-base-multilingual":"Jina Reranker v2 Base Multilingual 是一种跨编码器模型，旨在提高跨语言障碍和数据类型的搜索准确性。此重新排序器解决了多语言环境中精确信息检索的关键挑战，对于需要跨不同语言和内容类型优化搜索结果的全球企业尤其有价值。它支持 100 多种语言，并具有独特的函数调用和代码搜索功能，是需要跨国际内容、API 文档和多语言代码库进行精确搜索优化的团队的统一解决方案。该模型紧凑的 278M 参数设计使其对于寻求平衡高性能和资源效率的组织特别有吸引力。","reader-lm-05b":"Reader LM 0.5B 是一种专门的语言模型，旨在解决将 HTML 文档转换为干净、结构化的 Markdown 文本的复杂挑战。该模型满足了现代数据处理流程中的一项关键需求：高效地将杂乱的 Web 内容转换为适合大模型和文档系统的格式。与需要大量计算资源的通用语言模型不同，Reader LM 0.5B 仅使用 4.94 亿个参数即可实现专业级 HTML 处理，这使得计算资源有限的团队也可以使用它。处理 Web 内容处理、文档自动化或构建大模型支持的应用程序的组织会发现此模型对于简化其内容准备工作流程特别有用。","reader-lm-15b":"Reader LM 1.5B 代表了高效文档处理方面的突破，解决了将复杂的 Web 内容转换为干净的结构化格式这一关键挑战。这种专门的语言模型解决了现代 AI 流程中的一个基本问题：需要高效处理和清理 HTML 内容以用于下游任务，而无需依赖脆弱的基于规则的系统或资源密集型的大型语言模型。这个模型真正引人注目的地方在于，它能够超越其大小 50 倍的模型，同时保持令人惊讶的紧凑的 1.54B 参数占用空间。处理大规模 Web 内容处理、文档自动化或内容管理系统的组织会发现这个模型特别有价值，因为它能够处理极长的文档，同时在 HTML 到 Markdown 的转换中提供卓越的准确性。",title:"概述"},parameter_size:"参数",performance:{"ReaderLM-v2":"在综合基准测试中，ReaderLM-v2 在 HTML 到 Markdown 任务上的表现优于 Qwen2.5-32B-Instruct 和 Gemini2-flash-expr 等大型模型。对于主要内容提取，它实现了 0.84 的 ROUGE-L、0.82 的 Jaro-Winkler，并且与竞争对手相比，Levenshtein 距离 (0.22) 明显更低。在 HTML 到 JSON 任务中，它保持了具有竞争力的性能，F1 得分为 0.81，通过率为 98%。该模型在 T4 GPU 上以 67 个 token/s 的输入和 36 个 token/s 的输出进行处理，通过对比损失训练显著减少了退化问题。","jina-clip-v1":"Jina CLIP v1 在所有基准测试中都比 OpenAI 的原始 CLIP 有了显著的改进。在纯文本检索中，它的性能提高了 165%，得分为 0.429，而 CLIP 的得分为 0.162。对于与图片相关的任务，它显示出持续的改进：文本到图片检索提高了 2%（0.899），图片到文本检索提高了 6%（0.803），图片到图片检索提高了 12%（0.916）。该模型在零样本视觉分类任务中尤其出色，无需在特定领域进行事先训练即可成功对图片进行分类。在标准基准（如文本检索的 MTEB、图片任务的 CIFAR-100 以及跨模态性能的 Flickr8k/30k 和 MSCOCO Captions）上进行评估时，它始终优于专门的单模态模型，同时在跨模态任务中保持了有竞争力的性能。","jina-clip-v2":"该模型在 Flickr30k 图片到文本检索任务中实现了 98.0% 的准确率，超越了其前身和 NLLB-CLIP-SigLIP，达到了最佳性能。在多语言场景中，尽管参数比其最大的竞争对手少，但在跨语言图片检索任务中，该模型比 NLLB-CLIP-SigLIP 提高了 4%。即使向量被压缩，该模型仍能保持强劲的性能 - 将尺寸减少 75% 仍可在文本、图片和跨模态任务中保持 99% 以上的性能。在综合多语言 MTEB 基准测试中，它在检索任务中实现了 69.86%，在语义相似性任务中实现了 67.77%，与专门的文本向量模型相比具有竞争力。","jina-colbert-v1-en":"Jina-ColBERT-v1-en 在各种基准测试中都比基线模型表现出色。在 BEIR 数据集上，它在多个类别中取得了优异的表现：Arguana 上为 49.4%（而 ColBERTv2 为 46.5%），FEVER 上为 79.5%（而 ColBERTv2 为 78.8%），TREC-COVID 上为 75.0%（而 ColBERTv2 为 72.6%）。最令人印象深刻的是，它在长上下文理解的 LoCo 基准测试中表现出了显著的改进，得分为 83.7%，而 ColBERTv2 为 74.3%。该模型在需要详细语义理解的场景中尤其出色，通过其创新的后期交互方法，它的表现优于传统的向量模型，同时保持了计算效率。这些改进是在将模型的参数数量保持在 137M 的适中水平的同时实现的，使其功能强大且适用于生产部署。","jina-colbert-v2":"在实际测试中，Jina-ColBERT-v2 在多个基准测试中展现出卓越的能力。它在英语任务上的表现比原始的 ColBERT-v2 提高了 6.5%，在 14 个 BEIR 基准测试中的平均得分为 0.521。更令人印象深刻的是，它在 MIRACL 基准测试中在所有测试语言中的表现都优于传统的基于 BM25 的检索方法，在跨语言场景中表现出特别的优势。即使在使用减少的向量维度时，该模型也能保持这种高性能 - 从 128 维降至 64 维仅导致性能下降 1.5%，同时存储需求减半。这意味着生产成本显著节省：例如，在 AWS 上存储 1 亿份具有 64 维向量的文档每月成本为 659.62 美元，而 128 维则为 1,319.24 美元。","jina-embedding-b-en-v1":"在实际评估中，Jina Embedding B v1 展现出令人印象深刻的功能，尤其是在语义文本相似性任务中。该模型在 STS12 上以 0.751 的得分实现了最佳性能，超越了 all-mpnet-base-v2 和 all-minilm-l6-v2 等成熟模型。它在各种基准测试中都表现出色，同时保持了高效的推理时间。但是，用户应注意，该模型专门针对英语内容进行了优化，在多语言或特定于代码的任务上可能无法达到最佳性能。该模型已被 jina-embeddings-v2-base-en 和 jina-embeddings-v3 取代，它们在更广泛的用例中提供了增强的性能。","jina-embeddings-v2-base-code":"在实际测试中，Jina Embeddings v2 Base Code 展现出卓越的能力，在 15 个关键 CodeNetSearch 基准测试中的 9 个中处于领先地位。与微软和 Salesforce 等行业巨头的模型相比，它在保持更高效的占用空间的同时实现了卓越的性能。该模型在跨语言代码理解方面尤其出色，成功地匹配了不同编程语言中功能等效的代码片段。它的 8,192 个 token 上下文窗口对于大型函数和复杂代码文件特别有价值，远远优于通常只能处理几百个 token 的传统模型。该模型的效率体现在其 307MB（未量化）的紧凑尺寸上，可实现快速推理，同时在代码相似性和搜索任务中保持高精度。","jina-embeddings-v2-base-de":"在实际测试中，Jina Embeddings v2 Base German 表现出卓越的效率和准确性，尤其是在跨语言检索任务中。该模型在大小不到微软 E5 基础模型三分之一的情况下表现优于后者，尽管体积只有后者的七分之一，但性能却与 E5 大型模型相当。在包括用于英语到德语检索的 WikiCLIR、用于双向语言理解的 STS17 和 STS22 以及用于精确双语文本对齐的 BUCC 在内的关键基准测试中，该模型始终表现出卓越的能力。其紧凑的尺寸为 322MB，可在标准硬件上部署，同时保持最先进的性能，使其在考虑计算资源的生产环境中特别高效。","jina-embeddings-v2-base-en":"在实际测试中，Jina Embeddings v2 Base English 在多个基准测试中展现出卓越的能力。它在几个关键指标上都优于 OpenAI 的 text-embedding-ada-002：分类（73.45% vs 70.93%）、重新排序（85.38% vs 84.89%）、检索（56.98% vs 56.32%）和摘要（31.6% vs 30.8%）。这些数字在文档分类等任务中转化为实际优势，其中模型显示出对复杂文本进行分类的卓越能力，并在搜索应用中，它更好地理解用户查询并找到相关文档。但是，用户应注意，在处理训练数据中未表示的高度专业化领域特定内容时，性能可能会有所不同。","jina-embeddings-v2-base-es":"在综合基准评估中，该模型表现出卓越的能力，特别是在跨语言检索任务中，尽管其规模只有 E5 和 BGE-M3 等大型多语言模型的 15-30%，但其表现却优于后者。该模型在检索和聚类任务中表现出色，在跨语言匹配语义等效内容方面表现出色。在 MTEB 基准测试中，它在分类、聚类和语义相似性等各种任务中表现出色。8,192 个词元的扩展上下文窗口对于长文档处理尤其有价值，即使文档跨越多页，也能表现出一致的性能——这是大多数竞争模型所缺乏的能力。","jina-embeddings-v2-base-zh":"在中文 MTEB (C-MTEB) 排行榜的基准测试中，该模型在 0.5GB 以下的模型中表现出色，尤其是在中文任务中表现出色。它在中文特定应用中的表现明显优于 OpenAI 的 text-embedding-ada-002，同时在英语任务中保持了竞争力。此版本中的一个显着改进是改进了相似度分数分布，解决了预览版本中存在的分数膨胀问题。该模型现在提供更独特、更合乎逻辑的相似度分数，确保更准确地表示文本之间的语义关系。这种增强在比较测试中尤为明显，其中模型在两种语言中对相关和不相关内容表现出更好的区分能力。","jina-embeddings-v3":"该模型在实际测试中展现出卓越的效率性能比，在英语任务上的表现优于开源替代方案以及来自 OpenAI 和 Cohere 的专有解决方案，同时在多语言场景中也表现出色。最令人惊讶的是，它取得了比参数多 12 倍的 e5-mistral-7b-instruct 更好的结果，凸显了其卓越的效率。在 MTEB 基准评估中，它在所有任务中获得了 65.52 的平均分数，在分类准确率（82.58）和句子相似度（85.80）方面表现尤为出色。该模型在不同语言中保持一致的性能，在多语言任务上的得分为 64.44。当使用 MRL 进行降维时，即使在较低维度下也能保持强劲性能 - 例如，与完整的 1024 维相比，64 维可以保持 92% 的检索性能。","jina-reranker-v1-base-en":"在综合基准测试中，该模型在关键指标方面表现出色，与基线向量搜索相比，命中率提高了 8%，平均倒数排名提高了 33%。在 BEIR 基准测试中，它的平均得分为 0.5588，优于 BGE（0.5032）、BCE（0.4969）和 Cohere（0.5141）等其他重排器。其在 LoCo 基准测试中的表现尤其令人印象深刻，平均得分为 0.873，在理解局部连贯性和上下文感知排名方面远远领先于竞争对手。该模型在技术内容评估方面表现出色，在 qasper_abstract 任务中得分为 0.996，在政府报告分析中得分为 0.962，但在会议摘要任务中表现相对较低（0.466）。","jina-reranker-v1-tiny-en":"在综合基准评估中，该模型展现出卓越的能力，挑战了传统的大小与性能之间的权衡。在 BEIR 基准测试中，该模型的 NDCG-10 得分为 48.54，保留了基础模型 92.5% 的性能，而尺寸仅为其四分之一。更令人印象深刻的是，在 LlamaIndex RAG 基准测试中，它保持了 83.16% 的命中率，几乎与更大的模型相匹配，同时处理文档的速度明显更快。该模型在吞吐量方面尤其出色，处理文档的速度几乎是基础模型的五倍，而使用的内存甚至比 turbo 版本还要少 13%。这些指标转化为实际性能，可与 mxbai-rerank-base-v1（184M 个参数）和 bge-reranker-base（278M 个参数）等更大的模型相媲美或超过它们。","jina-reranker-v1-turbo-en":"在综合基准测试中，turbo 变体表现出卓越的效率，且准确度没有显著降低。在 BEIR 基准测试中，它获得了 49.60 的 NDCC-10 得分，保留了基础模型 95% 的性能（52.45），同时超越了许多更大的竞争对手，如 bge-reranker-base（47.89，278M 参数）。在 RAG 应用中，它保持了令人印象深刻的 83.51% 命中率和 0.6498 MRR，在实际检索任务中表现出特别的优势。该模型的速度提升更加惊人——它处理文档的速度比基础模型快三倍，吞吐量几乎随参数数量的减少而线性增长。但是，用户应该注意到，在极其细微的排名任务中，性能略有下降，其中较大模型的完整参数数量提供了边际优势。","jina-reranker-v2-base-multilingual":"在实际评估中，该模型在各种基准测试中都表现出色。它在 RAG 系统的 AirBench 排行榜上取得了最先进的性能，并在多语言任务中表现出色，包括涵盖 26 种语言的 MKQA 数据集。该模型在结构化数据任务中尤其出色，在函数调用（ToolBench 基准测试）和 SQL 模式匹配（NSText2SQL 基准测试）中都取得了高召回率。最令人印象深刻的是，它在提供这些结果的同时，处理文档的速度比 bge-reranker-v2-m3 等同类模型快 15 倍，使其适用于实时应用。但是，用户应注意，最佳性能需要具有 CUDA 功能的 GPU 进行推理。","reader-lm-05b":"在实际测试中，Reader LM 0.5B 在多个指标上表现出令人印象深刻的效率与性能比。该模型的 ROUGE-L 得分达到 0.56，表明内容保存效果良好，并且保持了 0.34 的低词元错误率，显示出最小的幻觉。在对 22 种不同的 HTML 源（包括多种语言的新闻文章、博客文章和电子商务页面）进行定性评估时，它在结构保存和 markdown 语法使用方面表现出色。该模型擅长处理复杂的现代网页，其中内联 CSS 和脚本可以扩展到数十万个词元 - 传统的基于规则的方法经常会失败。但是，需要注意的是，虽然该模型在简单的 HTML 到 markdown 转换任务上表现非常出色，但对于高度动态或 JavaScript 密集的页面，它可能需要额外的处理。","reader-lm-15b":"在全面的基准评估中，Reader LM 1.5B 展现出挑战行业标准的卓越能力。该模型的 ROUGE-L 得分为 0.72，Token 错误率为 0.19，在 HTML 到 Markdown 转换任务中明显优于 GPT-4（0.43 ROUGE-L、0.50 TER）和 Gemini-1.5-Pro（0.42 ROUGE-L、0.48 TER）等大型模型。其性能在四个关键维度的定性评估中尤为突出：标题提取、主要内容提取、丰富结构保存和 Markdown 语法使用。该模型在多种文档类型（从新闻文章和博客文章到登录页面和论坛帖子）中始终保持高精度，支持多种语言，包括英语、德语、日语和中文。这种性能是在处理长度高达 256K 的 token 的文档时实现的，无需使用大型模型通常需要的昂贵分块操作。",title:"性能"},performance_metrics:"绩效指标",publications:"出版物",tags:"标签",token_length:"输入词元长度",usage_requirements:"使用和要求",using_model:"可通过以下方式获取"},select_model:"从列表中选择一个模型以查看详细信息",sort:{direction:{asc:"升序",desc:"降序",name:"方向"},label:"种类",name:"名称",parameter_size:"尺寸",release_date:"日期"},title:"{_modelName} - 搜索底座模型",warnings:{deprecated:"此模型已被较新的模型弃用。"}},ie={back_to_newsroom:"返回新闻首页",categories:"类别",copy_link:"复制此部分的链接",in_this_article:"文章导览",learn_more:"了解更多",news_not_found:"糟糕！未找到文章",redirect_to_news:"将在5 秒后重定向至新闻首页..."},te={academic:"学术",academic_research:"学术论文",author:"按作者过滤",description:"读取 Jina AI 的最新新闻和更新。",description1:"一字一句地撰写AI技术创新。",engineering_group:"工程组",engineering_group_date:"2021 年 5 月 31 日",minutes_read:"分钟的读取量",most_recent_articles:"最新文章",news_description:"对于 Jina 2.0，我们听取了社区的意见。确实，深深地听过。 “你的痛点是什么？”我们询问并热切期待宝贵的反馈",news_title:"搜索所有内容：我们正在为 Jina 2.0 举办 MEME 竞赛",photos:"相片",product:"按产品过滤",search:"按标题搜索",tech_blog:"技术文章",title:"新闻",top_stories:"甄选文章"},re="🎉 我们的第一本书《神经搜索——与 Jina 一起从原型到生产》今天正式出版！",oe={description:"参访 Jina AI 内部的独家机会。",engage:"我们强烈鼓励全天进行互动对话。思想和观点的交流对我们来说非常宝贵。这些讨论产生的潜在合作可以为更加一体化和创新的未来做出重大贡献。",engage_title:"头脑风暴",experience:"我们为客人安排了三小时的沉浸式游览，提供德语、英语、法语、西班牙语、中文和俄语版本。这次巡演将深入探讨我们在多模态AI方面的进展、我们对人工智能领域的看法，然后对具体项目进行详细审查。最后我们将进行小组讨论，以促进思想和见解的交流。还可应要求提供午餐选择。",experience_title:"业内人士之旅",group_size:"预计参观人数",impact:"了解我们对开源社区的贡献以及我们在多模态AI技术方面的工作如何使 Jina AI 成为人工智能创新领域有影响力的参与者。我们的目标是在决策过程中发挥重要作用，确保人工智能技术的进步惠及所有人。",impact_title:"行业影响力",introduction:"Jina AI 很高兴向对那些对人工智能未来感兴趣的机构敞开大门。我们为政界、非政府机构、非营利机构和投资领域的人士提供开放日。在此，诚邀您做客柏林总部，了解我们的运营、愿景的行业洞察。",motivation_min_length_v1:"请提供更详细的动机。",motivation_placeholder_v2:"分享您的动机将帮助我们改善您的体验。",motivation_to_attend_v2:"您为什么对我们的开放日感兴趣？",one_hour:"1小时",organization:"机构",organization_website:"机构网站",organization_website_placeholder:"您机构的主页或 LinkedIn 个人资料的 URL",preferred_date:"首选日期",preferred_language:"首选语言",preferred_products:"您对哪些产品感兴趣？",subtitle:"多模态AI的未来一瞥",title:"开放日",tutor_subtitle:"精心策划的三小时导览，让您更接近 Jina AI 在多模态AI技术方面的开创性工作的核心。",tutor_title:"独家深入探讨",vision:"加入我们，全面了解我们所看到的人工智能前景。我们的讨论将重点关注大型语言模型、多模态AI的潜力以及开源技术在塑造全球创新未来方面的影响。",vision_title:"未来愿景"},se={answer1:"我们提供德语、英语、法语、西班牙语、中文和俄语的讲解服务。",answer2:"讲解通常持续大约三个小时。",answer3:"午餐是可选的，可根据要求安排。",answer4:"我们的开放日主要是为专业团体设计的，例如政治家、非政府机构、非营利机构和投资者。然而，我们偶尔会根据个人的个人资料做出例外情况。",answer5:"我们可以容纳各种规模的团体。请在报名表中注明您的团体人数，我们将与您确认详细信息。",answer6:"注册表中有一个部分，您可以在其中指定您感兴趣的领域或任何特殊要求。我们将尽力根据您的需求定制行程。",answer7:"目前，我们仅在位于克罗伊茨贝格的柏林总部提供开放日。我们的北京和深圳办事处目前不开放参观。",question1:"你们提供哪些语言的讲解服务？",question2:"讲解时长是多少？",question3:"提供午餐吗？",question4:"个人可以报名参加开放日吗？",question5:"开放日的团体可以有多少人？",question6:"我如何指定讲解的兴趣区域？",question7:"你们的北京或深圳办事处提供开放日吗？"},_e={description:"开源、云原生的大型多模态模型服务框架"},de={commercial_licence:{chip_label:"专为小型公司",company_size_note:"仅限员工人数少于 50 人或收入少于 50 万美元的公司",cta_button:"立即开始",download_title:"下载商业许可证",feature_api_desc:"购买前请先测试",feature_api_title:"免费 API 测试访问",feature_consulting:"与我们的模型专家进行两小时的咨询",feature_consulting_desc:"每个许可期限内提供两 (2) 小时的技术咨询服务。",feature_future_support:"无需许可即可访问未来的 CC BY-NC 模型",feature_future_support_desc:"许可期限内许可方根据 CC-BY-NC-4.0 发布的任何新模型。",feature_models:"无限制地使用我们的 CC BY-NC 模型进行商业使用",feature_models_desc:"将模型用于商业目的，包括内部使用或纳入面向客户的应用程序。",price_amount:"1,000 美元",price_period:"/ 每季度",read_the_terms:"查看许可条款",read_the_terms_btn:"条款",read_the_terms_desc:"购买前查看商业许可权利和限制",subtitle:"这些模型助您实现更好的搜索",test_before_purchase:"购买前请先试用",test_before_purchase_desc:"获取 100 万个免费 API 词元或使用我们的 Hugging Face 模型来验证性能",title:"团队许可",try_api:"首先尝试 API"},full_commercial:"不受限制的商业用途",full_commercial_description:"您可以将 API 用于商业目的，不受任何限制。",higher_limit:"更高速率！",higher_limit_description:"r.jina.ai 最高可达 1000 RPM，s.jina.ai 最高可达 100 RPM；更多详细信息请参阅速率限制部分。",key_manager:"基本密钥管理",key_manager_description:"在一个帐户中管理多个 API 密钥、跟踪使用历史记录和充值词元。",no_commercial:"仅限非商业用途（CC-BY-NC）",no_commercial_description:"您只能将API用于非商业用途，若要用于商业用途，请充值您的API密钥。",on_prem:"拥有本地使用的商业许可证",on_prem_explain:"购买商业许可证以在现场使用我们的模型。",premium_key:"高级密钥具有更高的速率限制",premium_key_description:"获得更高的速率限制并使用高级功能，请查看速率限制表了解详情。",premium_key_manager:"高级密钥管理",premium_key_manager_description:"基本功能和高级功能，例如自动提醒、撤销、词元传输。",priority_support:"优先技术支持",priority_support_description:"保证在 24 小时内通过电子邮件回复技术问题和事件。",secured_by_stripe:"通过 Stripe 安全付款",standard_key:"标准密钥",standard_key_description:"以标准速率限制访问所有Jina 搜索底座API。",via_api:"使用Jina 搜索底座API",via_api_explain:"访问我们所有产品的最简单方法。随时充值词元。"},le="基于",ce="打印",pe={archived:"已存档",cloud_native:"云原生",core:"基层核心",data_structure:"数据结构",embedding_serving:"大模型向量部署",embedding_tuning:"大模型向量调优",graduated:"已毕业",incubating:"孵化中",kubernetes:"Kubernetes",large_size_model:"大型模型",linux_foundation:"Linux 基金会",llm1:"LLM框架",mid_size_model:"中型模型",model_serving:"模型部署",model_tuning:"模型调优",observability:"可观察性",orchestration:"云编排",prompt_serving:"提示词部署",prompt_tuning:"提示词调优",rag1:"RAG应用",sandbox:"沙盒",small_size_model:"小型模型",vector_database:"向量数据库",vector_store:"向量数据库"},ue={description:"首屈一指的提示词工具箱",image_model:"图片模型",intro:"首屈一指的提示词工具箱",intro1:"提示词工程的首要工具",optimized:"你的任务是成为我的头脑风暴伙伴，为特定的主题或问题提供创造性的想法和建议。您的回答应该包括原创的、独特的和相关的想法，这些想法可以帮助解决问题或以有趣的方式进一步探索该主题。请注意，您的回答还应考虑任务的任何具体要求或限制。",optimized_title:"优化后的提示词",original:"你的角色是成为我的头脑风暴伙伴。",original_title:"原提示词",text_model:"文本模型"},me={features:[{description:"轻松在内容成产和提示词优化之间切换，将您的内容质量提升到新的水平。",name:"智能助手",title:"每日一罐生产力。"},{description:"不知道如何编写有效的提示词？只需输入您的想法，点一下鼠标，即可获得更好的提示词。",name:"提示词优化",title:"更好的输入，更好的输出"},{description:"通过比较同一提示词的输出来了解每个 AI 模型的性格。",name:"模型大比拼",title:"并排模型比较。"},{description:"这也许是将提示词部署为 API 最简单的方法。",name:"部署提示词",title:"告别繁琐Ops，直接部署。"},{description:"定制您自己的大模型智能体，并启动多智能体模拟。查看它们如何在虚拟环境中协作或竞争以达到目标。",name:"多智能体",title:"探索智能体如何协作"}],get_started:"开始使用 PromptPerfect"},ge={api_key:"充值API密钥",free_key:"免费 API 密钥",generation:"您的 API 密钥已准备好！",generation_caption:"您的 API 密钥已于 {_purchasedTime} 生成并可供使用！",success:"感谢您的购买！",success_caption:"您的订单已于{_purchasedTime}完成。您的 API 密钥已充值并可供使用！如果您发现词元余额未更新，请刷新页面。"},be="立即购买",Ae={batch_explain:"此 API 支持批量操作，每次请求最多允许 512 个文档，每个文档最多包含 8192 个词元。巧妙利用批量操作可以大幅减少请求次数并提高性能。",classifier:"使用训练样本训练分类器",classifier_few_shot:"使用经过训练的少样本分类器对输入进行分类",classifier_few_shot_token_counting:"词元计数为：输入词元",classifier_latency:"响应时间随输入大小而变化",classifier_token_counting:"词元计数为：输入词元 × 迭代次数",classifier_zero_shot:"使用零样本分类对输入进行分类",classifier_zero_shot_token_counting:"词元计数为：输入词元 加 标签词元",deepsearch:"推理、搜索和迭代以找到最佳答案",depends:"取决于输入大小",description:"描述",embeddings:"将文本/图片转为定长向量",endpoint:"API端口",explain:"速率限制以两种方式跟踪：<b>RPM</b>（每分钟请求数）和<b>TPM</b>（每分钟词元数）。限制是针对每个 IP/API 密钥强制执行的，并且可以根据首先达到的阈值（RPM 或 TPM）来达到。请注意，当请求中提供了 API 密钥时，速率限制是针对每个密钥而不是每个 IP 地址进行跟踪的。",flat_rate_per_request:"每个请求都需要固定数量的词元，从 {_number} 个词元开始",further_boost:"还不够？请联系我们以提升到更高的速率限制！",gjinaai:"用网络知识支撑声明",icon:"图标",input_token_counting:"以输入请求中的词元数量为准。",key_explain:"购买“白银”代币包可获得此级别的付费 API 密钥",latency:"平均延迟",llm_serp:"使用LLM生成搜索引擎结果页面",no_key_explain:"使用没有 API 密钥的 API 时，没有“用户”概念，因此速率限制是针对每个 IP 地址执行的。如果您恰好在共享环境中使用它，您可能会更快达到速率限制。",no_token_counting:"词元不计算使用量。",output_token_counting:"以输出响应中的词元数量为准。",premium_key_explain:"购买“黄金”代币包即可获得此级别的付费 API 密钥",premium_rate:"有可能提高速率限制",product:"产品",requestType:"请求类型",reranker:"按查询对文档进行精排",rjinaai:"将 URL 转换为大模型友好文本",search:"搜索",sjinaai:"搜索网络并将结果转换为大模型友好文本",tbd:"有待确定",title:"速率限制",tokenCounting:"词元使用计数",tokenizer:"对长文本进行分词分句",total_token_counting:"统计整个过程中词元的总数。",understanding:"了解速率限制",understanding_description:"速率限制是指每个 IP 地址/API 密钥 (RPM) 在一分钟内可以向 API 发出的最大请求数。请在下面详细了解每个产品和层级的速率限制。",wAPIkey:"使用 API 密钥",wPremium:"带有高级 API 密钥",woAPIkey:"无 API 密钥"},he={decision:"决定",description:"LLM辅助智能决策工具",intro:"看到硬币的两面，做出理性的决定"},Ie={beta:"实验",better_input:"从一开始就提高输入质量",better_input_description:"您的Agent或 RAG 系统输出出现问题？这可能是由于输入质量差造成的。",check_price_table:"查看价格表",copy:"复制",demo:{advanced_parameter_explain:"仅用于此端点的特定参数。",advanced_parameters:"具体的",advanced_usage:"高级用法",ask_llm:"询问大模型是否需要搜索依据",ask_llm_directly:"直接询问LLM",ask_llm_with_search_grounding:"通过搜索询问LLM",ask_question:"提出问题",ask_question_hint:"输入问题并将其与获取的大模型内容相结合以生成答案",basic_usage:"基本用法",basic_usage1:"使用 <code>r.jina.ai</code> 读取 URL 并获取其内容",basic_usage2:"使用 <code>s.jina.ai</code> 搜索网络并获取 SERP",basic_usage3:"查论",common_parameter_explain:"可用于{_product1}、{_product2}和{_product3}的通用参数。",common_parameters:"通用参数",content_length:"内容长度（字节）",copy:"复制",fetch:"获取内容",get_response:"获取响应",grounding_result_false:"此言差矣。",grounding_result_true:"此言正确。",headers:{atx:"替代标题语法",atx_explain:"使用文本下方行中的任意数量的“==”或“--”字符来创建标题。",auth_token:"添加 API 密钥以实现更高的速率限制",auth_token_explain:"输入您的 Jina API 密钥以访问更高的速率限制。有关最新速率限制信息，请参阅下表。",auto:"自动",auto_explain:"自动为 URL 选择最佳引擎。",base:"关注重定向页面",base_explain:"选择是否在遵循所有重定向后解析到最终目标网址。启用以遵循完整的重定向链。",browser:"质量优先",browser_explain:"高质量引擎旨在解决渲染问题并提供最佳内容输出。",browser_locale:"自定义浏览器区域设置",browser_locale_explain:"控制浏览器区域设置以呈现页面。许多网站根据区域设置提供不同的内容。",cf_browser:"实验性浏览器",cf_browser_explain:"我们正在试验一款能够处理 JavaScript 密集型网站的快速引擎。不建议用于生产用途。我们可能会更改其行为。",country:"@:llm_serp.parameters.country",country_explain:"@:llm_serp.parameters.country_explain",custom_script:"预运行 JavaScript",custom_script_explain:"执行预处理 JS 代码（内联字符串或远程 URL）。",deepdive:"深度源代码分析",deepdive_explain:"搜索更多来源并读取完整文档以进行彻底的事实核查。速度稍慢但更准确且参考资料更多。",default:"默认",default_explain:"针对大多数网站和大模型输入优化的默认管道。",direct:"速度优先",direct_explain:"最快的引擎，速度经过优化但无法处理 JavaScript 生成的动态内容。",discarded_explain:"链接被其锚文本替换。",dnt:"请勿缓存/跟踪！",dnt_explain:"启用后，请求结果将不会缓存在我们的服务器上。",engine:"读取引擎",engine_default:"默认",engine_default_explain:"最兼容的引擎在质量和速度之间提供良好的平衡。",engine_default_html:"默认使用原始 HTML 响应",engine_default_html_explain:"默认引擎具有 HTML 响应，无需 markdown 转换。",engine_default_markdown:"默认不带可读性过滤",engine_default_markdown_explain:"具有 Markdown 响应的默认引擎没有可读性过滤（删除页眉、页脚等）。",engine_default_timeout:"默认但设置了超时",engine_default_timeout_explain:"默认引擎但设置了5秒的最大超时。",engine_explain:"选择用于获取网页内容的浏览器引擎。这会影响内容的质量、速度、完整性和可访问性。",eu_compliance:"符合欧盟规定",eu_compliance_explain:"所有基础设施和数据处理操作完全在欧盟管辖范围内。",file:"本地 PDF/HTML 文件",file_explain:"通过上传本地 PDF 和 HTML 文件，使用读取器读取它们。仅支持 pdf 和 html 文件。",html:"HTML",html_explain:"返回 documentElement.outerHTML。",image_caption:"图片说明",image_caption_explain:"为指定 URL 上的所有图片添加标题，为没有标题的图片添加“Image [idx]: [caption]”作为 alt 标签。这允许下游大模型在推理和总结等活动中与图片进行交互。",images_summary:"将所有图片集中到最后",images_summary_all:"全部",images_summary_all_explain:"文末列出所有图片。",images_summary_explain:"最后会创建一个“图片”部分。这可以让下游的大模型概览页面上的所有视觉效果，从而提高推理能力。",images_summary_no:"无",images_summary_no_explain:"图片保持内联。",images_summary_true:"去重",images_summary_true_explain:"文末列出去重图片。",inlined_explain:"链接直接嵌入文本中。",instruction_explain:"根据指令提取信息",invalid_json:"JSON 格式错误",json_response:"JSON 响应",json_response_explain:"响应将采用 JSON 格式，包含 URL、标题、内容和时间戳（如果可用）。在搜索模式下，它会返回一个包含五个条目的列表，每个条目都遵循描述的 JSON 结构。",json_schema_explain:"使用 JSON Schema 进行 HTML 转 JSON 提取",language:"@:llm_serp.parameters.language",language_explain:"@:llm_serp.parameters.language_explain",links_summary:"将所有链接集中到最后",links_summary_all:"全部",links_summary_all_explain:"文末列出所有链接。",links_summary_explain:"最后会创建一个“按钮和链接”部分。这可以帮助下游大模型或 Web 代理浏览页面或采取进一步的行动。",links_summary_no:"无",links_summary_no_explain:"链接保持内联。",links_summary_true:"去重",links_summary_true_explain:"文末列出去重链接。",location:"@:llm_serp.parameters.location",location_explain:"@:llm_serp.parameters.location_explain",markdown:"Markdown",markdown_explain:"直接从 HTML 返回 markdown，绕过可读性过滤。",md:{bullet_list_marker:"项目符号样式",bullet_list_marker_explain:"设置项目符号列表标记字符（传递给 Turndown）。",em_delimiter:"强调风格",em_delimiter_explain:"定义 markdown 强调分隔符（传递给 Turndown）。",heading_style:"标题样式",heading_style_explain:"设置 markdown 标题格式（传递给 Turndown）。",hr:"水平线样式",hr_explain:"定义 markdown 水平规则格式（传递给 Turndown）。",link_reference_style:"参考链接样式",link_reference_style_explain:"设置 markdown 参考链接格式（传递给 Turndown）。",link_style:"链接样式",link_style_explain:"确定 markdown 链接格式（传递给 Turndown）。",strong_delimiter:"强调风格",strong_delimiter_explain:"设置 markdown 强强调分隔符（传递给 Turndown）。"},md_link_discarded:"纯文本",md_link_inline:"内联",md_link_referenced:"引用",mode:"读取或搜索模式",mode_explain:"读取模式用于访问 URL 的内容，而搜索模式允许您在网络上搜索查询，并将读取模式应用于每个搜索结果 URL。",no_cache:"绕过缓存",no_cache_explain:"我们的 API 服务器会将读取和搜索模式的内容缓存一段时间。要绕过此缓存，请将此标头设置为 true。",no_gfm:"已禁用",no_gfm_explain:"GFM（Github Flavored Markdown）功能已禁用。",no_gfm_table:"无 GFM 表",no_gfm_table_explain:"选择退出 GFM 表但保留表 HTML 元素作为响应。",number:"@:llm_serp.parameters.number",number_explain:"@:llm_serp.parameters.number_explain",opt_out_gfm:"Github 风格的 Markdown",opt_out_gfm_explain:"选择加入/退出 GFM（Github Flavored Markdown）功能。",page:"@:llm_serp.parameters.page",page_explain:"@:llm_serp.parameters.page_explain",pageshot:"页面快照",pageshot_explain:"返回整页截图的图片网址（尽力而为）。",post_with_url:"使用 POST 方法",post_with_url_explain:"使用 POST 方法代替 GET 方法，并在正文中传递 URL。对于使用基于哈希的路由构建 SPA 非常有用。",proxy:"使用特定国家/地区的代理服务器",proxy_explain:"设置基于位置的代理服务器的国家代码。使用“自动”进行最佳选择或使用“无”禁用。",proxy_server:"使用代理服务器",proxy_server_explain:"我们的 API 服务器可以利用您的代理来访问 URL，这对于只能通过特定代理访问的页面很有帮助。",reader_url:"@:reader.demo.reader_url",referenced_explain:"链接列于文末，在文中以数字引用。",references:"参考",references_explain:"以逗号分隔的用户提供的参考 (url) 列表",remove_all_images:"删除所有图片",remove_all_images_explain:"从响应中删除所有图片。",remove_selector:"CSS 选择器：排除",remove_selector_explain:"要删除的元素的 CSS 选择器（页眉、页脚等）。",respond_with:"使用 ReaderLM-v2",respond_with_explain:"使用 ReaderLM-v2 将 HTML 转换为 Markdown，为结构和内容复杂的网站提供高质量的结果。比其它引擎消耗 3 倍词元！",result_count:"数字",result_count_explain:"设置返回的最大结果数。使用 num 可能会导致延迟并排除特殊结果类型。除非您明确需要每页显示更多结果，否则请忽略。",return_format:"内容格式",return_format_explain:"您可以控制响应中的细节级别，以防止过度过滤。默认管道针对大多数网站和大模型输入进行了优化。",robot_txt:"严格遵守机器人政策",robot_txt_explain:"定义机器人用户代理 (User-Agent)，在获取内容之前对照 robots.txt 进行检查。",screenshot:"截屏",screenshot_explain:"返回第一个屏幕的图片URL。",search_engine:"搜索引擎",search_engine_explain:"选择搜索使用的引擎。影响结果的质量、速度和兼容性。",search_result_mode:"读取 SERP 的完整内容",search_result_mode_explain:"访问搜索结果中的每个 URL 并使用读取器返回完整内容。切换以启用更多特定于读取器的选项。",serp_type:"SERP 内容类型",serp_type_explain:"搜索的内容类型。目前属于高度实验性功能，不同类型计费方式不同。",set_cookie:"转发 Cookie",set_cookie_explain:"我们的 API 服务器可以在访问 URL 时转发您的自定义 Cookie 设置，这对于需要额外身份验证的页面非常有用。请注意，带有 Cookie 的请求不会被缓存。",setext:"数字符号标题",setext_explain:"在单词或短语前使用数字符号 (#) 来创建标题。",site_selector:"站内搜索",site_selector_explain:"仅返回指定网站或域的搜索结果。默认情况下，它会搜索整个网络。",stream_mode:"流模式",stream_mode_explain:"流模式有利于较大的目标页面，从而留出更多时间让页面完全呈现。如果标准模式导致内容不完整，请考虑使用流模式。",target_selector:"CSS 选择器：仅限",target_selector_explain:"用于定位特定页面元素的 CSS 选择器列表。",text:"文本",text_explain:"返回 document.body.innerText。",token_budget:"限制词元预算",token_budget_explain:"限制此请求使用的最大词元数。超出此限制将导致请求失败。",viewport:"视口配置",viewport_explain:"设置浏览器视口尺寸以实现响应式渲染。",vlm:"可变长度模型",vlm_explain:"非常适合具有丰富媒体和复杂布局的短页面。",wait_for_selector:"CSS 选择器：Wait-For",wait_for_selector_explain:"返回结果之前要等待的 CSS 选择器。",with_favicon:"获取图标",with_favicon_explain:"这将获取 SERP 中每个 URL 的图标，并将它们作为图片 URI 包含在响应中，这对于 UI 渲染很有用。",with_gfm:"已启用",with_gfm_explain:"已启用 GFM（Github Flavored Markdown）功能。",with_iframe:"iframe 提取",with_iframe_explain:"处理 DOM 树中所有嵌入 iframe 的内容。",with_shadow_dom:"影子 DOM 提取",with_shadow_dom_explain:"从文档中的所有 Shadow DOM 根中提取内容。",x_timeout:"超时时间",x_timeout_explain:"最大页面加载等待时间（不是总请求处理时间）。",your_query:"@:reader.demo.your_query"},how_to_stream:"要在内容可用时对其进行处理，请将请求标头设置为流模式。这可以最大限度地缩短收到第一个字节所需的时间。curl 中的示例：",how_to_use1:"将 https://r.jina.ai/ 添加到代码或工具中需要大模型访问的任何 URL。这将以干净、大模型友好的文本返回页面的主要内容。",how_to_use2:"将 https://s.jina.ai/ 添加到您的查询中。这将调用搜索引擎并返回其 URL 和内容，每个结果都以简洁、大模型友好的文本显示。",how_to_use3:"将 https://g.jina.ai/ 添加到您的语句中。这将调用判断引擎并返回真实性百分比、表示语句是真还是假的布尔值、原因摘要和参考列表。",key_required:"使用此端点需要 API 密钥",learn_more:"了解更多",open:"在新标签页中打开",params_classification:"参数",performance_compare:"性能比较",raw_html:"原始 HTML",reader_output:"读取器的输出",reader_response:"读取器反应",reader_search_hint:"如果您在代码中使用此 URL，请不要忘记对该 URL 进行编码。",reader_url:"读取器网址",reader_url_hint:"点击下面通过我们的读取器 API 获取内容",requires_post_method:"此功能需要 POST 方法。上传本地文件后，将自动启用 POST 方法。",response_time:"响应时间（毫秒）",search_params:"搜索参数",search_query_rewrite:"请注意，与上面的演示不同，在实践中，您不会在网上搜索原始问题来获取基础。人们经常做的是重写原始问题或使用多跳问题。他们读取检索到的结果，然后生成其他查询以根据需要收集更多信息，然后得出最终答案。",select_mode:"选择模式",show_read_demo:"了解 Reader 如何读取 URL",show_search_demo:"了解读取器如何搜索网络",slow_warning:"这可能需要长达 30 秒并且每个请求最多需要 300K 个词元。",standard_usage:"标准用法",stream_mode:"流模式",stream_mode_explain:"当目标页面很大而无法渲染时，流模式非常有用。如果您发现标准模式提供的内容不完整，请尝试流模式。",stream_mode_explain1:"当您发现标准模式提供的结果不完整时，流式模式很有用。这是因为流式模式将等待更长时间，直到页面完全呈现。使用 accept-header 切换流式模式：",tagline:"试用演示",try_demo:"演示",use_headers:"可以使用请求标头来控制 Reader API 的行为。以下是受支持标头的完整列表。",waiting_for_reader:"首先等待 Reader API 结果...",warn_grounding_message:"此过程可能需要长达 30 秒的时间，并且每个查论请求最多会消耗 300K 个词元。某些浏览器可能会因为较长的延迟而终止请求，因此我们建议复制代码并从终端运行它。",warn_grounding_title:"高延迟和词元使用率",your_query:"输入您的查询",your_query_hint:"输入需要最新信息或世界知识的问题。",your_statement:"您的事实核查声明",your_url:"输入您的 URL",your_url_hint:"点击下面直接获取页面源代码"},description:"读取URL或搜索为大模型提供更好的依据。",dont_panic_api_key_is_free:"不要惊慌！每个新的 API 密钥都包含一百万个免费词元！",engine_speed_test:"读取速度测试",faq_v1:{answer1:"读取器 API 是免费的，不需要 API 密钥。只需在您的 URL 前面添加“https://r.jina.ai/”即可。",answer10:"不可以，读取器 API 只能处理来自可公开访问的 URL 的内容。",answer11:"如果您在 5 分钟内请求相同的 URL，读取器 API 将返回缓存的内容。",answer12:"不幸的是没有。",answer13:"是的，您可以使用读取器中的原生 PDF 支持（https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4）或使用 arXiv 中的 HTML 版本（https://r.jina.ai/https://arxiv.org/html/2310.19923v4）",answer14:"Reader 为指定 URL 上的所有图片添加标题，并添加 `Image [idx]: [caption]` 作为 alt 标签（如果最初没有）。这使得下游大模型能够与图片进行推理、总结等交互。",answer15:"Reader API 的设计具有高度可扩展性。它根据实时流量自动扩展，最大并发请求数现在约为 4000。我们正在积极维护它，将其作为 Jina AI 的核心产品之一。因此，请放心在生产中使用它。",answer16:"请在下表中查找最新的速率限制信息。请注意，我们正在积极致力于改进 Reader API 的速率限制和性能，因此该表将进行相应更新。",answer17:"Reader-LM 是一种新型小型语言模型 (SLM)，专为从开放网络中提取和清理数据而设计。它将原始、嘈杂的 HTML 转换为干净的 markdown，灵感来自 Jina Reader。Reader-LM 注重成本效益和小模型尺寸，既实用又强大。它目前在 AWS、Azure 和 GCP 市场上可用。如果您有特定要求，请通过 sales AT jina.ai 联系我们。",answer2:"读取器 API 使用代理来获取任何 URL，并在浏览器中呈现其内容以提取高质量的主要内容。",answer3:"是的，读取器 API 是开源的，可以在 Jina AI GitHub 存储库中找到。",answer4:"读取器 API 通常会在 2 秒内处理 URL 并返回内容，但复杂或动态的页面可能需要更多时间。",answer5:"抓取可能很复杂且不可靠，尤其是复杂或动态页面。读取器 API 提供简洁、可靠的干净大模型级文本输出。",answer6:"读取器 API 返回 URL 原始语言的内容。它不提供翻译服务。",answer7:"如果您遇到阻止问题，请联系我们的支持团队寻求帮助和解决方案。",answer8:"虽然 读取器 API 主要用于网页，但它可以从 arXiv 等网站上以 HTML 格式查看的 PDF 中提取内容，但它并未针对一般 PDF 提取进行优化。",answer9:"目前，读取器 API 不处理媒体内容，但未来的增强功能将包括图片字幕和视频摘要。",question1:"使用 读取器 API 的相关费用是多少？",question10:"是否可以在本地 HTML 文件上使用 读取器 API？",question11:"读取器 API 是否缓存内容？",question12:"我可以使用 读取器API 来访问登录后的内容吗？",question13:"我可以使用读取器 API 访问 arXiv 上的 PDF 吗？",question14:"图片标注在读取器中如何发挥作用？",question15:"读取器的可扩展性如何？我可以在生产中使用它吗？",question16:"Reader API 的速率限制是多少？",question17:"什么是 Reader-LM？如何使用它？",question2:"读取器 API 如何发挥作用？",question3:"读取器 API 是开源的吗？",question4:"读取器 API 的典型延迟是多少？",question5:"为什么我应该使用 读取器 API 而不是自己抓取页面？",question6:"读取器 API 是否支持多种语言？",question7:"如果某个网站屏蔽了 读取器 API，我该怎么办？",question8:"读取器 API 可以从 PDF 文件中提取内容吗？",question9:"读取器 API 可以处理来自网页的媒体内容吗？",title:"与读取器相关的常见问题"},fast:"快速地",fast_stream:"即时数据流",fast_stream_description:"需要快速获取数据？我们的 读取器 API 可以传输数据以最大程度地减少延迟。",free:"永远免费",free_description:"读取器 API 是免费的！它不需要信用卡或 API 密码。它不会消耗您的词元配额。",is_free:"而且它是竟然是免费的！",is_free_description:"Reader API 可免费使用，并提供灵活的速率限制和定价。它建立在可扩展的基础架构上，具有高可访问性、并发性和可靠性。我们努力成为您大模型的首选基础解决方案。",lm_v2_description:"ReaderLM-v2 是一个 1.5B 参数语言模型，专门用于 HTML 到 Markdown 的转换和 HTML 到 JSON 的提取。它支持 29 种语言中多达 512K 个词元的文档，准确率比其前身高 20%。",lm_v2_title:"ReaderLM v2：从 HTML 到 Markdown 和 JSON 的小型语言模型",open:"在新标签页中打开",original_pdf:"原始 PDF",rate_limit:"速率限制",read_grounding_release_note:"读取发布说明",reader_also_read_images:"网页上的图片会使用读取器中的视觉语言模型自动添加标题，并在输出中格式化为图片 alt 标签。这为您的下游大模型提供了足够的提示，以将这些图片纳入其推理和总结过程。这意味着您可以询问有关图片的问题，选择特定的图片，甚至将其 URL 转发到更强大的 VLM 进行更深入的分析！",reader_description:"将 URL 转换为大模型友好输入，只需在前面添加 <code>r.jina.ai</code> 即可。",reader_do_grounding:"事实核查读取器",reader_do_grounding_explain:"新的基准端点提供端到端、近乎实时的事实核查体验。它获取给定的陈述，使用实时网络搜索结果对其进行基准化验，并返回事实性分数和使用的确切参考资料。您可以轻松对陈述进行基准化验，以减少大模型幻觉或提高人工编写内容的完整性。",reader_do_pdf_explain:"是的，Reader 本身支持 PDF 读取。它兼容大多数 PDF，包括包含大量图片的 PDF，而且速度极快！结合大模型，您可以轻松快速地构建 ChatPDF 或文档分析 AI。",reader_do_search:"用于网页搜索和 SERP 的读取器",reader_do_search_explain:"Reader 可用作 SERP API。它允许您将搜索结果引擎页面背后的内容提供给您的 LLM。只需在您的查询前面添加 <code>https://s.jina.ai/?q=</code>，Reader 就会搜索网络并返回前五个结果及其 URL 和内容，每个结果都以干净、LLM 友好的文本显示。这样，您就可以始终让您的 LLM 保持最新状态，提高其真实性，并减少幻觉。",reader_reads_images:"读取器也顺便识图！",reader_reads_pdf:"读取器还可以读取 PDF！",reader_result:"读取器结果",table:{td_1_0:"读取 URL 返回其内容，用于单渠道依据",td_1_1:"20 请求每分钟",td_1_2:"200 请求每分钟",td_1_3:"根据输出词元",td_1_4:"3 秒",td_1_5:"3 秒",td_2_0:"在网络上搜索返回前 5 个结果，有助于获得世界知识和搜索依据",td_2_1:"5 请求每分钟",td_2_2:"40 请求每分钟",td_2_3:"根据所有 5 个搜索结果的输出词元数",td_2_4:"10 秒",td_2_5:"10 秒",th0:"端点",th1:"描述",th2:"无 API 密钥的速率限制",th3:"使用 API 密钥进行速率限制",th4:"词元计数方案",th5:"平均延迟",th6:"平均延迟"},title:"读取器 API",usage:"用法",usage_details_false:"仅显示基本用法",usage_details_null:"显示基本用法和高级用法",usage_details_true:"仅显示高级用法",want_higher_rate_limit:"想要更高的速率限制（高达 1000 RPM）？我们可以为您提供支持！",what_is1:"什么是读取器？",what_is_answer_long:"将网络信息输入大模型是打好基础的重要一步，但这可能很有挑战性。最简单的方法是抓取网页并输入原始 HTML。但是，抓取可能很复杂且经常受阻，而且原始 HTML 中充斥着标记和脚本等无关元素。读取器 API 通过从 URL 中提取核心内容并将其转换为干净的、大模型友好的文本来解决这些问题，从而确保为您的Agent和 RAG 系统提供高质量的输入。",what_is_desc:"访问任何 URL 并提取其主要内容转换为针对大模型优化的文本。",which_engine_is_you:"哪种引擎适合您？",which_engine_is_you_explain:"不同的引擎针对不同的任务进行了优化，质量、速度、兼容性和成本各不相同。测试 URL 并选择最符合您需求的 URL。"},ke={confirm_message:"您的 API 密钥还剩 {_leftTokens} 个词元。将 {_numArticles} 篇文章的全文发送到重排器API，利用 {_selectedReranker} 模型去寻扎与当前页面的相关文章，这将显着消耗 API 密钥 {_APIKey} 的词元计数。您想继续吗？",confirm_title:"警告：大量消耗词元",out_of_quota:"此 API 密钥已用完词元。请为您的帐户充值或使用不同的 API 密钥。",recommend:"获取前 5 名",recommended_articles:"前 5 条类似文章"},ve={benchmark:{description0:"LlamaIndex 评估了 RAG 的向量模型和重排器器的各种组合，我们对起进行复现。研究结果显示 Jina 重排器 显着提高了搜索质量，这一优势与上游所使用的特定向量模型无关。",description1:"BIER（Benchmarking IR）评估模型的检索有效性，包括相关性和NDCG。较高的 BIER 分数与更准确的匹配和搜索结果排名相关。",description2:"通过 LoCo 基准测试，我们测量了模型对本地连贯性和上下文的理解，以及特定于查询的排名。 LoCo 分数越高，反映出识别相关信息并确定优先级的能力越好。",description3:"MTEB（多语言文本向量模型基准）总体上测试了模型在文本向量模型方面的能力，包括聚类、分类、检索和其他指标。然而，为了进行比较，我们仅使用 MTEB 的重排器任务。",title:"基准",title0:"LlamaIndex",title1:"BIER",title2:"LoCo",title3:"MTEB"},benchmark_description:"为了进行比较，我们在基准测试中纳入了 BGE (北京智源研究院)、BCE (网易有道) 和 Cohere 的其他三个领先的重排器。如下图所示，Jina 重排器 在所有相关重排类别中平均得分最高，在同行中处于明显领先地位。",benchmark_title:"性能基准测试",choose_turbo:"5倍提速的Turbo版本",choose_turbo_description:"我们还提供两个新的开源重排模型：jina-reranker-v1-turbo-en 和 jina-reranker-v1-tiny-en，后者只有 30M 个参数和 4 层。这两个新的重排器的推理速度比底座模型快 5 倍，而质量却只下降一点点。它们非常适合需要实时重排的应用程序。请读取下面的评测。",customize_urself:"试试改变它看看响应如何变化！",customize_urself_pl:"试试改变它们看看响应如何变化！",description:"世界一流的神经检索器，可最大限度地提高搜索相关性。",description_rich:"利用我们先进的重排 API 最大限度地提高搜索相关性和 RAG 准确性。",example_input_document:"待排序候选文档示例",example_input_query:"查询示例",faq_v1:{answer1:"重排器 API 的定价与我们的向量模型 API 定价结构一致。每个新 API 密钥都会获得 100 万个免费词元。除了免费词元之外，还可以购买不同的套餐。欲了解更多详情，请访问我们的定价部分。",answer10:"是的，我们的服务在 AWS、Azure 和 GCP 市场上可用。如果您有特定要求，请通过 sales AT jina.ai 联系我们。",answer11:"如果您对针对特定域数据量身定制的微调重排器感兴趣，请联系我们的销售团队。我们的团队将及时回复您的询问。",answer3:"<code>jina-reranker-v2-base-multilingual</code> 在多语言支持方面表现出色，性能优于 <code>bge-reranker-v2-m3</code>，吞吐量比 <code>jina-reranker-v1-base-en</code> 快 15 倍。它还支持代理任务和代码检索。<code>jina-colbert-v2</code> 在 <code>ColBERTv2</code> 的基础上进行了改进，检索性能提高了 6.5%，并增加了对 89 种语言的多语言支持。它具有用户控制的向量大小，以实现最佳效率和精度。",answer4:"是的，<code>jina-reranker-v2-base-multilingual</code> 和 <code>jina-colbert-v2</code> 都是基于 CC-BY-NC 4.0 协议开源。您可以自由地使用、共享和改编这些模型用于非商业用途。",answer5:"是的，<code>jina-reranker-v2-base-multilingual</code> 和 <code>jina-colbert-v2</code> 均支持 100 多种语言，包括英语、中文和其他全球主要语言。它们针对多语言任务进行了优化，并且表现优于以前的模型。",answer6:"最大查询词元长度为 512。文档没有词元限制。",answer7:"每个查询最多可以对 2048 个文档进行重排。",answer8:"与我们的向量模型 API 不同，没有批量大小的概念。每个请求只能发送一个查询文档元组，但该元组最多可以包含 2048 个候选文档。",answer9:"延迟从 100 毫秒到 7 秒不等，主要取决于文档和查询的长度。例如，使用 64 个词元的查询对 100 个包含 256 个词元的文档进行重排大约需要 150 毫秒。将文档长度增加到 4096 个词元将使时间增加到 3.5 秒。如果查询长度增加到 512 个词元，则时间进一步增加到 7 秒。",question1:"重排器 API 的费用是多少？",question10:"您的服务可以在 AWS、Azure 或 GCP 上私有化部署吗？",question11:"你们是否提供针对特定领域数据的微调重排器？",question3:"这两个重排器有什么区别？",question4:"Jina Rerankers 是开源的吗？",question5:"重新排序器是否支持多种语言？",question6:"查询和文档的最大长度是多少？",question7:"每个查询可以重排的最大文档数是多少？",question8:"批量大小是多少以及在一个请求中可以发送多少个查询文档元组？",question9:"对 100 个文档重排时，预计延迟会是多少？",title:"与 重排器 相关的常见问题"},feature_on_premises_description2:"在 AWS Sagemaker 上部署我们的重排模型，很快就会在 Microsoft Azure 和 Google Cloud Services 上部署，或者联系我们的销售团队，为您的虚拟私有云和本地服务器获取定制的 Kubernetes 部署。",feature_on_premises_description3:"在 AWS Sagemaker 和 Microsoft Azure 上部署 Jina Reranker，很快就会在 Google Cloud Services 上部署 Jina Reranker，或者联系我们的销售团队，为您的虚拟私有云和本地服务器获取定制的 Kubernetes 部署。",feature_solid_description:"由我们的尖端学术研究开发而成，并针对 SOTA 重排器器进行了严格测试，以确保无与伦比的性能。",how_it_works:"在搜索系统中，重排器的工作原理如下：",how_it_works_v1:{description1:"根据用户的查询，使用向量模型或BM25或TF-IDF等维度，在数据库中粗略匹配相关文档。",description2:"重排模型会获取这些初排结果，并在更精细的颗粒度上对文档和查询其进行相关性分析，同时考虑查询术语与文档内容交互等细微差别。",description3:"重排模型会将其认为最相关的结果放在顶部，从而改善搜索质量。",title1:"初始检索",title2:"重排",title3:"改善后的结果"},improve_performance:"搜索质量高人一等",improve_performance_description:"我们的评估表明，使用我们重排器的搜索系统得到了有效的改进，命中率提高了 8%，平均倒排(MRR)提高了 33%。",learning1:"了解重排器",learning1_description:"什么是重排模型？为什么向量搜索或两两余弦相似度还不够？通过我们的综合指南从头开始了解重排器。",read_more_about_benchmark:"读取有关基准测试的更多信息",read_more_about_turbo:"了解有关涡轮增压和微型模型的更多信息",read_more_about_v2:"Jina Reranker v2 是同类最佳的重排器，于 2024 年 6 月 25 日发布；它是为 Agentic RAG 构建的。它具有函数调用支持、超过 100 种语言的多语言检索、代码搜索功能，并且比 v1 的速度提高了 6 倍。了解有关 v2 模型的更多信息。",reranker_description:"尝试我们先进的重排器 API，最大限度地提高您的搜索相关性和 RAG 准确性。免费试用！",show_v2benchmark:"显示 v2 模型的基准（最新）",table:{number_token_document:"每个文档中的词元数量",number_token_query:"查询中的词元数量",title:"以下是对一个查询和 100 个文档进行重排的时间成本（以毫秒为单位）："},title:"重排器 API",top_n:"返回最优的重排文档数量",top_n_explain:"与查询最相关的文档的数量。",try_embedding:"免费使用向量模型 API",try_reranker:"免费试用重排器 API",v2_features:{description1:"Reranker v2 支持超过 100 种语言的文档检索，无论查询语言是什么。",description2:"Reranker v2 根据自然语言查询对代码片段和函数签名进行排名，非常适合 Agentic RAG 应用程序。",description3:"Reranker v2 根据自然语言查询对最相关的表进行排名，帮助对不同的表模式进行排序并在生成 SQL 查询之前确定最相关的表模式。",title1:"多语言检索",title2:"函数调用和代码搜索",title3:"表格和结构化数据支持"},v2benchmark:{descBeir:"针对 Beir 数据集的不同重排器报告的 NDCG 10 分数",descCodeSearchNet:"CodeSearchNet 数据集中不同重排模型的 MRR 10 得分报告",descMKQA:"回忆一下 MKQA 数据集中不同重排器报告的 10 个分数",descNSText2SQL:"回顾 NSText2SQL 数据集中不同重排器报告的 3 个分数",descRTX4090:"RTX 4090 GPU 上不同重新排名模型的吞吐量（50 毫秒内检索到的文档）分数报告。",descToolBench:"召回 ToolBench 数据集中不同重排器报告的 3 个分数",titleBeir:"BEIR（不同 IR 任务的异构基准测试）",titleCodeSearchNet:"CodeSearchNet。基准测试是文档字符串和自然语言格式的查询的组合，并带有与查询相关的标记代码段。",titleMKQA:"MKQA（多语言知识问答）",titleNSText2SQL:"NSText2SQL",titleRTX4090:"Jina Reranker v2 在 RTX4090 上的吞吐量",titleToolBench:"ToolBench。该基准测试收集了超过 16,000 个公共 API 以及相应的合成生成指令，以便在单 API 和多 API 设置中使用它们。"},vs_table:{col0:"重排器",col0_1:"增强的搜索精度和相关性",col0_2:"初始、快速过滤",col0_3:"跨广泛查询的一般文本检索",col1:"向量搜索",col1_1:"详细：子文档和查询段",col1_2:"广泛：整个文档",col1_3:"中级：各种文本片段",col2:"BM25",col2_1:"高的",col2_2:"中等的",col2_3:"低的",col3_1:"不需要",col3_2:"高的",col3_3:"低，利用预建索引",col4_1:"高的",col4_2:"高的",col4_3:"不需要",col5_1:"更适合细致入微的查询",col5_2:"效率与准确性之间的平衡",col5_3:"对于广泛的查询来说一致且可靠",col6_1:"高度准确，具有深入的上下文理解",col6_2:"快速高效，准确度适中",col6_3:"高度可扩展，具有既定的功效",col7_1:"资源密集且实施复杂",col7_2:"可能无法捕获深层查询上下文或细微差别",col7_3:"对于高度具体或上下文搜索可能表现不佳",header0:"最适合场景",header1:"粒度",header2:"查询时间复杂度",header3:"索引时间复杂度",header4:"训练时间复杂度",header5:"搜索质量",header6:"优势",header7:"弱点",subtitle:"下表提供了 重排器、向量搜索和 BM25 的全面比较，突出显示了它们在各个类别中的优缺点。",title:"重排器、向量搜索和 BM25 的比较"},what_is:"什么是重排器？",what_is_answer_long:`搜索的本质就是快速有效地找到最用户想要的结果。上世纪的BM25 或 tf-idf 等关键字匹配算法已成熟用在各类搜索结果进行排名。近几年来，基于向量模型的余弦相似度大放异彩，已在许多向量数据库成为标配。但这些方法的本质都相对简单，经常会忽略掉自然语言的微妙之处，最重要的是，忽略掉文档和查询意图之间的关联信息。

“重排模型”由此而生！重排模型实际是一种高级AI模型，它从搜索中获取初始候选集（通常由基于向量/基于词元的搜索结果提供）并重新评估它们与用户搜索意图之间的相关性。重排器超越了文字的表层匹配，探索查询和文档内容之间更深层次的交互。`,what_is_answer_long_ending:"重排器可以显着提高搜索质量，因为它在子文档和子查询级别运行，这意味着它会查看各个单词和短语、它们的含义以及它们在查询和文档中如何相互关联。这会产生一组更精确且与上下文相关的搜索结果。",what_is_desc:"重排器是一种AI模型，可优化BM25搜索或向量召回的搜索结果。通过我们的文章了解更多。"},Pe={caption_image_desc:"生成图片的文字描述。",caption_image_title:"标题图片",description:"探索每一个像素背后的故事",example1:"该视频似乎是一段自然镜头，其中有一只迷人的白色兔子和一只蝴蝶在草地上。兔子以不同的方式与蝴蝶互动，展示了它们独特的关系。周围的自然环境提供了风景如画的背景，增强了这个简单而迷人的场景的美丽。",generate_story_desc:"根据图片创作一个故事，通常以人物的对话或独白为特色。",generate_story_title:"生成故事",intro1:"独领风骚的图片视频理解AI",json_image_desc:"使用预定义的架构从图片生成结构化 JSON 格式。这允许从图片中提取特定的数据。",json_image_title:"从图片中提取 JSON",summarize_video_desc:"生成视频的简洁摘要，突出显示关键事件。",summarize_video_title:"总结视频",visual_q_a_desc:"根据图片内容回答查询。",visual_q_a_title:"视觉问答"},we={ask_deepsearch:"向深度搜索提问...",ask_on_current_page:"向当前页提问...",find_solution:"生成相应的解决方案...",hint:"搜索产品、新闻和您的问题",hotkey:"按 / 键可在本页提问",hotkey1:"按",hotkey2:"切换",hotkey_long1:"在任何页面，按下",hotkey_long3:"打开搜索栏",more_results:"另外 {_numMore} 个结果",placeholder:"对本页内容提问",proposing_solution:"根据当前页面内容生成答案...",required:"请更详细地描述您的问题。",results:"结果"},ye={description:"导航、交互、优化：重新定义产品搜索"},fe={beta:"实验"},xe={description:"弥合现有搜索底座设施中的语义差距"},Le={"Hacker News":"HN新闻",LinkedIn:"领英",facebook:"脸书",reddit:"红迪网",rss:"RSS订阅",share_btn:"分享",twitter:"X（前推特）"},Re={click_to_learn_more:"点击了解更多",contextualization:"上下文理解",contextualization_desc:"重排器根据查询的深度上下文相关性调整初始搜索结果。这可以优化排名，以更好地匹配用户可能认为有用的内容。",coreInfra:"核心基础设施",coreInfra_desc:"Core infra 提供了一个云原生层，用于在公共云和本地开发、部署和编排搜索底座模型，使服务能够毫不费力地扩大和缩小。",embedding_serving:"大模型向量部署",embedding_serving_description:"使用云原生技术通过强大、可扩展的微服务部署大模型向量推理。",embedding_tech:"向量模型",embedding_tech_description:`在Jina AI，我们正通过向量模型技术彻底转变人工智能应用的面貌。这种技术能够作为一种统一的手段，有效地表达和压缩多种类型的数据，同时确保关键信息不会丢失。我们的核心目标是将复杂的数据集转化为普遍易懂的向量模型格式，为精准和深入的人工智能分析提供支持。

向量模型在AI领域扮演着基础但至关重要的角色。在精确图片识别和语音识别等领域，它们帮助我们识别更为微妙的细节和差异。在自然语言处理中，它们能增强对上下文和情感的理解，使对话式AI和语言翻译工具更加准确。此外，在构建复杂的推荐系统时，这些系统需要对不同内容形式（如文本、音频和视频）的用户偏好有深入的了解，而向量模型在这方面发挥着关键作用。`,embedding_tuning:"大模型向量精调",embedding_tuning_description:"通过代入专业知识和行业数据来训练高质量的大模型向量，以增强特定任务上的性能。",embeddings:"向量模型",embeddings_desc:"向量模型是现代搜索系统的基石，将多模态数据表示为数字向量。此过程使内容的理解更加细致入微，远超简单的关键字匹配。",for_developers:"对于开发者",for_enterprise:"对于企业",for_power_users:"对于高级用户",grounding:"溯源",grounding_desc:"读取器通过大模型完善输入和结果。它们提高了最终答案的质量、可读性和真实性。",model_serving:"模型部署",model_serving_description:"在生产环境中部署调优模型，通常需要大量资源，例如 GPU 托管。 MLOps 强调以可扩展、高效和可靠的方式服务中型到大型模型。",model_tuning:"模型调优",model_tuning_description:"于特定任务的数据集上调整预训练模型的参数，以提高其性能并使其适应特定应用程序。",personalization:"个性化",personalization_desc:"使用用户指令引导的合成数据自动训练特定领域的向量化和重排器。",preprocessing:"预处理",preprocessing_desc:"预处理包括清理、规范化和将原始数据转换为搜索系统可理解的格式。",promptOps:"提示词工作流",promptOps_desc:"Prompt Ops 改进了搜索系统的输入和输出，包括用于查询扩展、大模型输入和结果重写的输入和输出。这确保搜索更容易理解，结果也更好。",prompt_serving:"提示词部署",prompt_serving_description:"通过 API 包装和提供提示，无需托管重型模型。该 API 调用公共大型语言模型服务并处理操作链中输入和输出的编排。",prompt_tech:"提示词和智能体工程",prompt_tech_description:`在Jina AI，我们深知提示词工程在与大型语言模型（LLM）交流中的重要性。随着这些模型的不断进化，我们的提示词也变得越来越复杂，涵盖了深入的推理和逻辑思维。这种进步彰显了LLM和提示词工程之间相互加强的关系。

展望未来，我们相信LLM将成为编译器的角色，而提示词则将演变成新型的编程语言。这意味着，未来的技术技能可能更侧重于掌握提示词的艺术，而不仅仅是传统的编程技巧。在Jina AI，我们的目标是引领这场技术变革，通过精通这种新兴的“语言”，让先进的人工智能变得更加易于理解和应用。`,prompt_tuning:"提示词调优",prompt_tuning_description:"精心设计和完善输入提示的过程，以引导其输出达到特定的、期望的响应。",representation:"表征学习",representation_desc:"向量化将多模态数据转换为统一的向量格式。这使搜索系统能够理解和分类简单关键字以外的内容。",rerankers:"重排器",rerankers_desc:"重排器会从向量模型中获取初始结果并对其进行优化，确保向用户呈现最相关的结果。这对于提供符合用户意图的高质量搜索结果至关重要。"},Me={care_most:"你最关心什么？",care_most_options:{accuracy:"准确性",cost:"成本",other:"其他",scalability:"吞吐量",speed:"速度"},care_most_required:"选择API服务时，您最关心什么？",company_size:"贵公司规模有多大？",company_size_required:"告诉我们您公司的规模有助于我们提供更好的服务",company_url:"贵公司的网站是什么？",company_url_required:"告诉我们您公司的网站有助于我们提供更好的服务",contactName:"你的名字",contactName_required:"我们该如何称呼你呢？",contactTitle:"您在公司内如何任职？",contactTitle_required:"您的职位名称为必填项",contact_us:"联系我们",domain_required:"告诉我们您的工作领域有助于我们提供更好的服务",email:"电子邮件",email_contact:"您的联系邮箱",email_invalid:"电子邮件无效",email_required:"电子邮件为必填项",fine_tuned_embedding:"想要您私域数据专属向量模型？我们随时恭候！",fine_tuned_reranker:"想要您私域数据上的专属重排器？来！我们讨论一下！",full_survey:"参加完整的调查并获得我们团队的更快回复",get_new_key:"获取 API 密钥",get_update_blog_posts:"提醒我博客文章的最新更新",get_update_embeddings:"提醒我向量模型的最新消息",send:"发送",sign_up:"订阅",subscribe:"订阅",tell_domain:"您的领域或微调方向",usage_type:"哪种用法最能描述您？",usage_type_options:{other:"其他",poc:"原型设计或概念验证",production:"生产环境",research:"研究阶段"},usage_type_required:"告诉我们您的使用类型有助于我们提供更好的服务",used_product:"您使用的是哪个模型？",used_product_required:"选择您正在使用或您感兴趣的模型"},qe={description:"增强你的大模型并将其推向极限"},Ce="目录",je={advance_usage:"使用 POST 请求获取更多功能",basic_usage:"使用 GET 请求直接返回词元数量",basic_usage_explain:"您可以简单地发送一个 GET 请求来计算文本中的词元数量。",change_content:"更改“content”参数并查看实时结果",chars:"字符",chinese:"中文",chunk:"切块",chunk_all:"所有区块",chunking:"对长文档进行切块，快如闪电鞭！",chunking_explain:"您还可以使用切分器将长文档分割成较小的块，从而更轻松地在向量模型或重排器中处理它们。我们利用常见的结构线索并构建了一套规则和启发式方法，这些规则和启发式方法在不同类型的内容（例如 Markdown、HTML、LaTeX 和 CJK 语言）中表现良好。",chunking_short:"切块",chunks_in_total:"总共 {_numChunks} 个切块",count_tokens_hint:"<b>{_numTokens}</b> 个词元，{_numChars} 个字符。",description:"将长文本切分成块或词元。",description_long:"我们的切分器对于帮助大模型在上下文限制内管理输入以及优化模型性能至关重要。它允许开发人员计算词元并提取相关文本段，从而确保高效的数据处理和成本管理。",description_long1:"用于将长文本分割成块并进行切词的免费 API。",english:"英语",explain:"分段器是将文本转换为词元或块的关键组件，它们是向量模型/重排器或大模型处理的基本数据单位。词元可以表示整个单词、单词的一部分，甚至是单个字符。",faq_v1:{answer1:"切分器可免费使用。通过提供您的 API 密钥，您可以访问更高的速率限制，并且不会向您的密钥收费。",answer10:"除了西方语言外，分块技术还适用于中文、日语和韩语。",answer2:"如果没有 API 密钥，您可以以 20 RPM 的速率限制访问切分器。",answer3:"使用 API 密钥，您可以以 200 RPM 的速率限制访问切分器。对于高级付费用户，速率限制为 1000 RPM。",answer4:"不可以，您的 API 密钥仅用于访问更高的速率限制。",answer5:"是的，切分器是多语言的，支持超过 100 种语言。",answer6:"GET 请求仅用于计算文本中的词元数，可让您轻松将其作为计数器集成到应用程序中。POST 请求支持更多参数和功能，例如返回第一个/最后一个 N 个词元。",answer7:"每个请求最多可以发送 64k 个字符。",answer8:"切块功能可根据常见的结构线索将长文档分割成较小的块，从而确保将文本准确地分割成有意义的块。本质上，它是一个（大！）正则表达式模式，可根据某些通常与语义边界一致的句法特征（例如句子结尾、段落分隔符、标点符号和某些连词）对文本进行分割。它不是语义切块。这个（大）正则表达式在正则表达式的限制范围内尽可能强大。它平衡了复杂性和性能。虽然正则表达式无法实现真正的语义理解，但它可以通过常见的结构线索很好地近似上下文。",answer9:"如果输入包含特殊词元，我们的切分器会将它们放入“special_tokens”字段中。这样您就可以轻松识别它们并根据下游任务进行相应的处理，例如在将文本输入大模型之前将其删除以防止注入攻击。",question1:"切分器的价格是多少？",question10:"分块是否支持英语以外的其他语言？",question2:"如果我不提供 API 密钥，速率限制是多少？",question3:"如果我提供 API 密钥，速率限制是多少？",question4:"您会从我的 API 密钥中收取词元吗？",question5:"切分器是否支持多种语言？",question6:"GET 和 POST 请求有什么区别？",question7:"每个请求可以切词的最大长度是多少？",question8:"切块功能如何工作？是语义切块吗？",question9:"如何在切分器中处理诸如“endoftext”之类的特殊词元？",title:"与分段器相关的常见问题"},free_api:"切分器可免费使用。通过提供您的 API 密钥，您可以访问更高的速率限制，并且不会向您的密钥收费。",input_text:"输入文本",is_free:"切分器是免费的！",is_free_description:"通过提供您的 API 密钥，您可以访问更高的速率限制，并且不会对您的密钥收费。",japanese:"日语",korean:"韩语",parameters:{auth_token:"添加 API 密钥以实现更高的速率限制",auth_token_explain:"输入您的Jina API密钥以访问更高的速率限制。有关最新速率限制信息，请参阅下表。",head:"返回前 N 个词元",head_explain:"返回给定内容的前 N 个词元。不包括恰好切在的边界点。不能与“tail”一起使用。",learn_more:"了解更多",max_chunk_length:"每个块的最大长度",max_chunk_length_explain:"每个块中的最大字符数。实际上，如果文本中有自然边界，块长度可以小于此值。",return_chunks:"返回切块",return_chunks_explain:"将输入切为具有语义意义的片段，根据常见的结构线索，使用启发式规则适应各种文本类型和边缘情况。",return_tokens:"是否返回分词结果",return_tokens_explain:"在响应中返回词元及其对应的 id。切换以查看结果可视化。",tail:"返回最后 N 个词元",tail_explain:"返回给定内容的最后 N 个词元。不包括恰好切在的边界点。不能与“head”一起使用。",type:"切分器",type_explain:"选择要使用的切分器。",used_by_models:"用于 {_usedBy}。"},remove_boundary_cues:"删除换行符",remove_boundary_cues_explain:"从输入中删除所有换行符（主要边界提示），这会使问题更具挑战性，并查看响应如何变化！",show_space:"显示前导/尾随空格",table:{td_1_0:"对文本进行分词、计数并获取第一个/最后一个 N 个词元。",td_1_1:"20 转/分",td_1_2:"200 转/分",td_1_3:"1000 转/分",td_1_4:"免费",td_1_5:"800毫秒"},title:"切分器 API",token_index:"词元代码：{_index}",usage:"用法",visualization:"可视化",what_is:"什么是切分器？"},Se={cta:"翻译成{_lang}代码",select_language:"语言"},Te={description:"您只需要一个 Python 向量数据库 - 不多也不少"},Je="zzz",Be={PRODUCT_DESCRIPTION:e,SEO_TAG_LINE:n,about_us_page:a,api_general_faq:i,autotune:t,avatar:r,best_banner:o,beta:s,billing_general_faq:_,blog_tags:d,book2024:l,cclicence:c,classifier:p,clip_as_service:u,cloud:m,contact_us_page:g,copy:b,copy_to_clipboard_success:A,dalle_flow:h,deepsearch:I,"dev-gpt":{description:"您的虚拟开发团队"},disco_art:k,doc_array:v,download:P,embedding:w,embeddings:y,estimator:f,faq:x,faq_button:L,farewell:R,finetuner:M,finetuner_plus:q,finetuning:C,footer:j,get_new_key:S,github:T,grounding:J,header:B,hub:E,huggingface:G,impact_snapshots:U,inference:z,insufficient_error_message:O,integrations:D,internship_faq:H,internship_page:N,jcloud:F,jerboa:W,jina:K,jina_chat:Q,key_manager:X,lab_dialog:Y,landing_page:V,langchain_serve:$,legal_page:Z,llm_serp:ee,model_graph:ne,models:ae,news_page:ie,newsroom_page:te,notice:re,open_day:oe,open_day_faq:se,open_gpt:_e,paywall:de,powered_by:le,print:ce,project_status:pe,prompt_perfect:ue,promptperfect:me,purchase:ge,purchase_now:be,rate_limit:Ae,rationale:he,reader:Ie,recommender:ke,reranker:ve,scenex:Pe,searchbar:we,searchscape:ye,sefo_api:fe,semantic:xe,share:Le,spectrum:Re,subscribe_system:Me,think_gpt:qe,toc:Ce,tokenizer:je,translator:Se,vectordb:Te,zzz:Je};export{e as PRODUCT_DESCRIPTION,n as SEO_TAG_LINE,a as about_us_page,i as api_general_faq,t as autotune,r as avatar,o as best_banner,s as beta,_ as billing_general_faq,d as blog_tags,l as book2024,c as cclicence,p as classifier,u as clip_as_service,m as cloud,g as contact_us_page,b as copy,A as copy_to_clipboard_success,h as dalle_flow,I as deepsearch,Be as default,k as disco_art,v as doc_array,P as download,w as embedding,y as embeddings,f as estimator,x as faq,L as faq_button,R as farewell,M as finetuner,q as finetuner_plus,C as finetuning,j as footer,S as get_new_key,T as github,J as grounding,B as header,E as hub,G as huggingface,U as impact_snapshots,z as inference,O as insufficient_error_message,D as integrations,H as internship_faq,N as internship_page,F as jcloud,W as jerboa,K as jina,Q as jina_chat,X as key_manager,Y as lab_dialog,V as landing_page,$ as langchain_serve,Z as legal_page,ee as llm_serp,ne as model_graph,ae as models,ie as news_page,te as newsroom_page,re as notice,oe as open_day,se as open_day_faq,_e as open_gpt,de as paywall,le as powered_by,ce as print,pe as project_status,ue as prompt_perfect,me as promptperfect,ge as purchase,be as purchase_now,Ae as rate_limit,he as rationale,Ie as reader,ke as recommender,ve as reranker,Pe as scenex,we as searchbar,ye as searchscape,fe as sefo_api,xe as semantic,Le as share,Re as spectrum,Me as subscribe_system,qe as think_gpt,Ce as toc,je as tokenizer,Se as translator,Te as vectordb,Je as zzz};

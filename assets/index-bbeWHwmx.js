const e="Proporcionamos las mejores incorporaciones, reclasificadores, lectores de LLM y optimizadores rápidos de su clase, siendo pioneros en la búsqueda de IA para datos multimodales.",a="¡Tu base de búsqueda, a todo motor!",o={approach:"Nuestro enfoque",approach_connect_dots:"Conectando los puntos: usuarios avanzados para empresas",approach_connect_dots_description:"Entonces, ¿por qué es esencial un enfoque en el usuario avanzado para nuestro modelo centrado en la empresa? Porque se trata de establecer relaciones tempranas. Al atender a los usuarios avanzados ahora, estamos construyendo puentes hacia las empresas en las que influirán en el futuro. Es una jugada estratégica: una inversión a largo plazo para garantizar que nuestra oferta empresarial siga siendo una prioridad cuando estos usuarios avanzados asciendan a roles de toma de decisiones dentro de las organizaciones.",approach_content1:"En el mundo de la IA en rápida evolución, las estrategias deben ser ágiles y con visión de futuro. Si bien nuestra oferta principal sigue centrada en las empresas, el panorama de la IA ha cambiado de manera que es necesario repensar nuestro enfoque para la adquisición de clientes. He aquí por qué presentar a los usuarios avanzados como el punto de entrada de nuestro embudo no solo es innovador, sino crucial para nuestro crecimiento sostenido en el sector empresarial.",approach_content2:"En Jina AI, nuestra estrategia es ser proactivos en lugar de reactivos. La inclusión de usuarios avanzados como punto de entrada del embudo garantiza que no solo capturamos las tendencias actuales del mercado, sino que también estamos estratégicamente preparados para el crecimiento empresarial futuro. Nuestro compromiso con las empresas sigue siendo inquebrantable; sin embargo, nuestro enfoque para llegar a ellos es innovador, sólido y, sobre todo, con visión de futuro.",approach_content4:'Todos queremos una mejor búsqueda. En Jina AI, facilitamos una mejor búsqueda al proporcionar la <span class="text-primary text-bold">Base de búsqueda</span>, que consta de incrustaciones, reclasificadores, lectores y operaciones de aviso. Estos componentes funcionan en conjunto para revolucionar la forma en que buscamos y entendemos los datos.',approach_miss_mark:"Por qué los MLOps tradicionales no dan en el blanco",approach_miss_mark_description:"Si bien la afluencia de usuarios avanzados es significativa, las herramientas tradicionales de MLOps están mal equipadas para satisfacer sus necesidades. Estas herramientas recuerdan el uso de un tractor para moverse por las calles de la ciudad: son pesadas y, a menudo, excesivas. Los desarrolladores de nueva generación exigen herramientas ágiles e intuitivas que complementen su rápido ritmo de desarrollo.",approach_new_paradigm:"Tecnología basada en avisos: un nuevo paradigma",approach_new_paradigm_description:`2023 anunció un cambio significativo: el surgimiento de la tecnología basada en avisos. Al simplificar el proceso de desarrollo de IA, ha democratizado el acceso a las herramientas de IA. Ahora, aquellos que no tienen una amplia experiencia en programación, denominados "usuarios avanzados", pueden participar en el desarrollo de IA sin las pronunciadas curvas de aprendizaje asociadas con herramientas como Pytorch, Docker o Kubernetes.

Trazando un paralelo, esto es similar a la evolución de la computación personal. Inicialmente, solo los expertos en tecnología operaban computadoras. Pero con la llegada de las interfaces fáciles de usar, podría participar una audiencia más amplia. Hoy, con la tecnología basada en avisos, estamos presenciando una democratización similar en la IA.`,awards:"Premios y reconocimientos",berlin:"Berlín, Alemania (sede central)",berlin_address:"Prinzessinnenstraße 19-20, 10969 Berlín, Alemania",berlin_address2:"Geschäftsanschrift: Leipzigerstr. 96, 10117 Berlín, Alemania",bj:"Beijing, China",bj_address:"Piso 5, Edificio 6, No.48 Haidian West St. Pekín, China",brochure_info:"Su guía de nuestra empresa le espera",description:"El futuro comienza aquí.",download_brochure1:"Descargar folleto",download_docarray_logo:"Descargue el logotipo de DocArray",download_docarray_logo_desc:"Acceda al logotipo de DocArray, un proyecto de código abierto iniciado por Jina AI y contribuido a la Fundación Linux en diciembre de 2022. Disponible en modos claro y oscuro, en formatos PNG y SVG.",download_jina_logo:"Descargue el logotipo de Jina AI",download_jina_logo_desc:"Obtenga el logotipo de Jina AI en modo claro y oscuro, disponible en formatos PNG y SVG. Este logotipo es una marca registrada en la Oficina de Propiedad Intelectual de la Unión Europea (EUIPO).",download_logo:"Descargar logotipos",employees:"Los empleados hoy",empower_developers:"Desarrolladores empoderados",fastApiCaption:"Contribuyó con más de $ 20,000 desde 2021.",founded:"Fundado",founded_in:"Fundado en",investors:"Nuestros inversores",linuxFoundationCaption:"Realiza un aporte anual de $10,000 a partir de 2022.",many:"Muchos",media:{video:"Entrevista en video"},mission:"Nuestra misión",mission_content1:"Nosotros Nuestras tecnologías clave, que incluyen el ajuste rápido, el servicio rápido, el ajuste de modelos y el servicio de modelos, encarnan nuestro compromiso de democratizar el acceso a la IA. A través de nuestra iniciativa de código abierto, nos esforzamos por fomentar la innovación, la colaboración y la transparencia, garantizando soluciones escalables, eficientes y sólidas. Jina AI es más que una simple empresa; es una comunidad dedicada a capacitar a las empresas para que enfrenten los desafíos dinámicos de la era digital y prosperen en sus dominios.",mission_content2:"En el corazón de Jina AI se encuentra nuestra misión de ser el portal hacia la IA multimodal para una clientela diversa, desde usuarios avanzados y desarrolladores hasta empresas. Creemos profundamente en el poder del código abierto y estamos dedicados a crear herramientas avanzadas y accesibles para la comunidad de IA. Nuestras tecnologías clave, que incluyen el ajuste rápido, el servicio rápido, el ajuste integrado y el servicio integrado, encarnan nuestro compromiso de democratizar el acceso a la IA. A través de nuestra iniciativa de código abierto, nos esforzamos por fomentar la innovación, la colaboración y la transparencia, garantizando soluciones escalables, eficientes y sólidas. Jina AI es más que una simple empresa; es una comunidad dedicada a capacitar a las empresas para que enfrenten los desafíos dinámicos de la era digital y prosperen en sus dominios.",mission_content3:"En Jina AI, nuestra misión es liderar el avance de la IA multimodal a través de tecnologías innovadoras de integración y basadas en avisos, centrándonos específicamente en áreas como el procesamiento del lenguaje natural, el análisis de imágenes y videos y la interacción de datos intermodales. Esta especialización nos permite ofrecer soluciones únicas que convierten datos complejos de múltiples fuentes en conocimientos prácticos y aplicaciones innovadoras.",mit_report_title:"Multimodal: la nueva frontera de la IA",mit_techreview:"Revisión de tecnología del MIT",numfocusCaption:"Dona regularmente cada mes a partir de 2022.",office:"Nuestras oficinas",otherProjectsCaption:"Donó más de $ 3,000 a través del patrocinio de Github.",our_answer:"Absolutamente Yann. ¡Estamos en ello, construyendo puentes hacia un futuro de IA multimodal!",pythonSoftwareFoundationCaption:"Proporcionó una donación única de $ 10,000 y patrocinó múltiples eventos de PyCon, incluidos los de Alemania, Italia, China y los EE. UU.",sectors:{ecommerceRetail:"Mercados de próxima generación",ecommerceRetail_description:"Los líderes del comercio electrónico y la venta minorista se asocian con Jina AI para ofrecer recomendaciones precisas de productos y experiencias de búsqueda detalladas. Nuestras integraciones multilingües y rerankers impulsados por IA ayudan a optimizar el descubrimiento, impulsar la conversión y reducir el tiempo de obtención de información para los catálogos de productos globales.",ecommerceRetail_short:"Comercio electrónico y venta minorista",financeConsulting:"Asesores y analistas visionarios",financeConsulting_description:"Las empresas financieras y las consultoras aprovechan la limpieza de datos a gran escala y el entrenamiento de modelos específicos de dominio de Jina AI para obtener información en tiempo real. Nuestras licencias empresariales y las implementaciones locales seguras les permiten mantener la confidencialidad y, al mismo tiempo, beneficiarse de la recuperación y el análisis avanzados.",financeConsulting_short:"Fin & Consulta",media:"Creadores de contenido cautivadores",media_description:"Las organizaciones de medios utilizan Jina AI para transformar grandes cantidades de recursos multimedia en conocimiento que se puede buscar, lo que agiliza la investigación interna y enriquece las experiencias de los usuarios. Nuestros servicios especializados de lectura y reclasificación garantizan un descubrimiento preciso y contextualizado en artículos, videos y archivos.",media_short:"Medios de comunicación",misc:"Pioneros con visión de futuro",misc_description:"Las organizaciones que abarcan la educación, la agricultura, el sector inmobiliario y más aprovechan las soluciones flexibles de Jina AI para limpiar, extraer y transformar datos a gran escala. Al aprovechar nuestra innovadora pila de recuperación neuronal, descubren nuevas posibilidades y se mantienen a la vanguardia en sus respectivos campos.",misc_short:"Otros",technology:"Innovadores tecnológicos pioneros",technology_description:"Los líderes en software, nube, IA y datos confían en las soluciones de búsqueda neuronal de Jina AI para impulsar sus sistemas de búsqueda basados en LLM, RAG y agentes de IA. Nuestros lectores avanzados, integraciones, rerankers y pequeños LM los ayudan a pasar de la prueba de concepto a la preparación para la empresa más rápido que nunca.",technology_short:"Tecnología"},sefo:{layer0:"Aplicaciones de usuario final",layer1:"RAG / orquestación",layer3:"GPU/móvil/borde/computación local"},segmentFaultCaption:"Contribuyó con una donación única de $ 6,000.",show_position:"¿Cómo buscar posiciones en el ecosistema?",stats_1:"Fundada en febrero de 2020, Jina AI se ha convertido rápidamente en pionera mundial en tecnología de IA multimodal. En un período impresionante de 20 meses, recaudamos con éxito $37,5 millones, lo que marca nuestra sólida posición en la industria de la IA. Nuestra tecnología innovadora, de código abierto en GitHub, ha permitido a más de 40 000 desarrolladores de todo el mundo crear e implementar aplicaciones multimodales sofisticadas sin problemas.",stats_2:"En 2023, hemos logrado avances significativos en el avance de las herramientas de generación de IA basadas en tecnología multimodal. Esta innovación ha beneficiado a más de 250 000 usuarios en todo el mundo, atendiendo a una plétora de requisitos comerciales únicos. Desde facilitar el crecimiento empresarial y mejorar la eficiencia operativa hasta optimizar los costos, Jina AI se dedica a empoderar a las empresas para que sobresalgan en la era multimodal.",stats_4:'Fundada en 2020, Jina AI es una empresa líder en inteligencia artificial para búsquedas. Nuestra plataforma <span class="text-primary text-bold">Search Foundation</span> combina incrustaciones, rerankers y pequeños modelos de lenguaje para ayudar a las empresas a crear aplicaciones de búsqueda multimodal y GenAI confiables y de alta calidad.',stats_v1:"Buscar/cuenta",subtitle:"Revolucionando la creación de contenido a través de soluciones generadas por IA para desbloquear infinitas posibilidades. Dando forma al futuro del contenido generado por IA y mejorando la creatividad humana.",sues_und_sauer:"Sué y Sauer",sues_und_sauer_tooltip:"Süß-Sauer, un sabor popular (aunque estereotipado) en la cocina chino-alemana, significa agridulce. Es una metáfora de los altibajos de la vida de una startup.",sunnyvale_address:"710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, EE. UU.",sz:"Shenzhen, China",sz_address:"Piso 402, Edificio de Tecnología Fu'an, Shenzhen, China",team:"Dentro del Portal de Jina AI",team_content1:"Desde diversos rincones del mundo, estamos construyendo el futuro de la IA. Nuestras distintas perspectivas enriquecen nuestro trabajo y generan innovaciones. Dentro de este portal, abrazamos nuestra individualidad y perseguimos apasionadamente nuestros sueños. Bienvenido al portal del futuro de la IA.",team_join:"Únete a nosotros",team_size:"Estas fotografías incluyen a nuestros antiguos compañeros y pasantes; agradecemos a cada uno de ellos.",technologies:"Tecnologías",title:"Acerca de Jina AI",title0:"El futuro",title1:"Empieza",title2:"Aquí",title3:"Comienza aquí",understand_our_strength:"Comprender nuestra fuerza",understand_our_view2:"Comprender la Fundación de Búsqueda",users:"Usuarios Registrados",value:"Nuestros premios",value_content1:"No nos conformamos. No hacemos concesiones. Buscamos la excelencia.",vision:"Nuestra misión",vision_content1:"Inspirado por la idea de Yann LeCun de que '",vision_content3:'El futuro de la IA es <span class="text-primary text-bold">multimodal</span> y nosotros somos parte de él. Sabemos que las empresas enfrentan desafíos al aprovechar los datos multimodales. En respuesta, estamos comprometidos con la <span class="text-primary text-bold">Search Foundation</span> para ayudar a las empresas y desarrolladores a buscar mejor y utilizar datos multimodales para el crecimiento empresarial.',yannlecun_quote:"Un sistema de inteligencia artificial entrenado solo con palabras y oraciones nunca se aproximará a la comprensión humana."},n={answer1:"Sí, la misma clave API es válida para todos los productos de la base de búsqueda de Jina AI. Esto incluye las API de lectura, incrustación, reclasificación, clasificación y ajuste, con tokens compartidos entre todos los servicios.",answer10:'Esto se debe a que nuestra arquitectura sin servidor descarga ciertos modelos durante períodos de bajo uso. La solicitud inicial activa o "calienta" el modelo, lo que puede tardar unos segundos. Después de esta activación inicial, las solicitudes posteriores se procesan mucho más rápido.',answer12:"Cumplimos con una estricta política de privacidad y no utilizamos los datos ingresados por los usuarios para entrenar nuestros modelos. También cumplimos con los estándares SOC 2 Tipo I y Tipo II, lo que garantiza altos estándares de seguridad y privacidad.",answer3:'Sí, el uso de tokens se puede monitorear en la pestaña "Clave API y facturación" ingresando su clave API, lo que le permite ver el historial de uso reciente y los tokens restantes. Si ha iniciado sesión en el panel de API, estos detalles también se pueden ver en la pestaña "Administrar clave API".',answer4:"Si ha perdido una clave recargada y desea recuperarla, comuníquese con el servicio de asistencia de jina.ai con su correo electrónico registrado para recibir asistencia. Se recomienda iniciar sesión para mantener su clave API almacenada de forma segura y de fácil acceso.",answer5:"No, nuestras claves API no tienen fecha de vencimiento. Sin embargo, si sospecha que su clave ha sido comprometida y desea retirarla, comuníquese con nuestro equipo de soporte para obtener ayuda. También puede revocar su clave en el <a class='text-primary' href='https://jina.ai/api-dashboard'>panel de administración de claves API</a>.",answer6:"Sí, puedes transferir tokens de una clave premium a otra. Después de iniciar sesión en tu cuenta en el <a class='text-primary' href='https://jina.ai/api-dashboard'>panel de administración de claves API</a>, usa la configuración de la clave que deseas transferir para mover todos los tokens pagos restantes.",answer7:"Sí, puedes revocar tu clave API si crees que se ha visto comprometida. Al revocar una clave, se deshabilitará de inmediato para todos los usuarios que la hayan almacenado, y todo el saldo restante y las propiedades asociadas quedarán inutilizables de forma permanente. Si la clave es una clave premium, tienes la opción de transferir el saldo restante pagado a otra clave antes de la revocación. Ten en cuenta que esta acción no se puede deshacer. Para revocar una clave, ve a la configuración de claves en el <a class='text-primary' href='https://jina.ai/api-dashboard'>panel de control de administración de claves API</a>.",question1:"¿Puedo usar la misma clave API para las API de lectura, inserción, reclasificación, clasificación y ajuste?",question10:"¿Por qué la primera solicitud de algunos modelos es lenta?",question12:"¿Se utilizan los datos de entrada del usuario para entrenar sus modelos?",question3:"¿Puedo monitorear el uso del token de mi clave API?",question4:"¿Qué debo hacer si olvido mi clave API?",question5:"¿Caducan las claves API?",question6:"¿Puedo transferir tokens entre claves API?",question7:"¿Puedo revocar mi clave API?",title:"Preguntas comunes relacionadas con API"},i={base_model:"Modelo base para ajuste fino",check_data:"Descargar datos sintéticos",check_model:"Descargar modelo ajustado",data_size:"Datos sintéticos generados",description:"Obtenga incorporaciones optimizadas para cualquier dominio que desee.",description_long:"Simplemente díganos en qué dominio desea que sus incrustaciones destaquen y le entregaremos automáticamente un modelo de incrustación optimizado y listo para usar para ese dominio.",does_it_work_tho:"¿Pero funciona?",does_it_work_tho_explain:"El ajuste automático tiene la promesa automática de ofrecer incrustaciones ajustadas para cualquier dominio que desee. pero de verdad funciona? Esta es una duda bastante razonable. Lo hemos probado en una variedad de dominios y modelos base para averiguarlo. Echa un vistazo a los resultados seleccionados con cereza y limón a continuación.",domain_instruction:"Instrucción de dominio",embedding_provider:"Seleccione un modelo de incrustación base",eval_evaluation:"Validación",eval_map:"MAPA",eval_mrr:"MRR",eval_ndcg:"NDCG",eval_performance_before_after:"Rendimiento en el conjunto de validación sintética antes y después del ajuste",eval_syntheticDataSize:"Total",eval_test:"Datos reales para realizar pruebas.",eval_training:"Capacitación",faq_v1:{answer1:"La función se encuentra actualmente en versión beta y cuesta 1 millón de tokens por modelo ajustado. Puede usar su clave API existente de la API Embedding/Reranker si tiene suficientes tokens, o puede crear una nueva clave API, que incluye 1 millón de tokens gratuitos.",answer10:"Actualmente no. Tenga en cuenta que esta función aún está en versión beta. Almacenar los modelos ajustados y los datos sintéticos públicamente en el centro de modelos de Hugging Face nos ayuda a nosotros y a la comunidad a evaluar la calidad de la capacitación. En el futuro, planeamos ofrecer una opción de almacenamiento privado.",answer11:"Dado que todos los modelos ajustados se cargan en Hugging Face, puedes acceder a ellos a través de SentenceTransformers simplemente especificando el nombre del modelo.",answer12:"Por favor revisa tu carpeta de spam. Si aún no puede encontrarlo, comuníquese con nuestro equipo de soporte utilizando la dirección de correo electrónico que proporcionó.",answer2:"No es necesario proporcionar ningún dato de entrenamiento. Simplemente describa su dominio de destino (el dominio para el cual desea que se optimicen las incrustaciones ajustadas) en lenguaje natural, o use una URL como referencia, y nuestro sistema generará datos sintéticos para entrenar el modelo.",answer3:"Unos 30 minutos.",answer4:"Los modelos ajustados y los datos sintéticos se almacenan públicamente en el centro de modelos de Hugging Face.",answer5:"El sistema utiliza la API Reader para recuperar el contenido de la URL. Luego analiza el contenido para resumir el tono y el dominio, que utiliza como pautas para generar datos sintéticos. Por lo tanto, la URL debe ser de acceso público y representativa del dominio de destino.",answer6:"Sí, puede ajustar un modelo para un idioma distinto del inglés. El sistema detecta automáticamente el idioma de las instrucciones de su dominio y genera datos sintéticos en consecuencia. También recomendamos elegir el modelo base adecuado para el idioma de destino. Por ejemplo, si se dirige a un dominio alemán, debe seleccionar 'jina-embeddings-v2-base-de' como modelo base.",answer7:"No, nuestra API de ajuste solo admite modelos Jina v2.",answer8:"Al final del proceso de ajuste, el sistema evalúa el modelo utilizando un conjunto de pruebas disponible e informa las métricas de rendimiento. Recibirá un correo electrónico detallando el rendimiento antes y después de este equipo de prueba. También le recomendamos que evalúe el modelo en su propio equipo de prueba para garantizar su calidad.",answer9:"El sistema genera datos sintéticos integrando la instrucción del dominio objetivo que usted proporciona con el razonamiento de los agentes de LLM. Produce tripletes negativos duros, que son esenciales para entrenar modelos de incrustación de alta calidad. Para obtener más detalles, consulte nuestro próximo artículo de investigación sobre Arxiv.",question1:"¿Cuánto cuesta la API de ajuste fino?",question10:"¿Puedo mantener la privacidad de mis modelos ajustados y mis datos sintéticos?",question11:"¿Cómo puedo utilizar el modelo ajustado?",question12:"Nunca recibí el correo electrónico con los resultados de la evaluación. ¿Qué tengo que hacer?",question2:"¿Qué necesito ingresar? ¿Necesito proporcionar datos de entrenamiento?",question3:"¿Cuánto tiempo lleva perfeccionar un modelo?",question4:"¿Dónde se almacenan los modelos ajustados?",question5:"Si proporciono una URL de referencia, ¿cómo la usa el sistema?",question6:"¿Puedo ajustar un modelo para un idioma específico?",question7:"¿Puedo ajustar incrustaciones que no sean de Jina, por ejemplo, bge-M3?",question8:"¿Cómo se garantiza la calidad de los modelos ajustados?",question9:"¿Cómo se generan datos sintéticos?",title:"Preguntas comunes relacionadas con el ajuste automático"},find_on_hf:"Lista de modelos ajustados",temporarily_unavailable:"Temporalmente no disponible. Estamos actualizando nuestro sistema de ajuste automático para brindarle un mejor servicio. Por favor, vuelva más tarde.",test_on:"Probado en {_dataSize} muestras aleatorias de {_dataName}",test_performance_before_after:"Rendimiento en el conjunto de pruebas retenido antes y después del ajuste fino",title:"API de ajuste automático",total_improve:"Promedio mejora",usage:"Uso",what_is:"¿Qué es el ajuste fino automático?",what_is_answer_long:"El ajuste fino le permite tomar un modelo previamente entrenado y adaptarlo a una tarea o dominio específico entrenándolo en un nuevo conjunto de datos. En la práctica, encontrar datos de entrenamiento efectivos no es sencillo para muchos usuarios. La formación eficaz requiere algo más que simplemente incluir archivos PDF y HTML sin formato en el modelo; y es difícil hacerlo bien. El ajuste automático resuelve este problema al generar automáticamente datos de capacitación efectivos utilizando una canalización avanzada de agentes LLM; y ajustar el modelo dentro de un flujo de trabajo de ML. Puede pensarlo como una combinación de generación de datos sintéticos y AutoML, por lo que todo lo que necesita hacer es describir su dominio de destino en lenguaje natural y dejar que nuestro sistema haga el resto."},s={auth_required:"Se requiere autenticación para utilizar la generación de avatar",classificationError:"Error al clasificar la imagen. Inténtalo de nuevo.",clickToDownload:"Haga clic para descargar SVG",customize:"Personalizar funciones",description:"Genera avatares únicos con funciones personalizables",downloadError:"Error al descargar el avatar",downloadSuccess:"Avatar descargado exitosamente",download_success:"Avatar descargado exitosamente",error_loading:"No se pudieron cargar los recursos del avatar. Inténtalo de nuevo.",error_processing:"Error al procesar la imagen",file_hint:"Formatos admitidos: JPG, PNG, GIF, WebP",generate:"Generar avatar",how_does_it_work:"¿Cómo funciona?",noImageSelected:"Por favor seleccione una imagen primero",select_file:"Seleccione un archivo de imagen de retrato",title:"Generador de avatares",upload_description:"Seleccione una imagen para convertir a base64 (256x256)",upload_title:"Subir imagen",usage:"Generación de avatares"},r={description:"¡Blog a banner, sin las indicaciones!",example_description:'Alicia empezaba a cansarse mucho de estar sentada junto a su hermana en la orilla y de no tener nada que hacer: una o dos veces había echado un vistazo al libro que su hermana estaba leyendo, pero no tenía dibujos ni conversaciones, "y de qué sirve un libro", pensó Alicia, "sin dibujos ni conversaciones". Así que estaba considerando en su propia mente (lo mejor que podía, porque el día caluroso la hacía sentir muy soñolienta y estúpida), si el placer de hacer una cadena de margaritas valdría la pena de levantarse y recoger las margaritas, cuando de repente un Conejo Blanco con ojos rosados ​​corrió cerca de ella.',example_title:"Las aventuras de Alicia en el país de las maravillas - Capítulo 1"},t="Beta",d={answer10:'Ofrecemos una prueba gratuita de bienvenida a nuevos usuarios, que incluye un millón de tokens para usar con cualquiera de nuestros modelos, facilitada por una clave API generada automáticamente. Una vez que se alcanza el límite de tokens gratuitos, los usuarios pueden comprar fácilmente tokens adicionales para sus claves API a través de la pestaña "Comprar tokens".',answer13:"No, los tokens no se deducen por solicitudes fallidas.",answer14:"Los pagos se procesan a través de Stripe y admiten una variedad de métodos de pago que incluyen tarjetas de crédito, Google Pay y PayPal para su comodidad.",answer15:"Sí, se emitirá una factura a la dirección de correo electrónico asociada a su cuenta de Stripe tras la compra de tokens.",answer9:"Nuestro modelo de precios se basa en la cantidad total de tokens procesados, lo que permite a los usuarios la flexibilidad de asignar estos tokens en cualquier cantidad de oraciones, ofreciendo una solución rentable para diversos requisitos de análisis de texto.",question10:"¿Hay una prueba gratuita disponible para nuevos usuarios?",question13:"¿Se cobran tokens por solicitudes fallidas?",question14:"¿Qué métodos de pago se aceptan?",question15:"¿Está disponible la facturación para compras de tokens?",question9:"¿La facturación se basa en el número de sentencias o solicitudes?",title:"Preguntas comunes relacionadas con la facturación"},c={all:"Todo",events:"Evento",featured:"Presentado",insights:"Opinión","knowledge-base":"Base de conocimientos",latest:"El último",press:"presione soltar",releases:"Actualización de software","tech-blog":"Blog de tecnología"},l={caption:'Descubra "Re·Search", nuestro anuario bellamente diseñado que muestra nuestros mejores artículos de investigación y modelos de base de búsqueda en 2024.',order_now:"Ordene ahora"},u={api_free_trial:"Clave API gratuita",api_paid:"Clave API paga",api_paid_or_free:"¿Estás utilizando una clave API paga o una clave de prueba gratuita?",are_you:"Eres:",commercial_contact_sales:"Esto es comercial. Contacte con nuestro equipo comercial.",contact_sales_for_licensing:"Contacte con nuestro equipo de ventas para obtener licencias.",csp_user:"¿Está utilizando nuestras imágenes de modelos oficiales en AWS y Azure?",educational_teaching:"¿Una institución educativa lo utiliza para enseñar?",for_profit_internal_use:"¿Una empresa con fines de lucro que lo utiliza internamente?",free_use:"Puedes utilizar los modelos libremente.",government_public_services:"¿Una entidad gubernamental que lo utiliza para servicios públicos?",is_use_commercial:"¿Su uso es comercial?",may_be_commercial_contact:"Esto puede ser comercial. Póngase en contacto con nosotros para obtener más información.",no:"No",no1:"No",no2:"No",no3:"No",no_restrictions:"Sin restricciones. Utilícelo según su contrato actual.",no_restrictions_apply:"No se aplican restricciones.",non_commercial_free_use:"Esto no es para uso comercial. Puedes usar los modelos libremente.",non_profit_ngo_mission:"¿Una organización sin fines de lucro u ONG lo utiliza para su misión?",not_sure:"No estoy seguro",personal_hobby_projects:"¿Lo estás usando para proyectos personales o de hobby?",product_service_sale:"¿Lo estás utilizando en un producto o servicio que vendes?",title:"Autocomprobación de licencia CC BY-NC",trial_key_restrictions:"La clave de prueba gratuita solo se puede utilizar para fines no comerciales. Adquiera un paquete pago para uso comercial.",typically_non_commercial_check:"Por lo general, esto no es comercial, pero consulte con nosotros si no está seguro.",typically_non_commercial_free_use:"Por lo general, no se trata de un proyecto comercial. Puedes utilizar los modelos libremente.",using_api_or_cloud:"¿Está utilizando nuestra API oficial o imágenes oficiales en Azure o AWS?",using_cc_by_nc_models:"¿Estas utilizando estos modelos?",yes:"Sí",yes1:"Sí",yes2:"Sí",yes3:"Sí"},m={access:"Acceso público",access_explain:"Cualquier persona con el <code>classifier_id</code> puede usar clasificadores públicos, y su uso consumirá la cuota de tokens del autor de la llamada en lugar de la tuya. Solo tú puedes acceder a los clasificadores privados.",access_private:"Privado",access_public:"Público",api_delete:"Eliminar clasificador",api_delete_explain:"Eliminar un clasificador por su ID.",api_list:"Clasificadores de listas",api_list_explain:"Enumere todos los clasificadores que ha creado.",classifier_id:"Identificación del clasificador",classify_inputs:"Entradas para clasificar",classify_inputs_explain:"En el caso del texto, puede ser una oración de hasta 8192 tokens. En el caso de las imágenes, puede ser una URL o una imagen codificada en base64.",classify_labels:"Etiquetas de candidatos",classify_labels_explain:"Las entradas se clasificarán en estas etiquetas. Pueden ser hasta 256 clases. Utilice etiquetas semánticas para un mejor rendimiento.",compare_table:{access_control:"Control de acceso",classifier_id_required:"Se requiere identificación del clasificador",continuous_updates:"Actualizaciones continuas del modelo",default_solution:"Solución predeterminada para la clasificación general",feature:"Característica",few_shot:"Pocos disparos",image_multi_lingual_support:"Soporte multimodal y multilingüe",labels_required_classify:"Etiquetas requeridas en /classify",labels_required_train:"Etiquetas requeridas en /train",max_classes:"Máximo de clases",max_classifiers:"Clasificadores Máximos",max_inputs_request:"Entradas máximas por solicitud",max_token_length:"Longitud máxima de token por entrada",na:"N / A",no:"No",out_of_domain_solution:"Para datos fuera del dominio de v3/clip-v1 o datos sensibles al tiempo",primary_use_case:"Caso de uso principal",semantic_labels_required:"Etiquetas semánticas requeridas",state_management:"Gestión del Estado",stateful:"Con estado",stateless:"Apátrida",token_count:"{count} fichas",training_data_required:"Datos de entrenamiento necesarios",yes:"Sí",zero_shot:"Disparo cero"},create_classifier:"Nuevo clasificador de pocos disparos",create_classifier_explain:"Cree un nuevo clasificador de pocos disparos y entrénelo con ejemplos etiquetados.",description:"Clasificación de cero disparos y pocos disparos para imágenes y texto.",description_long:"Pruebe nuestro API Games para ver cómo funciona nuestro clasificador.",description_long1:"Clasificador de alto rendimiento de disparo cero y de pocos disparos para datos multimodales y multilingües.",explain:"El clasificador es un servicio API que categoriza texto e imágenes utilizando modelos de incrustación (jina-embeddings-v3 y jina-clip-v1), que admiten tanto la clasificación de disparo cero sin datos de entrenamiento como el aprendizaje de pocos disparos con ejemplos mínimos.",faq_v1:{answer1:"Zero-shot requiere etiquetas semánticas durante la clasificación y none durante el entrenamiento, mientras que few-shot requiere etiquetas durante el entrenamiento pero no en la clasificación. Esto significa que zero-shot es mejor para necesidades de clasificación flexibles e inmediatas, mientras que few-shot es mejor para categorías fijas y específicas del dominio que pueden evolucionar con el tiempo.",answer10:"Sí, puedes elegir entre <code>jina-embeddings-v3</code> para la clasificación de texto (especialmente útil para multilingües) y <code>jina-clip-v1</code> para la clasificación multimodal. Nuevos modelos como <code>jina-clip-v2</code> estarán disponibles automáticamente a través de la API cuando se publiquen.",answer2:"<code>num_iters</code> controla la intensidad del entrenamiento: los valores más altos refuerzan los ejemplos importantes, mientras que los valores más bajos minimizan el impacto de los datos menos confiables. Se puede utilizar para implementar el aprendizaje consciente del tiempo al otorgar a los ejemplos recientes un mayor número de iteraciones, lo que lo hace valioso para desarrollar patrones de datos.",answer3:"Cualquiera que tenga el <code>classifier_id</code> puede usar clasificadores públicos, consumiendo su propia cuota de tokens. Los usuarios no pueden acceder a los datos de entrenamiento ni a la configuración, y no pueden ver las solicitudes de clasificación de otros, lo que permite compartir clasificadores de forma segura.",answer4:"La clasificación de pocos disparos requiere de 200 a 400 ejemplos de entrenamiento para superar la clasificación de cero disparos. Si bien en última instancia logra una mayor precisión, necesita este período de calentamiento para volverse efectiva. La clasificación de cero disparos proporciona un rendimiento constante de inmediato sin datos de entrenamiento.",answer5:"Sí, la API admite consultas multilingües usando <code>jina-embeddings-v3</code> y clasificación multimodal (texto/imagen) usando <code>jina-clip-v1</code>, con soporte para URL o imágenes codificadas en base64 en la misma solicitud.",answer6:"Zero-shot admite 256 clases sin límite de clasificadores, mientras que few-shot está limitado a 16 clases y 16 clasificadores. Ambos admiten 1024 entradas por solicitud y 8192 tokens por entrada.",answer7:"El modo de pocos disparos permite la actualización continua a través del punto final <code>/train</code> para adaptarse a los patrones de datos cambiantes. Puede agregar nuevos ejemplos o clases de manera incremental cuando cambia la distribución de datos, sin tener que reconstruir todo el clasificador.",answer8:"La API utiliza aprendizaje en línea de una sola pasada: los ejemplos de entrenamiento actualizan los pesos del clasificador, pero no se almacenan posteriormente. Esto significa que no se pueden recuperar datos de entrenamiento históricos, pero garantiza la privacidad y la eficiencia de los recursos.",answer9:"Comience con el método zero-shot para obtener resultados inmediatos y cuando necesite una clasificación flexible con etiquetas semánticas. Cambie al método few-shot cuando tenga entre 200 y 400 ejemplos, necesite mayor precisión o necesite manejar datos específicos del dominio o sensibles al tiempo.",question1:"¿Qué diferencias hay entre las etiquetas en zero-shot y few-shot?",question10:"¿Puedo utilizar diferentes modelos para diferentes idiomas/tareas?",question2:"¿Para qué sirve num_iters y cómo debo usarlo?",question3:"¿Cómo funciona el intercambio de clasificadores públicos?",question4:"¿Cuántos datos necesito para que la función de pocos disparos funcione bien?",question5:"¿Puede manejar múltiples idiomas y tanto texto como imágenes?",question6:"¿Cuáles son los límites estrictos que debo conocer?",question7:"¿Cómo manejo los cambios de datos a lo largo del tiempo?",question8:"¿Qué pasa con mis datos de entrenamiento después de enviarlos?",question9:"Cero disparos versus pocos disparos: ¿cuándo utilizar cuál?",title:"Preguntas frecuentes relacionadas con los clasificadores"},more:"más",num_iters:"Iteraciones de entrenamiento",num_iters_explain:"Controla la intensidad del entrenamiento: los valores más altos mejoran la precisión en los ejemplos actuales, pero aumentan el costo de tokens. El valor predeterminado de 10 suele funcionar bien.",read_notes:"Leer las notas de la versión",select_classifier_or_model:"Seleccione un clasificador o un modelo de incrustación",task_classify:"Clasificar",task_classify_explain:"Utilice un clasificador de cero disparos o de pocos disparos para categorizar texto o imágenes en clases definidas.",task_manage:"Administrar",task_manage_explain:"Enumere o elimine sus clasificadores de pocos disparos.",task_select:"Seleccione una tarea",task_train:"Tren",task_train_explain:"Cree o actualice un clasificador de pocas tomas con ejemplos etiquetados.",title:"API de clasificador",train_inputs:"Datos de entrenamiento",train_inputs_explain:"Ejemplos de texto o imágenes con etiquetas para el entrenamiento. Puede actualizar el clasificador de forma incremental con nuevos ejemplos y etiquetas a lo largo del tiempo.",train_label:"Etiqueta",what_is:"¿Qué es el clasificador?",when_to_use_what:"¿Cuándo utilizar disparos cero o pocos disparos?",when_to_use_what_explain:"Utilice la clasificación de disparo cero como su solución predeterminada para obtener resultados inmediatos en tareas de clasificación general con hasta 256 clases, mientras que el aprendizaje de pocos disparos es más adecuado cuando se trabaja con datos específicos del dominio fuera del conocimiento de los modelos de incorporación o cuando necesita manejar datos sensibles al tiempo que requieren actualizaciones continuas del modelo."},p={description:"Incruste imágenes y oraciones en vectores de longitud fija con CLIP"},g={description:"Plataforma de alojamiento en la nube para aplicaciones de IA multimodal"},v={agreement:"Al enviar, confirma que está de acuerdo con el procesamiento de sus datos personales por parte de Jina AI como se describe en el",anything_else:"Cuéntanos más sobre tu idea",cc_by_nc:"Solicitar uso comercial de modelos CC BY-NC",cc_by_nc_description:"Nuestros últimos modelos suelen tener licencia CC BY-NC. Para uso comercial, acceda a ellos a través de nuestra API, Azure Marketplace o AWS SageMaker. Marque esta casilla para uso local fuera de estos canales.",company:"Organización",company_size:"Tamaño de la organización",company_website:"Sitio web de la organización",company_website_placeholder:"URL de la página de inicio de su empresa o del perfil de LinkedIn",country:"País",department:"Departamento",description:"Haga crecer su negocio con Jina AI.",drop_area_for_image:"Deja tus imágenes aquí",faq:"Preguntas más frecuentes",feedback_sent:"¡Enviado! Nos pondremos en contacto contigo en breve.",field_required:"Se requiere campo",get_api_key:"¿Cómo obtener mi clave API?",image_upload:"Adjuntar imágenes",image_validate:"Puedes adjuntar hasta {_num} imágenes. Solo JPG, JPEG, PNG, WEBP.",impact_snapshots:"Instantáneas de impacto",invalid_date_format:"Formato de fecha no válido. Utilice el formato DD-MM-AAAA.",invalid_email:"el correo electrónico es invalido",invalid_number:"Número invalido. Por favor ingrese de nuevo",invalid_url:"La URL no es válida.",name:"Nombre",nc_check:"¿Necesito una licencia comercial?",other_questions:"Otras preguntas",preferred_models:"¿En qué modelos estás interesado?",preferred_products:"¿En qué productos estás interesado?",pricing:"¿Precios?",priority:"Soporte prioritario para usuarios pagos",private_statement:"Declaracion de privacidad",rate_limit:"¿Cuál es el límite de velocidad?",role:"Puesto de trabajo",self_check:"Autocomprobación",sending_feedback:"Envío...",shortcut:"Atajo",submit:"Entregar",submit_failed:"Envío fallido. Por favor, inténtelo de nuevo más tarde.",submit_success:"Gracias por tu envío. Nos pondremos en contacto con usted en breve.",subtitle:"Jina AI, líder en IA multimodal, se destaca en el ajuste de modelos, el servicio de modelos, el ajuste de avisos y el servicio de avisos. Al aprovechar las tecnologías nativas de la nube como Kubernetes y las arquitecturas sin servidor, ofrecemos soluciones sólidas, escalables y listas para producción. Con experiencia en modelos de lenguaje grande, texto, imagen, video, comprensión de audio, búsqueda neuronal y arte generativo, brindamos estrategias innovadoras y preparadas para el futuro para elevar su negocio.",subtitle1:"Jina AI, líder en IA multimodal, se destaca en el ajuste de integración, el servicio de integración, el ajuste rápido y el servicio rápido. Aprovechando tecnologías nativas de la nube como Kubernetes y arquitecturas sin servidor, ofrecemos soluciones sólidas, escalables y listas para producción. Con experiencia en modelos de lenguaje grandes, texto, imágenes, video, comprensión de audio, búsqueda neuronal e inteligencia artificial generativa, brindamos estrategias innovadoras y preparadas para el futuro para elevar su negocio.",subtitle2:"Explore Jina AI, la vanguardia de la IA multimodal. Nos destacamos en la integración y rapidez de tecnologías, utilizando soluciones nativas de la nube como Kubernetes para sistemas robustos y escalables. Especializados en grandes modelos de lenguaje y procesamiento de medios, ofrecemos estrategias comerciales innovadoras y preparadas para el futuro con nuestra experiencia avanzada en IA.",title:"Contactar con ventas",trusted_by:"Confiado por",turn_on_volume:"Sube el volumen",work_email:"Correo electrónico del trabajo"},b="Copiar",_="Copiado al portapapeles",f={description:"Un flujo de trabajo human-in-the-Loop para crear imágenes HD a partir de texto"},q={api_endpoint:"Punto final de API",api_key:"Clave API",api_tagline:"Totalmente compatible con el esquema de API de chat de OpenAI, simplemente intercambie <code>chat.openai.com</code> con <code>deepsearch.jina.ai</code> para comenzar.",api_title:"API de búsqueda profunda",assistant_message:"Asistente",chat_ui:{clear_context_message:"¿Estás seguro de que deseas borrar el contexto? Esto restablecerá la conversación.",clear_context_title:"¿Nuevo chat?",description:"Comprobación de vibraciones con una interfaz de chat sencilla. DeepSearch es ideal para preguntas complejas que requieren razonamiento iterativo, conocimiento del mundo o información actualizada.",example_q1:"¿Cuál es la última publicación del blog de OpenAI?",example_q2:"¿Cuál es la idea detrás del proyecto Node Deep Research?",example_q3:"¿Qué es exactamente lo que mejora jina-colbert-v2 respecto a jina-colbert-v1?",input_cant_be_empty:"La entrada no puede estar vacía",input_placeholder:"Escribe tu pregunta aquí",keyman:"Administrador de claves",new_chat:"Borrar el chat actual y comenzar una nueva conversación",payment_required:"No tienes suficientes tokens en tu clave API. Si tienes varias claves API, puedes cambiar a la que tenga suficientes tokens en el administrador de claves. De lo contrario, puedes recargar tu clave API para continuar.",purchase:"Recargar clave API",thinking:"Pensamiento...",title:"Chat de búsqueda profunda"},client_3p:"Integración de clientes",client_3p_explain:"DeepSearch es totalmente compatible con el esquema de API de chat de OpenAI. Es fácil utilizar DeepSearch con cualquier cliente de chat compatible con OpenAI.",comparison:{group1:{bestFor:"Respuestas rápidas a preguntas de conocimientos generales",feature1:"Las respuestas se generan puramente a partir de conocimientos previamente entrenados con una fecha límite fija.",limitations:"No se puede acceder a información en tiempo real o posterior al entrenamiento",timeCost:"alrededor de 1s",title:"LLM estándar",tokenCost:"alrededor de 1000 tokens"},group2:{bestFor:"Preguntas que requieren información actual o específica del dominio",feature1:"Respuestas generadas al resumir los resultados de una búsqueda de una sola pasada",feature2:"Puede acceder a información actual más allá del límite de capacitación",limitations:"Tiene dificultades con preguntas complejas que requieren razonamiento de múltiples saltos.",timeCost:"alrededor de 3s",title:"RAG y LLM fundamentados",tokenCost:"alrededor de 10.000 tokens"},group3:{bestFor:"Preguntas complejas que requieren investigación y razonamiento exhaustivos",feature1:"Agente autónomo que busca, lee y razona iterativamente",feature2:"Decide dinámicamente los próximos pasos en función de los hallazgos actuales.",feature3:"Autoevalúa la calidad de la respuesta antes de devolver los resultados",feature4:"Puede realizar inmersiones profundas en temas a través de múltiples ciclos de búsqueda y razonamiento.",limitations:"Requiere más tiempo que los enfoques LLM o RAG simples",timeCost:"Alrededor de los 50",title:"Búsqueda profunda",tokenCost:"alrededor de 500.000 tokens"}},demo:"Chatea con DeepSearch",demo_description:"Comprobación de vibraciones con una interfaz de chat sencilla. DeepSearch es ideal para preguntas complejas que requieren razonamiento iterativo, conocimiento del mundo o información actualizada.",description:"Busca, lee y razona hasta encontrar la mejor respuesta.",explain:"DeepSearch combina la búsqueda web, la lectura y el razonamiento para realizar una investigación exhaustiva. Piense en él como un agente al que le asigna una tarea de investigación: busca exhaustivamente y trabaja a través de múltiples iteraciones antes de proporcionar una respuesta. Este proceso implica una investigación continua, razonamiento y abordaje del problema desde varios ángulos. Esto es fundamentalmente diferente de los LLM estándar que generan respuestas directamente a partir de datos previamente entrenados y de los sistemas RAG tradicionales que se basan en búsquedas únicas y superficiales.",faq:{answer1:"DeepSearch es una API LLM que realiza búsquedas, lecturas y razonamientos iterativos hasta que encuentra una respuesta precisa a una consulta o alcanza su límite de presupuesto de tokens.",answer10:"Los límites de velocidad varían según el nivel de clave API y van desde 10 RPM hasta 30 RPM. Es importante tener esto en cuenta para aplicaciones con grandes volúmenes de consultas.",answer11:"DeepSearch envuelve los pasos de pensamiento en etiquetas XML <think>...</think> y proporciona la respuesta final después, siguiendo el formato de transmisión de OpenAI pero con estos marcadores especiales para la cadena de pensamientos.",answer12:"Sí. Jina Reader se utiliza para búsquedas y lecturas web, lo que proporciona al sistema la capacidad de acceder y procesar contenido web de manera eficiente.",answer14:"Sí, el uso de tokens de DeepSearch en consultas complejas es indiscutiblemente alto: un promedio de 70 000 tokens en comparación con los 500 para las respuestas LLM básicas. Esto demuestra la profundidad de la investigación, pero también tiene implicaciones de costos.",answer18:"El sistema se controla principalmente por el presupuesto de tokens en lugar del conteo de pasos. Una vez que se excede el presupuesto de tokens, ingresa al modo Bestia para generar la respuesta final. Consulte <code>reasoning_effort</code> para obtener más detalles.",answer19:"Las referencias se consideran tan importantes que si una respuesta se considera definitiva pero carece de referencias, el sistema continúa buscando en lugar de aceptar la respuesta.",answer2:"A diferencia de OpenAI y Gemini, DeepSearch se centra específicamente en brindar respuestas precisas mediante iteraciones en lugar de generar artículos extensos. Está optimizado para obtener respuestas rápidas y precisas a partir de búsquedas en la web profunda en lugar de crear informes completos.",answer20:'Sí, pero con pasos de investigación extensos. El ejemplo de "quién será presidente en 2028" muestra que puede manejar preguntas especulativas a través de múltiples iteraciones de investigación, aunque no se garantiza la precisión de tales predicciones.',answer3:"Necesita una clave API de Jina. Ofrecemos 1 millón de tokens gratis para nuevas claves API.",answer5:"Genera una respuesta final basada en todo el conocimiento acumulado, en lugar de simplemente darse por vencido o devolver una respuesta incompleta.",answer6:"No. Si bien utiliza un proceso de búsqueda iterativo para mejorar la precisión, la evaluación muestra que logra una tasa de aprobación del 75 % en las preguntas de la prueba, significativamente mejor que la línea de base del 0 % (gemini-2.0-flash), pero no perfecta.",answer7:"Varía significativamente: las consultas pueden requerir entre 1 y 42 pasos, con un promedio de 4 pasos según los datos de evaluación. Eso equivale a 20 segundos. Las consultas simples pueden resolverse rápidamente, mientras que las preguntas de investigación complejas pueden implicar muchas iteraciones y hasta 120 segundos.",answer9:'Sí, la API oficial de DeepSearch en deepsearch.jina.ai/v1/chat/completions es totalmente compatible con el esquema de API de OpenAI, y utiliza "jina-deepsearch-v1" como nombre de modelo. Por lo tanto, es muy fácil cambiar de OpenAI a DeepSearch y usarla con clientes locales o cualquier cliente compatible con OpenAI. Recomendamos encarecidamente Chatwise para una experiencia perfecta.',question1:"¿Qué es DeepSearch?",question10:"¿Cuáles son los límites de velocidad para la API?",question11:"¿Cuál es el contenido dentro de la etiqueta <think>?",question12:"¿DeepSearch utiliza Jina Reader para búsquedas y lecturas web?",question14:"¿Por qué DeepSearch utiliza tantos tokens para mis consultas?",question18:"¿Hay alguna forma de controlar o limitar el número de pasos?",question19:"¿Qué tan confiables son las referencias en las respuestas?",question2:"¿En qué se diferencia DeepSearch de las capacidades de investigación profunda de OpenAI y Gemini?",question20:"¿Puede DeepSearch manejar preguntas sobre eventos futuros?",question3:"¿Qué clave API necesito para utilizar DeepSearch?",question5:"¿Qué sucede cuando DeepSearch alcanza su presupuesto de tokens? ¿Devuelve una respuesta incompleta?",question6:"¿DeepSearch garantiza respuestas precisas?",question7:"¿Cuánto tiempo tarda una consulta típica de DeepSearch?",question9:"¿Puede DeepSearch funcionar con cualquier cliente compatible con OpenAI como Chatwise, CherryStudio o ChatBox?",title:"Preguntas frecuentes relacionadas con DeepSearch"},high_explain:"Máximo razonamiento y búsqueda de consultas complejas (2M tokens/solicitud)",low_explain:"Razonamiento básico y búsqueda de consultas simples (máximo 500 000 tokens/solicitud)",medium_explain:"Razonamiento moderado y profundidad de búsqueda (1 millón de tokens/requisito)",message:"Mensaje",messages:"Mensajes",messages_explain:"Una lista de mensajes entre el usuario y el asistente que comprende la conversación hasta el momento.",model:"Modelo",model_explain:"ID del modelo a utilizar.",model_name:"Nombre del modelo",open:"Abierto",reasoning_effort:"Esfuerzo de razonamiento",reasoning_effort_explain:"Limita el esfuerzo de razonamiento para los modelos de razonamiento. Los valores admitidos actualmente son bajo, medio y alto. Reducir el esfuerzo de razonamiento puede generar respuestas más rápidas y menos tokens utilizados en el razonamiento de una respuesta.",stream:"Transmisión",stream_explain:"Si es verdadero, devuelve una secuencia de eventos que ocurren durante la ejecución como eventos enviados por el servidor y finaliza cuando la ejecución ingresa a un estado terminal con un mensaje de datos: [DONE].",tagline:"@:deepsearch.descripción",title:"Búsqueda profunda",user_message:"Usuario",what_is:"¿Qué es DeepSearch?"},y={description:"Cree atractivas obras de arte de Disco Diffusion en una línea de código"},h={description:"La estructura de datos para datos multimodales"},A="Descargar Certificación SOC 2 Tipo 1",j={"11B tokens":"11 mil millones","11B tokens_intuition1":"Similar a leer todos los artículos en inglés en Wikipedia.","11B tokens_targetUser":"Despliegue de producción","1B tokens":"1 mil millones","1B tokens_intuition1":'Casi lo mismo que leer las obras completas de Shakespeare y toda la serie "Harry Potter".',"1B tokens_targetUser":"Desarrollo de prototipos","1M tokens":"1 millón","1M tokens_intuition1":'Equivale a leer el texto completo de "El Hobbit" y "El gran Gatsby".',"1M tokens_targetUser":"experimento de juguete","1M_free":"1 millón de fichas gratis","1M_free_description":"Disfrute de su nueva clave API con tokens gratuitos, sin necesidad de tarjeta de crédito.","2_5B tokens":"2.500 millones de fichas","2_5B tokens_intuition1":`Comparable a transcribir cada palabra pronunciada en la trilogía de la película "El Señor de los Anillos" 1.000 veces.
`,"3p_integration":"Con <b>{_numPartners}</b> servicios de terceros","3p_integration_desc":"Integre nuestra base de búsqueda con sus servicios existentes. Nuestros socios han creado conectores para nuestra API, lo que facilita el uso de nuestros modelos en sus aplicaciones.","500M tokens":"500 millones de fichas","500M tokens_intuition1":'Similar a ver todos los episodios de "Los Simpson" desde la temporada 1 hasta la temporada 30.',"59B tokens":"59 mil millones de fichas","59B tokens_intuition1":"Igual a todos los tweets publicados en todo el mundo durante un período de dos días.","5_5B tokens":"5.500 millones de fichas","5_5B tokens_intuition1":"Equivale a leer el texto completo de la Encyclopaedia Britannica.",Free1M:"1 millón de fichas","ReaderLM-v2_description":"Un pequeño modelo de lenguaje para convertir HTML sin formato en Markdown o JSON",add_pair:"Nuevo",add_time_explain:"La fecha en que se agregó este modelo a la Fundación de búsqueda.",api_integration_short:"Nuestra API de incrustación está integrada de forma nativa con varias bases de datos, almacenes de vectores, marcos RAG y LLMOps de renombre.",api_integrations:"Integraciones API",api_key_update_message:"Al reemplazar su antigua clave API, la nueva clave aparecerá en la interfaz de usuario cada vez que visite jina.ai. Las futuras recargas se aplicarán a esta nueva clave. Su antigua clave sigue siendo válida, por lo que, si planea volver a usarla, guárdela de forma segura.",api_key_update_title:"Reemplazo de clave API",auto_recharge:"Recarga automática para saldo bajo de tokens",auto_recharge_confirm_message:"¿Está seguro de que desea desactivar la recarga automática? Esto detendrá las recargas automáticas cuando el saldo de su token sea bajo y puede interrumpir su servicio o aplicación.",auto_recharge_confirm_title:"Desactivar recarga automática",auto_recharge_description:"Recomendado para un servicio ininterrumpido en producción. Cuando el saldo de su token caiga por debajo del límite establecido, recargaremos automáticamente su método de pago guardado por el último paquete comprado, hasta que se alcance el límite.",auto_recharge_enable:"Habilitaste la recarga automática en tokens bajos",auto_recharge_enable_message:"Para habilitar la recarga automática, compre un paquete con la recarga automática configurada como verdadera.",auto_recharge_enable_message2:"Seleccione el paquete que desea comprar cuando se active la recarga automática.",auto_recharge_enable_title:"Habilitar recarga automática",auto_request:"Vista previa automática",auto_request_tooltip:'Obtenga una vista previa automática de la respuesta de la API al cambiar el modelo, utilizando cientos de tokens de su clave API. Desactive el envío manual de una solicitud haciendo clic en "Obtener respuesta".',autostart:"La inserción comenzará automáticamente después de un breve retraso.",base64_description:"Las incrustaciones se devuelven como una cadena codificada en base64. Más eficiente para la transmisión.",batch_job:"Trabajo por lotes",batch_upload_hint:"Usaremos la clave API y el modelo siguiente para procesar los documentos.","bge-base-en-v1_5_description":"Un modelo inglés robusto que equilibra rendimiento y eficiencia para un uso versátil.","bge-base-en_description":"Un modelo inglés equilibrado diseñado para un rendimiento sólido y fiable.","bge-base-zh-v1_5_description":"Un modelo chino completo que equilibra capacidad y eficiencia.","bge-base-zh_description":"Un modelo chino versátil que combina eficiencia y rendimiento robusto.","bge-large-en-v1_5_description":"Un potente modelo inglés que ofrece incrustaciones de primer nivel con una calidad excepcional.","bge-large-en_description":"Un modelo inglés de alto rendimiento diseñado para incrustaciones de primera calidad.","bge-large-zh-v1_5_description":"Un modelo chino de alta capacidad que ofrece incrustaciones superiores y detalladas.","bge-large-zh_description":"Un modelo chino de alto rendimiento optimizado para incrustaciones de primer nivel.","bge-m3_description":"Un modelo multilingüe versátil que ofrece amplias capacidades e incorporaciones de alta calidad.","bge-small-en-v1_5_description":"Un modelo inglés simplificado que ofrece incrustaciones eficientes y de alta calidad.","bge-small-en_description":"Un modelo inglés eficiente para incrustaciones optimizadas y precisas.","bge-small-zh-v1_5_description":"Un modelo chino compacto que proporciona incrustaciones ágiles y precisas.","bge-small-zh_description":"Un modelo chino ágil para incrustaciones eficientes y precisas.",binary_description:"Las incrustaciones están empaquetadas como int8. Mucho más eficiente para almacenamiento, búsqueda y transmisión.",bulk:"Incrustación por lotes",bulk_embedding_failed:"No se pudo crear el trabajo de incrustación por lotes",buy_more_quota:"Recarga esta clave API con más tokens",buy_poster:"Compre una copia impresa",cancel_button:"Cancelar",click_upload_btn_above:"Haga clic en el botón de carga de arriba para comenzar.",clip_v2_description:"jina-clip-v2 es un modelo estilo CLIP 0.9B que aporta tres avances importantes: soporte multilingüe para 89 idiomas, alta resolución de imagen a 512x512 y aprendizaje de representación Matryoshka para incrustaciones truncadas.",clip_v2_title:"clip-v2: incrustaciones multimodales multilingües",code:"código",colbert_dimensions_explain:"El tamaño de la dimensión de la incrustación por token.",compatible:"Modo compatible",compatible_explain:"Sigue el mismo formato de solicitud que nuestros modelos de incrustación de texto. Esto le permite cambiar entre modelos sin cambiar la solicitud. Tenga en cuenta que la entrada de imágenes no es compatible con este modo.",cosine_similarity:"Similitud del coseno",debugging:"Prueba",delete_pair:"Borrar",description:"@:landing_page.embedding_desc1",dimensions:"Dimensiones de salida",dimensions_error:"El tamaño de la dimensión debe estar entre 1 y 1024.",dimensions_explain:"Las dimensiones más pequeñas permiten un almacenamiento y recuperación eficiente, con un impacto mínimo gracias a la representación de Matrioska.",dimensions_warning:"Recomendamos mantener el tamaño de la dimensión por encima de {_minDimension} para mejorar el rendimiento.",document:"Documento",download:"Descargar",edit_text1_text:"Editar texto de la izquierda",edit_text2_text:"Editar texto correcto",embedding_done:"{_Count} oraciones insertadas correctamente.",embedding_none_description:"No utilice ningún modelo de incrustación.",example_inputs:"Entradas de ejemplo",faq:"@:contact_us_page.faq",faqs_v2:{answer0:"Para obtener información detallada sobre nuestros procesos de capacitación, fuentes de datos y evaluaciones, consulte nuestro informe técnico disponible en arXiv.",answer1:"Jina CLIP <code>jina-clip-v2</code> es un modelo de incrustación multimodal avanzado que admite tareas de recuperación de texto-texto, texto-imagen, imagen-imagen e imagen-texto. A diferencia del OpenAI CLIP original, que tiene dificultades con la búsqueda de texto-texto, Jina CLIP se destaca como recuperador de texto. <code>jina-clip-v2</code> ofrece una mejora del rendimiento del 3 % con respecto a <code>jina-clip-v1</code> tanto en tareas de recuperación de texto-imagen como de texto-texto, admite 89 idiomas para la recuperación de imágenes multilingües, procesa imágenes de mayor resolución (512x512) y reduce los requisitos de almacenamiento con representaciones de Matryoshka. Puede leer más sobre esto en nuestro informe técnico.",answer17:"Sí, <code>jina-clip-v2</code> y <code>jina-clip-v1</code> pueden incorporar imágenes y textos. ¡Pronto anunciaremos la incorporación de modelos en más modalidades!",answer18:"Si tiene consultas sobre cómo ajustar nuestros modelos con datos específicos, contáctenos para analizar sus requisitos. Estamos abiertos a explorar cómo nuestros modelos se pueden adaptar para satisfacer sus necesidades.",answer19:"Sí, nuestros servicios están disponibles en los mercados de AWS, Azure y GCP. Si tiene requisitos específicos, comuníquese con nosotros a sales AT jina.ai.",answer3:"A partir de su lanzamiento el 18 de septiembre de 2024, <code>jina-embeddings-v3</code> es el mejor modelo multilingüe y ocupa el segundo puesto en la clasificación de inglés de MTEB para modelos con menos de mil millones de parámetros. v3 admite un total de 89 idiomas, incluidos los 30 principales con mejor rendimiento: árabe, bengalí, chino, danés, holandés, inglés, finlandés, francés, georgiano, alemán, griego, hindi, indonesio, italiano, japonés, coreano, letón, noruego, polaco, portugués, rumano, ruso, eslovaco, español, sueco, tailandés, turco, ucraniano, urdu y vietnamita. Para obtener más detalles, consulte el informe técnico de <code>jina-embeddings-v3</code>.",answer4:'Nuestros modelos permiten una longitud de entrada de hasta 8192 tokens, lo que es significativamente mayor que la mayoría de los demás modelos. Un token puede variar desde un solo carácter, como "a", hasta una palabra completa, como "manzana". La cantidad total de caracteres que se pueden ingresar depende de la longitud y la complejidad de las palabras utilizadas. Esta capacidad de entrada extendida permite que nuestros modelos <code>jina-embeddings-v3</code> y <code>jina-clip</code> realicen un análisis de texto más completo y logren una mayor precisión en la comprensión del contexto, especialmente para datos textuales extensos.',answer5:"Una sola llamada API puede procesar hasta 2048 oraciones o textos, lo que facilita un análisis de texto extenso en una sola solicitud.",answer6:"Puede utilizar <code>url</code> o <code>bytes</code> en el campo <code>input</code> de la solicitud de API. Para <code>url</code>, proporcione la URL de la imagen que desea procesar. Para <code>bytes</code>, codifique la imagen en formato base64 e inclúyala en la solicitud. El modelo devolverá las incrustaciones de la imagen en la respuesta.",answer7:"En las evaluaciones de los puntos de referencia MTEB English, Multilingual y LongEmbed, <code>jina-embeddings-v3</code> supera a las últimas integraciones patentadas de OpenAI y Cohere en tareas de inglés, y supera a <code>multilingual-e5-large-instruct</code> en todas las tareas multilingües. Con una dimensión de salida predeterminada de 1024, los usuarios pueden reducir las dimensiones de la integración a 32 sin sacrificar el rendimiento, gracias a la integración de Matryoshka Representation Learning (MRL).",answer8:"La transición se simplifica, ya que <a class='text-primary' href='https://api.jina.ai/v1/embeddings'>nuestro punto final de API</a> coincide con los esquemas JSON de entrada y salida del modelo <code>text-embedding-3-large</code> de OpenAI. Esta compatibilidad garantiza que los usuarios puedan reemplazar fácilmente el modelo de OpenAI por el nuestro cuando utilicen el punto final de OpenAI.",answer9:`Los tokens se calculan en función de la longitud del texto y el tamaño de la imagen. Para el texto de la solicitud, los tokens se cuentan de la forma estándar. Para las imágenes, se llevan a cabo los siguientes pasos:

1. Tamaño de mosaico: cada imagen se divide en mosaicos. Para <code>jina-clip-v2</code>, los mosaicos son de 512 x 512 píxeles, mientras que para <code>jina-clip-v1</code>, los mosaicos son de 224 x 224 píxeles.
2. Cobertura: se calcula la cantidad de mosaicos necesarios para cubrir la imagen de entrada. Incluso si las dimensiones de la imagen no son perfectamente divisibles por el tamaño del mosaico, los mosaicos parciales se cuentan como mosaicos completos.
3. Mosaicos totales: la cantidad total de mosaicos que cubren la imagen determina el costo. Por ejemplo, una imagen de 600 x 600 píxeles estaría cubierta por mosaicos de 2 x 2 (4 mosaicos) en v2 y mosaicos de 3 x 3 (9 mosaicos) en v1.
4. Cálculo de costos: para <code>jina-clip-v2</code>, cada mosaico cuesta 4000 tokens, mientras que para <code>jina-clip-v1</code>, cada mosaico cuesta 1000 tokens.

Ejemplo:
Para una imagen con dimensiones de 600x600 píxeles:

• Con <code>jina-clip-v2</code>
• La imagen se divide en mosaicos de 512x512 píxeles.
• La cantidad total de mosaicos necesarios es 2 (horizontal) x 2 (vertical) = 4 mosaicos.
• El costo para <code>jina-clip-v2</code> será 4*4000 = 16000 tokens.

• Con <code>jina-clip-v1</code>
• La imagen se divide en mosaicos de 224x224 píxeles.
• La cantidad total de fichas requeridas es 3 (horizontales) x 3 (verticales) = 9 fichas.
• El costo de jina-clip-v1 será 9*1000 = 9000 tokens.`,question0:"¿Cómo se entrenaron los modelos jina-embeddings-v3?",question1:"¿Qué son los modelos jina-clip y puedo usarlos para buscar texto e imágenes?",question17:"¿Proporcionan modelos para incrustar imágenes o audio?",question18:"¿Se pueden ajustar los modelos de Jina Embedding con datos privados o de la empresa?",question19:"¿Se pueden alojar sus puntos finales de forma privada en AWS, Azure o GCP?",question3:"¿Qué idiomas admiten sus modelos?",question4:"¿Cuál es la longitud máxima para la entrada de una sola oración?",question5:"¿Cuál es el número máximo de frases que puedo incluir en una sola solicitud?",question6:"¿Cómo envío imágenes a las modelos de jina-clip?",question7:"¿Cómo se comparan los modelos de Jina Embeddings con las últimas incorporaciones de OpenAI y Cohere?",question8:"¿Qué tan fluida es la transición de text-embedding-3-large de OpenAI a su solución?",question9:"¿Cómo se calculan los tokens cuando se utilizan los modelos jina-clip?",title:"Preguntas comunes relacionadas con incrustaciones"},feature_8k1:"8192 longitud del token",feature_8k_description1:"Ser pionero en el primer modelo de integración de código abierto con una longitud de 8192 tokens, que permite la representación de un capítulo completo en un solo vector.",feature_cheap:"20 veces más barato",feature_cheap_v1:"5 veces más barato",feature_cheap_v1_description1:"Comience con pruebas gratuitas y disfrute de una estructura de precios sencilla. Obtenga acceso a potentes incorporaciones por solo el 20 % del coste de OpenAI.",feature_multilingual:"Ofreciendo modelos bilingües para alemán-inglés, chino-inglés, entre otros, ideales para aplicaciones multilingües.",feature_on_premises:"Privacidad primero",feature_on_premises_description1:"Implemente sin problemas nuestros modelos integrados directamente dentro de su nube privada virtual (VPC). Actualmente es compatible con AWS Sagemaker, con próximas integraciones para Microsoft Azure y Google Cloud Platform. Para implementaciones personalizadas de Kubernetes, comuníquese con nuestro equipo de ventas para obtener asistencia especializada.",feature_on_premises_description2:"Implemente modelos de Jina Embeddings en AWS Sagemaker y pronto en Microsoft Azure y Google Cloud Services, o comuníquese con nuestro equipo de ventas para obtener implementaciones personalizadas de Kubernetes para su nube privada virtual y servidores locales.",feature_on_premises_description3:"Implemente modelos de Jina Embeddings en AWS Sagemaker y Microsoft Azure, y pronto en Google Cloud Services, o comuníquese con nuestro equipo de ventas para obtener implementaciones personalizadas de Kubernetes para su nube privada virtual y servidores locales.",feature_on_premises_description4:"Implemente modelos Jina Embedding y Reranker en las instalaciones utilizando AWS SageMaker, Microsoft Azure o Google Cloud Services, garantizando que sus datos permanezcan seguros bajo su control.",feature_solid:"Mejor en clase",feature_solid_description1:"Desarrollado a partir de nuestra investigación académica de vanguardia y probado rigurosamente con los modelos SOTA para garantizar un rendimiento incomparable.",feature_top_perform1:"Integración perfecta",feature_top_perform_description1:"Totalmente compatible con la API de OpenAI. Se integra sin esfuerzo con más de 10 bases de datos vectoriales y sistemas RAG para una experiencia de usuario fluida.",file_required:"Se requiere archivo",file_size_exceed:"Superar el tamaño máximo de archivo {_size}",file_type_not_supported:"Tipo de archivo no compatible",fill_example:"Complete un ejemplo",float_description:"Las incrustaciones se devuelven como una lista de números de punto flotante. El más común y fácil de usar.",free:"Gratis",generate_api_key_error:"Error al generar la clave API.",generating_visualization:"Generando visualización...",get_new_key_button:"Obtener nueva clave",get_new_key_button_explain:"Optar por una nueva clave resultará en la pérdida del historial de uso asociado con la clave anterior.",get_new_key_survey:"Complete la encuesta, ayúdenos a comprender su uso y obtenga una nueva clave API gratis.",includes:"Fichas válidas para:",index_and_search:"Índice y búsqueda",index_and_search1:"Índice y búsqueda",input:"Pedido",input_api_key_error1:"¡Su clave API no es válida!",input_length:"Longitud de entrada",input_type:"Incrustar como documento/consulta",input_type_explain:"La misma entrada puede servir como consulta o como incrustación de documento, dependiendo de su función de búsqueda.",integrate:"Integrar","jina-clip-v1_description":"Modelos de incrustación multimodal para imágenes y texto en inglés","jina-clip-v2_description":"Incrustaciones multimodales multilingües para textos e imágenes","jina-colbert-v1-en_description":"ColBERT mejorado con una longitud de token de 8K para tareas de incrustación y reclasificación","jina-colbert-v2_description":"El mejor ColBERT multilingüe con el máximo rendimiento en integración y reclasificación","jina-embedding-b-en-v1_description":"La primera versión del modelo Jina Embedding, el OG.","jina-embeddings-v2-base-code_description":"Optimizado para búsqueda de código y cadenas de documentos","jina-embeddings-v2-base-de_description":"Integraciones bilingües alemán-inglés con rendimiento SOTA","jina-embeddings-v2-base-en_description":"A la par con text-embedding-ada002 de OpenAI","jina-embeddings-v2-base-es_description":"Incorporaciones bilingües español-inglés con rendimiento SOTA","jina-embeddings-v2-base-zh_description":"Integraciones bilingües chino-inglés con rendimiento SOTA","jina-embeddings-v2-small-en_description":"Optimizado para baja latencia y uso de memoria","jina-embeddings-v3_description":"Modelo de integración multilingüe de Frontier con rendimiento SOTA","jina-reranker-v1-base-en_description":"Nuestro primer modelo de reranker que maximiza la búsqueda y la relevancia de RAG","jina-reranker-v1-tiny-en_description":"El modelo de reordenación más rápido, más adecuado para clasificar una gran cantidad de documentos de manera confiable","jina-reranker-v1-turbo-en_description":"La mejor combinación de velocidad de inferencia rápida y puntuaciones de relevancia precisas","jina-reranker-v2-base-multilingual_description":"El último y mejor modelo de reranker con soporte multilingüe, llamadas de funciones y búsqueda de códigos.",key:"Clave API",key_enter_placeholder:"Por favor ingrese su clave API",key_enter_placeholder_to_topup:"Ingresa la clave API que deseas recargar",key_to_top_up:'¿Tiene una clave API diferente para recargar? Péguela arriba y haga clic en "Guardar".',key_warn:"Asegúrese de guardar su clave API en un lugar seguro. De lo contrario, deberá generar una nueva clave.",key_warn_v2:"Esta es tu clave única. ¡Guárdala de forma segura!",language_explain:"Este modelo es el que mejor admite el idioma {_language}.",last_7_days:"Uso",late_chunking:"Troceo tardío",late_chunking_explain:"Aplique la técnica de fragmentación tardía para aprovechar las capacidades de contexto largo del modelo para generar incrustaciones de fragmentos contextuales.",learn_more:"Aprende más",learn_poster:"Aprende cómo lo hicimos",learning1:"Aprendiendo sobre incrustaciones",learning1_description:"¿Por dónde empezar con las incrustaciones? Te tenemos cubierto. Aprenda sobre las incrustaciones desde cero con nuestra guía completa.",length:"Longitud del token",manage_billing:"Gestionar factura",manage_billing_tip:"Administre su información de facturación, obtenga facturas y configure la recarga automática.",manage_quota1:"Clave API y facturación",max_file_size:"Tamaño máximo permitido: {_maxSize}.",maximize_tooltip:"Maximizar este panel con Shift+1",mistake_contact:"Si cree que esto es un error, por favor contáctenos.",mminput_placeholder:"Texto, URL de la imagen, cadena base64 de la imagen",model_required:"Se requiere modelo",more_models:"{_numMore} más modelos",more_than_two2:"Introduzca más de dos documentos, es decir, más de dos líneas.",multi_embedding:"Multivector",multi_embedding_explain:"Este modelo devolverá una bolsa de incrustaciones contextualizadas para una entrada determinada. Cada token en la entrada se asigna a un vector en la salida.",multilingual:"Soporte multilingüe",multimodal:"Multimodal",multimodal_explain:"Este modelo puede codificar entradas de texto e imágenes, lo que lo hace ideal para tareas de búsqueda multimodal.",new:"Nuevo modelo",no_data1:"Agrega un par de oraciones para calcular la similitud.",none:"Ninguno",normalized:"Normalización L2",normalized_explain:"Escala la incrustación de modo que su norma euclidiana (L2) se convierta en 1, lo que preserva la dirección. Resulta útil cuando el proceso posterior implica un producto escalar, una clasificación y una visualización.",oncsp:"En CSP",onprem:"Local",open_tensorboard:"Visualizador abierto",opensource:"SO",opensource_explain:"Este modelo es de código abierto y está disponible en Hugging Face. Haga clic en este botón para ver el modelo en Hugging Face.",original_documents:"Oraciones para insertar",original_documents_hint:"Introduzca aquí sus frases. Cada nueva línea se considerará una oración/documento independiente.",output:"Respuesta",output_dim:"Dimensiones",output_dim_explain:"La dimensión de salida de un vector de incrustación de este modelo es {_outputDim}.",output_dimension:"Dimensiones de salida",pairwise_test:"Por parejas",per_k:"/ 1K fichas",per_m:"/ 1 millón de fichas",please_fill_docs_first:"Primero ingrese algunas oraciones a continuación antes de realizar la búsqueda.",please_select_model:"Seleccione un modelo de incrustación o un modelo de Reranker",poster:"La evolución de las incrustaciones Póster",poster_description:"Descubra el póster ideal para su espacio, con infografías cautivadoras o imágenes impresionantes que rastrean la evolución de los modelos de incrustación de texto desde 1950.",pricing:"Precios de API",pricing_desc:"El precio de la API se basa en el uso de tokens. Una clave API le otorga acceso a todos los productos de Search Foundation.",protectData1:"Los datos y documentos de la solicitud no se utilizan para los modelos de capacitación.",protectData2:"Cifrado de datos en tránsito (TLS 1.2+) y en reposo (AES-GCM 256).",protectData3:"Cumple con SOC 2 y GDPR.",protect_data:"Proteja sus datos",public_cloud_integration:"Con <b>{_numPartners}</b> proveedores de servicios en la nube",public_cloud_integration_desc:"¿Su empresa utiliza AWS o Azure? Luego, implemente directamente nuestros modelos básicos de búsqueda en estas plataformas de su empresa, para que sus datos se mantengan seguros y cumplan con las normas.",query:"Consulta",raise_issue:"Plantear un problema",rank_none_description:"No utilices ningún modelo de reranker.",read_api_docs:"Especificación API",read_release_note:"Leer la nota de lanzamiento","reader-lm-05b_description":"Un pequeño modelo de lenguaje para convertir HTML sin formato en Markdown","reader-lm-15b_description":"Un pequeño modelo de lenguaje para convertir HTML sin formato en Markdown",recharge_threshold:"Umbral de recarga",refresh:"Actualizar",refresh_key_tooltip1:"Obtenga una nueva clave API gratis",refresh_token_count1:"Actualice para obtener tokens disponibles de la clave API actual",regenerate:"Regenerado",remaining:"Fichas disponibles",remaining_left:"Te quedan <b>{_leftTokens}</b> tokens en la clave API a continuación.",request_number:"Horarios de solicitud",request_path:"Punto final de solicitud",results_as_final_result:"#docs como resultado final",results_fed_to_reranker:"#docs enviados al reranker",retry:"Rever",return_base64:"Base64 (como cadena)",return_binary:"Binario (empaquetado como int8)",return_float:"Predeterminado (como flotante)",return_format:"Formato de incrustaciones",return_format_explain:"Además del flotante, puede pedirle que regrese como binario para una recuperación de vectores más rápida o como codificación base64 para una transmisión más rápida.",return_format_title:"Tipo de datos de retorno",return_ubinary:"Binario (empaquetado como uint8)",right_api_key_to_charge:"Ingrese la clave API correcta para recargar",running:"Activo",score:"Puntaje",search:"Buscar",search_hint:"Escriba para buscar dentro de las oraciones que se enumeran a continuación",select_classify_model:"Seleccionar clasificador",select_embedding_model:"Seleccionar incrustaciones",select_rerank_model:"Seleccionar reclasificador",show_api_key:"Mostrar clave API",size:"Parámetros",size_explain:"La cantidad de parámetros en el modelo es {_size}; tenga en cuenta que este no es el tamaño del archivo del modelo.",sleeping:"Inactivo",start_batch:"Iniciar la incrustación por lotes",start_embedding:"Índice",status_explain:"Nuestra arquitectura sin servidor puede descargar ciertos modelos durante períodos de bajo uso. Para los modelos activos, las respuestas son inmediatas. Los modelos inactivos requieren unos segundos para cargarse tras la solicitud inicial. Después de la activación, las solicitudes posteriores se procesan más rápidamente.",task_type:"Tarea posterior",task_type_classification:"Clasificación",task_type_classification_explain:"Clasificación de texto.",task_type_explain:"Seleccione la tarea posterior para la que se utilizarán las incrustaciones. El modelo devolverá las incrustaciones optimizadas para esa tarea.",task_type_none_explain:"No se utilizará ningún adaptador. Se devolverá una incrustación genérica, útil para depuración o piratería.",task_type_retrieval_passage:"Pasaje de recuperación",task_type_retrieval_passage_explain:"Incrustar documentos en una tarea de recuperación de documentos de consulta.",task_type_retrieval_query:"Consulta de recuperación",task_type_retrieval_query_explain:"Incorporación de consultas en una tarea de recuperación de documentos de consulta.",task_type_separation:"Separación",task_type_separation_explain:"Agrupamiento de documentos, visualización de corpus.","task_type_text-matching":"Coincidencia de texto","task_type_text-matching_explain":"Similitud de texto semántico, recuperación simétrica general, recomendación, búsqueda de similares, deduplicación.",tax_may_apply:"Dependiendo de su ubicación, es posible que se le cobre en USD, EUR u otras monedas. Se pueden aplicar impuestos.",text1:"Izquierda",text2:"Bien",three_ways:"Tres formas de comprar",three_ways_desc:"Suscríbete a nuestra API, compra a través de proveedores de la nube u obtén una licencia comercial para tu organización.",title:"API de incrustación",token_example:'Un tweet equivale a aproximadamente 20 tokens, un artículo de noticias equivale a aproximadamente 1000 tokens y la novela de Charles Dickens "A Tale of Two Cities" tiene más de un millón de tokens.',token_length_explain:"La longitud máxima de la secuencia del token de entrada es {_tokenLength} para este modelo.",tokens:"Fichas",tools:"Herramientas",top_up_button:"Recargar clave antigua",top_up_button_explain:"La integración de esta clave API ofrece una solución más profesional, eliminando la necesidad de cambios frecuentes de clave. Los datos de uso se conservan y son accesibles en cualquier momento.",top_up_warning_message1:"A la clave API actual le quedan {_remainedTokens} tokens y será reemplazada por una nueva clave con tokens {_freeTokens}. Puede continuar usando o recargar la clave anterior si la ha almacenado de forma segura. ¿Como quieres proceder?",top_up_warning_title:"Reemplazar la antigua clave API",total_documents:"Progreso de la incrustación: {_Processed}/{_Count} oraciones.",tuning:"Afinar",turnstile_error:"No podemos generar una clave API porque no pudimos verificar si eres humano.",turnstile_unsupported:"No podemos generar una clave API porque su navegador no es compatible.",ubinary_description:"Las incrustaciones se empaquetan como uint8. Mucho más eficiente para almacenamiento, búsqueda y transmisión.",upload:"Subir",upload_file:"Haga clic aquí para cargar un archivo",usage:"Uso",usage_amount:"Fichas",usage_history:"Uso en los últimos 7 días",usage_history_explain:"Los datos no están en tiempo real y pueden sufrir un retraso de unos minutos.",usage_reason:"Descripción",usage_reason_consume:"Usado",usage_reason_purchase:"Comprado",usage_reason_transfer_in:"Transferencia en",usage_reason_transfer_out:"Transferencia de salida",usage_reason_trial:"Ensayo",usage_rerank:"Uso",usage_time:"Fecha y hora",v3_description:"<code>jina-embeddings-v3</code> es un modelo de incrustación de texto multilingüe de vanguardia con 570 millones de parámetros y una longitud de token de 8192, que supera a las últimas incrustaciones patentadas de OpenAI y Cohere en MTEB. Lea nuestra publicación de blog y nuestro artículo de investigación a continuación.",v3_title:"v3: Integraciones multilingües de Frontier",vector_database_integration1:"Integraciones",vector_database_integration2:"Nuestra API de incrustación está integrada de forma nativa con varias bases de datos, almacenes de vectores, marcos RAG y LLMOps de renombre. Para comenzar, simplemente copie y pegue su clave API en cualquiera de las integraciones enumeradas para un comienzo rápido y sin problemas.",vector_database_integration3:"Nuestra API Embedding & Reranker está integrada de forma nativa con varias bases de datos, almacenes de vectores, RAG y marcos LLMOps de renombre. Para comenzar, simplemente copie y pegue su clave API en cualquiera de las integraciones enumeradas para un comienzo rápido y sin problemas.",vector_database_integration_description:"Integre fácil y perfectamente la API de Jina Embeddings con cualquiera de las bases de datos vectoriales, marcos de orquestación LLM y aplicaciones RAG que aparecen a continuación. Nuestros tutoriales le mostrarán cómo.",view_details:"Ver detalles",visualization_example:"Mapear todas las oraciones de esta sección a un espacio vectorial 3D",visualization_example_you_can:"Utilice nuestra API a continuación, ¡usted también puede hacerlo!",visualize:"Visualizar",visualize_done:"La visualización ha terminado, ahora puede hacer clic en el botón superior para abrir el visualizador.",wait_for_processing:"Se está procesando su petición.",wait_stripe:"Abriendo pago de Stripe, por favor espere",what_are_embedding:"¿Qué son las incrustaciones?",what_are_embedding_answer:`Imagínese enseñarle a una computadora a captar los significados matizados de palabras y frases. Los métodos tradicionales, que se basaban en sistemas rígidos basados ​​en reglas, se quedaron cortos porque el lenguaje es demasiado complejo y fluido. Ingrese a las incrustaciones de texto: una poderosa solución que traduce texto a un lenguaje de números, específicamente, a vectores en un espacio de alta dimensión.

Considere las frases "clima soleado" y "cielo despejado". Para nosotros, nos pintan un cuadro similar. A través de la lente de las incrustaciones, estas frases se transforman en vectores numéricos que residen cerca unos de otros en este espacio multidimensional, capturando su parentesco semántico. Esta cercanía en el espacio vectorial no se trata solo de que las palabras o frases sean similares; se trata de comprender el contexto, el sentimiento e incluso los matices sutiles del significado.

¿Por qué es importante este avance? Para empezar, cierra la brecha entre la riqueza del lenguaje humano y la eficiencia computacional de los algoritmos. Los algoritmos destacan por procesar números, no por interpretar textos. Al convertir texto en vectores, las incrustaciones hacen posible que estos algoritmos "comprendan" y procesen el lenguaje de una manera que antes estaba fuera de su alcance.

Las aplicaciones prácticas son amplias y variadas. Ya sea recomendar contenido que resuene con sus intereses, impulsar una IA conversacional que parezca sorprendentemente humana o incluso detectar patrones sutiles en grandes volúmenes de texto, las incrustaciones son la clave. Permiten que las máquinas realicen tareas como análisis de sentimientos, traducción de idiomas y mucho más, con una comprensión del lenguaje cada vez más matizada y refinada.`,what_is_a_token:'Un token en el procesamiento de textos es una unidad, a menudo una palabra. Por ejemplo, "¡Jina AI es genial!" se convierte en cinco fichas, incluida la puntuación.',why_do_you_need:"Elegir las incrustaciones adecuadas",why_do_you_need_after:"Aprovechando las redes neuronales profundas y los LLM, nuestros modelos integrados representan datos multimodales en un formato optimizado, lo que mejora la comprensión de las máquinas, el almacenamiento eficiente y permite aplicaciones avanzadas de IA. Estas incorporaciones desempeñan un papel crucial en la comprensión de los datos, la mejora de la participación del usuario, la superación de las barreras del idioma y la optimización de los procesos de desarrollo.",why_do_you_need_before:"Nuestros modelos de integración están diseñados para cubrir diversas aplicaciones de búsqueda y GenAI.",why_need_1_description:"Nuestro modelo de integración central, impulsado por JinaBERT, está diseñado para un amplio espectro de aplicaciones. Destaca en la comprensión de textos detallados, lo que lo hace ideal para búsqueda semántica, clasificación de contenido y análisis de lenguaje complejo. Su versatilidad es incomparable y admite la creación de herramientas avanzadas de análisis de sentimientos, resúmenes de texto y sistemas de recomendación personalizados.",why_need_1_title:"Incrustaciones de uso general",why_need_2_description:"Nuestros modelos bilingües facilitan la comunicación entre idiomas, mejorando las plataformas multilingües, la atención al cliente global y el descubrimiento de contenido en varios idiomas. Diseñados para dominar las traducciones alemán-inglés y chino-inglés, estos modelos simplifican las interacciones y fomentan la comprensión entre diversos grupos lingüísticos.",why_need_2_title:"Incorporaciones bilingües",why_need_3_description:"Diseñado para desarrolladores, nuestro modelo de incorporación de código optimiza las tareas de codificación como el resumen, la generación de código y las revisiones automáticas. Aumenta la productividad al ofrecer información más profunda sobre las estructuras del código y sugerir mejoras, lo que lo hace esencial para desarrollar complementos IDE avanzados, documentación automática y herramientas de depuración de vanguardia.",why_need_3_title:"Incorporaciones de código",why_need_4_description:"Jina CLIP es nuestro último modelo de incrustación multimodal para imágenes y texto. Una gran mejora con respecto a OpenAI CLIP es que este modelo único se puede utilizar para la recuperación de texto-texto, así como para tareas de recuperación de texto-imagen, imagen-texto e imagen-imagen. ¡Así que un modelo, dos modalidades, cuatro direcciones de búsqueda!",why_need_4_title:"Incrustaciones multimodales",write_email_here:"Ingrese el correo electrónico donde desea recibir el enlace de descarga al finalizar.",you_can_leave:"Puede abandonar esta página y le enviaremos el enlace de descarga una vez finalizada."},z={description:"Integraciones multilingües y multimodales de clase mundial."},x={contractType:{department:"Licencia departamental",poc:"Prueba de concepto (3-6 meses)",standard:"Licencia empresarial estándar",title:"Tipo de contrato"},department:{businessSponsor:"Patrocinador de la unidad de negocios",executionModel:"Modelo de ejecución",growth:{high:"Claro potencial a nivel empresarial",highDesc:"Iniciativa estratégica con adopción planificada en toda la empresa",limited:"Limitado al Departamento",limitedDesc:"Concéntrese en las necesidades de un solo departamento con soporte estándar",steady:"Potencial para otros departamentos",steadyDesc:"Expansión planificada a 2-3 departamentos dentro de 12 meses"},growthTitle:"Trayectoria de crecimiento",sponsorDescription:"Patrocinador ejecutivo dedicado que defiende la implementación, proporciona dirección estratégica y garantiza que se asignen los recursos."},descriptions:{contractType:{department:"Implementación de un solo departamento con flexibilidad para expandirse más adelante",poc:"Implementación de prueba para probar modelos en su entorno específico y casos de uso",standard:"Implementación empresarial completa con uso de modelos sin restricciones"},department:{growth:"Planes para ampliar el uso del modelo en otros departamentos",sponsorship:"Indica si un departamento generador de ingresos respalda la implementación"},features:{csm:"Gestor personal de éxito del cliente para orientación y apoyo estratégico",priority:"Respuesta rápida garantizada para problemas críticos.",training:"Ayuda con el entrenamiento previo o el ajuste de modelos para sus datos específicos"},models:{clip:"Procesar imágenes y texto para aplicaciones multimodales",colbert:"Modelo especializado para recuperación de documentos de alta precisión",embeddings:"Modelo de incrustación de texto para búsqueda semántica y similitud de texto",reader:"Convierte contenido HTML a formato Markdown limpio",reranker:"Ajusta los resultados de búsqueda para lograr una mayor relevancia"},payment:{annual:"Pago único anual para contabilidad simplificada",quarterly:"Pagos regulares cada tres meses"},poc:{duration:"Cronograma para probar y validar el rendimiento del modelo en su entorno",metrics:"Seguimiento de indicadores clave de rendimiento y eficacia del modelo"},support:{enterprise:"Cobertura de soporte completa con máxima prioridad.",premium:"Horas de consultoría adicionales y tiempos de respuesta más rápidos",standard:"Soporte técnico básico y orientación para la implementación"},usage:{business:"¿Cuántas empresas distintas utilizarán aplicaciones basadas en nuestros modelos?",consumer:"¿Cuántos usuarios finales interactuarán con nuestros modelos mensualmente?"}},features:{csm:"Gerente de éxito del cliente dedicado",priority:"SLA de respuesta prioritaria (4 horas)",title:"Características adicionales",training:"Soporte de capacitación de modelos personalizados"},interests:"Estoy interesado en esta licencia comercial configurada (${_Price})",labels:{basePrice:"Precio base",custom:"Comuníquese con el departamento de ventas para obtener precios personalizados",discountApplied:"Descuento aplicado",included:"Incluido en el precio base",learnMore:"Más información",priceQuarterly:"Precio por trimestre",selectAll:"Seleccionar todos los modelos",selectSupport:"Seleccionar nivel de soporte",totalPrice:"Precio total",upTo:"Hasta {count}"},messaging:{additionalFeatures:"Características adicionales incluidas:",baseModelIncluded:"Modelo de incrustaciones base (jina-embeddings-v3) incluido",deptIncludes:"La licencia del departamento incluye:",deptReviews:"Reuniones trimestrales de revisión de negocios",deptRoadmap:"Planificación de la hoja de ruta de expansión empresarial",deptSponsor:"Sesiones de alineación de patrocinadores ejecutivos",deptWorkshops:"Talleres de colaboración entre departamentos",enterpriseAlert:"Su nivel de uso sugiere una oportunidad para toda la empresa. Programemos una llamada para analizar un acuerdo empresarial personalizado.",noModelsSelected:"No se seleccionaron modelos adicionales. Se utiliza el modelo de incrustaciones base.",pocCheckins:"Reuniones quincenales con el equipo técnico",pocIncludes:"El paquete POC incluye:",pocMetrics:"Panel de seguimiento de métricas de éxito",pocMigration:"Soporte de migración a licencia completa",pocTemplate:"Plantilla de documentación de resultados de POC",selectedModels:"Modelos Seleccionados:",standardFeatures:"Características de la licencia estándar:",supportTierIncluded:"{tier} nivel de soporte incluido {horas}",usageTierBusiness:"Nivel de uso comercial: hasta {count} cuentas comerciales",usageTierConsumer:"Nivel de uso del consumidor: hasta {count} usuarios activos mensuales"},models:{clip:"clip de jina v2",colbert:"jina-colbert-v2",description:"Elige los modelos a incluir en tu paquete comercial",embeddings:"incrustaciones de jina v3",lm:"lector-lm",reranker:"Jina Reranker V2",title:"Seleccionar modelos"},payment:{annual:"Facturación anual (10% de descuento)",features:"Características incluidas",quarterly:"Facturación trimestral",title:"Condiciones de pago"},poc:{description:"La prueba de concepto incluye el seguimiento de métricas de éxito y una ruta de actualización hacia la licencia completa",duration:"Duración de la prueba de concepto (meses)"},pricing:{annual:"año",cta:"Hable con nuestro equipo de ventas",disclaimer:"Esta calculadora de precios ofrece una estimación. El precio final puede variar según los requisitos específicos, los compromisos de volumen y las configuraciones personalizadas. Comuníquese con nuestro equipo de ventas para obtener una cotización detallada.",frequency:"${price} / {frequency}",oneTime:"${precio} pago único",pocTotal:"Precio de POC de {meses}-mes: ${price}",quarterly:"cuarto",title:"Precio estimado"},short_title:"Licencia de configuración",subtitle:"Configure su licencia empresarial para los modelos de Jina AI",support:{enterprise:"De primera calidad",hoursQuarter:"{horas} horas/cuarto",premium:"Estándar",standard:"Ligero",title:"Nivel de soporte"},title:"Configurador de licencias empresariales",tooltips:{annualDiscount:"Ahorre un 10% pagando anualmente",businessSponsor:"Tener una unidad de negocio patrocinadora puede calificar para descuentos adicionales",pocDuration:"Seleccione la duración de su período de prueba de concepto",supportTier:"Elige el nivel de soporte que mejor se adapte a tus necesidades",usageLimit:"Contáctenos para obtener precios personalizados si excede estos límites"},usage:{business:"B2B (Cuentas comerciales)",businessCount:"Número de cuentas comerciales",businessDescription:"Número de cuentas comerciales distintas que utilizan nuestros modelos",consumer:"B2C (usuarios finales)",consumerCount:"Usuarios activos mensuales",consumerDescription:"Número de usuarios finales activos mensuales en todas las aplicaciones",title:"Configuración de uso"}},P={answer1:"Jina AI se especializa en tecnologías de IA multimodales, incluido el ajuste de modelos, el servicio de modelos, el ajuste de avisos y el servicio de avisos. Aprovechamos herramientas avanzadas como Kubernetes y arquitecturas sin servidor para crear soluciones robustas, escalables y listas para producción.",answer10:"Brindamos diferentes opciones de licencia según la naturaleza del proyecto y las necesidades del cliente. Los términos detallados se pueden discutir con nuestro equipo de ventas.",answer11:"Brindamos servicios a nivel mundial, con nuestra sede central en Berlín, Europa, y oficinas adicionales en Beijing y Shenzhen.",answer12:"Sí, ofrecemos soporte en el sitio, especialmente para clientes ubicados cerca de nuestras oficinas en Berlín, Beijing y Shenzhen. Para otras ubicaciones, nos esforzamos por brindar el mejor soporte remoto posible y podemos organizar el soporte en el sitio si es necesario.",answer2:"Nuestra experiencia abarca un amplio espectro, que abarca grandes modelos de lenguaje, texto, imagen, video, comprensión de audio, búsqueda neuronal y arte generativo.",answer3:"Sí, nuestras soluciones están diseñadas para ser escalables y listas para la producción. Creamos nuestras soluciones utilizando tecnologías nativas de la nube que permiten un escalado eficiente y un rendimiento confiable en entornos de producción.",answer4:"Nuestros servicios son versátiles y adaptables, lo que los hace adecuados para una amplia gama de industrias, que incluyen comercio electrónico, tecnología legal, marketing digital, juegos, atención médica, finanzas y muchas más.",answer5:"Puede ponerse en contacto con nuestro equipo comercial a través del formulario de contacto de esta página. Nos encantaría discutir los requisitos de su proyecto y cómo nuestras soluciones pueden ayudar a su negocio.",answer6:"Brindamos soporte continuo para garantizar el buen funcionamiento de nuestras soluciones. Esto incluye resolución de problemas, actualizaciones periódicas y mejoras basadas en sus comentarios y necesidades.",answer7:"La duración del proyecto varía según la complejidad y el alcance del proyecto. Después de comprender sus requisitos, podemos proporcionarle una estimación más precisa.",answer8:"La seguridad de los datos es nuestra máxima prioridad. Nos adherimos a estrictas políticas y regulaciones de protección de datos para garantizar que sus datos estén seguros y confidenciales.",answer9:"El precio depende de la complejidad y los requisitos del proyecto. Ofrecemos modelos de precios basados ​​en proyectos y de retención. Póngase en contacto con nuestro equipo de ventas para obtener más información.",question1:"¿En qué se especializa Jina AI?",question10:"¿Cuáles son los términos de licencia para sus soluciones?",question11:"¿Cuál es su área de servicio?",question12:"¿Ofrecen soporte en el sitio?",question2:"¿Con qué tipos de IA trabaja Jina AI?",question3:"¿Sus soluciones son escalables y están listas para la producción?",question4:"¿Qué industrias pueden beneficiarse de las soluciones de Jina AI?",question5:"¿Cómo comenzamos un proyecto con Jina AI?",question6:"¿Qué apoyo brindan después de implementar una solución?",question7:"¿Cuál es la duración típica de un proyecto?",question8:"¿Cómo protege Jina AI mis datos?",question9:"¿Cuál es la estructura de precios de sus servicios?"},k="Preguntas más frecuentes",E={text:"Despedida.",toggle_btn:"Mantenga este panel abierto en su próxima visita",warning_message:"Este panel se abrirá automáticamente cuando visites jina.ai. Deberás cerrarlo para ver el contenido del sitio web. ¿Habilitar esta configuración?",warning_title:"Mostrar al iniciar"},I={description:"Ajuste las incrustaciones en datos específicos del dominio para una mejor calidad de búsqueda",intro:"Tu compañía. Tu información. tu modelo"},L={description:"Potencie su empresa con soluciones de ajuste fino en las instalaciones"},C={api_key:"Ingrese su clave API.",back:"Atrás",base_model_selected:"Modelo base seleccionado",click_start:"Acepte los términos y comience a realizar ajustes.",confirm_title:"Confirmar el trabajo de ajuste",confirm_your_email:"Vuelva a ingresar su dirección de correo electrónico para confirmar el trabajo de ajuste. Las actualizaciones y el enlace de descarga se enviarán a este correo electrónico.",consent0:"Acepto que se generen datos sintéticos para el ajuste del modelo según mis instrucciones.",consent1:"Reconozco que el modelo final y los datos sintéticos estarán accesibles públicamente en Hugging Face.",consent2:"Entiendo que esta función está en versión beta y Jina AI no ofrece garantías. El precio y la UX pueden cambiar.",continue:"Continuar",cost_1m_token:"Cada trabajo de ajuste consume 1 millón de tokens. Asegúrese de tener suficientes tokens o recargue su saldo. También puede generar una nueva clave API. Cada clave API viene con 1 millón de tokens gratuitos.",doc_explain:"Describe cómo debería verse un documento coincidente.",domain_explain:"Proporcione una descripción detallada de cómo se utilizarán las incrustaciones ajustadas. Esto es esencial para generar datos sintéticos de alta calidad que mejorarán el rendimiento de sus incrustaciones.",domain_explain2:"Hay tres formas de especificar sus requisitos: una instrucción general, una URL o una descripción del documento de consulta. Elige uno.",domain_hint:"Describe el dominio que deseas ajustar.",email_not_match:"Las direcciones de correo no coinciden. Por favor verificar.",failed_job:"La solicitud de ajuste falló. Vea el motivo a continuación.",find_on_huggingface:"Encuentra resultados en Abrazar la cara",general_instruction:"O instrucción general",general_instruction_caption:"Proporcione una descripción detallada de cómo se utilizarán las incrustaciones ajustadas.",general_instruction_explain:'Describe tu dominio en texto de formato libre. Puedes imaginarlo como un "mensaje" como en ChatGPT.',how_it_works:"Obtenga más información sobre el proceso de ajuste.",job_acknowledged:"Su trabajo de ajuste ha sido puesto en cola. Recibirás un correo electrónico cuando comience el trabajo. El proceso completo suele tardar 20 minutos en completarse.",new_key:"Obtener nueva clave",not_enough_token:"No hay suficientes tokens en esta clave API. Recargue su saldo o utilice una clave API diferente.",placeholder:"Reclamaciones de seguros de automóviles",preview:"Avance",query_doc:"Descripción del documento de consulta",query_doc_caption:"Describe cómo se ve la consulta y cómo se ve el documento coincidente en tu dominio.",query_explain:"Describe cómo se ve una consulta.",reset:"Comenzar de nuevo",select_base_model:"Elija un modelo de incrustación base para realizar ajustes.",select_base_model_explain:"Seleccione un modelo base como punto de partida para el ajuste fino. Normalmente, base-en es una buena opción, pero para tareas en otros idiomas, considere utilizar un modelo bilingüe.",start_tuning:"Comience a realizar ajustes",url:"O URL de la página web",url_caption:"Consulte el contenido de una URL para realizar ajustes.",url_explain:"URL pública de una página web que contiene el contenido que desea ajustar.",use_url:"Utilice URL en su lugar. Activarlo significa que nos basaremos en el contenido de la página de esa URL para generar datos sintéticos para realizar ajustes.",wait_for_processing:"Espere mientras procesamos su solicitud...",which_domain:"Dominio de ajuste",write_email_explain:"El ajuste fino lleva tiempo. Nos comunicaremos por correo electrónico sobre el inicio, el progreso, la finalización y cualquier problema relacionado con su trabajo de ajuste, junto con detalles sobre el modelo ajustado y el conjunto de datos de entrenamiento."},S={address_beijing:"Beijing, China",address_berlin:"Berlín, Alemania (sede central)",address_shenzhen:"Shenzhen, China",address_sunnyvale:"Sunnyvale, California",all_rights_reserved:"Reservados todos los derechos.",api_documentation:"Documentación API",company:"Compañía",developers:"Desarrolladores",docs:"Documentos",enterprise:"Empresa",get_api_key:"Obtener la clave API de Jina",offices:"Oficinas",power_users:"Usuarios avanzados",privacy:"Privacidad",privacy_policy:"política de privacidad",privacy_settings:"Administrar cookies",security:"Seguridad",sefo:"Fundación de búsqueda",soc2:"Cumplimos con la norma SOC 2 Tipo 1 y 2 del Instituto Americano de Contadores Públicos Certificados (AICPA).",status:"Estado de la API",status_short:"Estado",tc:"Términos y condiciones",tc1:"Términos"},w="Obtenga su clave API",M={stars:"Estrellas"},R={description:"Declaraciones de base con conocimiento web",title:"Verificación de hechos",usage:"Uso de la conexión a tierra"},T={about_us:"Sobre nosotros",api_docs:"Documentación de la API",api_docs_explain:"Generación automática de código para su IDE o LLM de Copilot",company:"Compañía",contact_us:"Contactar con ventas",developers_others:"Más herramientas para desarrolladores",enterprise_others:"Más herramientas empresariales",for_developers:"Para desarrolladores",for_developers_description:"Experimente una pila integral de IA multimodal de código abierto diseñada para desarrolladores.",for_enterprise:"Para Empresas",for_enterprise_description:"Descubra estrategias escalables de IA multimodal diseñadas para satisfacer las necesidades comerciales.",for_power_users:"Para usuarios avanzados",for_power_users_description:"Utilice nuestras herramientas multimodales optimizadas para mejorar su productividad.",internship1:"Programa de prácticas",jobs:"Únete a nosotros",join_discord:"Únete a nuestra comunidad de Discord",logos:"Descargar logotipo",maximize:"⇧1",maximize_btn:"Maximizar",news:"Noticias",open_day:"Día abierto",open_in_full:"Mostrar todos los productos empresariales en una nueva ventana",power_users_others:"Más herramientas para usuarios avanzados",products:"Productos"},D={description:"Comparta y descubra componentes básicos para aplicaciones de IA multimodal"},N={sentence_similarity:"incrustación de oraciones",updated_about:"actualizado sobre"},B={project1:"Búsqueda de alta precisión habilitada dentro de datos de malla 3D utilizando información de nube de puntos.",project10:"Aprovechó la visión artificial para mejorar la accesibilidad digital de los sitios web gubernamentales.",project11:"LLM ajustado para una firma de consultoría para optimizar el análisis de datos financieros.",project12:"Estrategias de marketing avanzadas mediante el ajuste fino de los modelos de texto a imagen para la transferencia de estilo.",project2:"Diseñé un motor de búsqueda basado en contenido para cortometrajes de animación.",project3:"Tasas de conversión de comercio electrónico mejoradas mediante el ajuste fino de los modelos integrados.",project4:"Realicé ajustes rápidos para aumentar la eficiencia de una empresa de consultoría empresarial.",project5:"Pionero en la comprensión de la escena del juego y la anotación automática para una empresa de juegos líder.",project6:"Implementé la expansión de entrada en tiempo real para una empresa de chatbot, mejorando la experiencia del usuario.",project7:"Revolucionó la tecnología legal al permitir una búsqueda eficiente en documentos legales extensos.",project8:"Apoyó un servicio de arte generativo de alto rendimiento para operaciones a gran escala.",project9:"Realización de minería y modelado de procesos utilizando modelos de lenguaje avanzado."},U={description:"Modelos multimodales de última generación disponibles para inferencia"},J={copy_full_prompt:"Copiar mensaje completo",embedding:"Incrustaciones",how_to_use_meta_prompt:"Cómo utilizar",meta_prompt:"Utilice Meta-prompt para la generación de código",meta_prompt_description:"Meta-prompt guía a los LLM (como ChatGPT y Claude) a través de todas nuestras API de Search Foundation, lo que hace que la generación de código sea más sencilla y de mayor calidad.",reranker:"reclasificador",which_to_go:"¿Cuál integrar con {_vendor}?"},G={answer1:"Licenciatura, maestría y doctorado. Se alienta a los estudiantes de todo el mundo, con interés en campos como la investigación, la ingeniería, el marketing y las ventas, a postularse. También damos la bienvenida a pasantías no técnicas en marketing, ventas, asistencia ejecutiva y más. Estamos buscando personas apasionadas listas para ser pioneros en la IA multimodal con nosotros.",answer10:"Sí, nuestro programa de prácticas ofrece una remuneración competitiva.",answer11:"Como pasante de Jina AI, obtendrá experiencia práctica trabajando en proyectos desafiantes, aprenderá de expertos de la industria, será parte de una comunidad vibrante y tendrá la oportunidad de hacer contribuciones reales a nuestro trabajo pionero en AI multimodal.",answer2:"Las pasantías deben llevarse a cabo en el sitio en una de nuestras oficinas, que se encuentran en Berlín, Beijing y Shenzhen.",answer3:"Sí, Jina AI ofrece asistencia razonable en el proceso de visa para los solicitantes seleccionados.",answer4:"Sí, Jina AI brinda una cantidad razonable de cobertura de costo de vida para los pasantes durante el período de pasantía.",answer5:"Sí, es posible trabajar en su tesis de maestría durante su pasantía en Jina AI, generalmente aplicable a estudiantes en universidades alemanas. Sin embargo, debe tener una comunicación previa y acuerdo del supervisor de su universidad. Tenga en cuenta que no ayudamos a los estudiantes a encontrar asesores.",answer6:"El proceso de solicitud incluye enviar su formulario de solicitud, un currículum, una carta de presentación que exprese su interés y motivación, y cualquier enlace profesional relevante como GitHub o LinkedIn. Evaluamos a los candidatos en función de su desempeño durante la entrevista y su desempeño en su universidad.",answer7:"Sí, los pasantes exitosos pueden recibir una carta de recomendación al final de su pasantía, firmada por nuestro CEO.",answer8:"La duración de la pasantía varía según el rol y el proyecto. Sin embargo, por lo general oscila entre tres y seis meses.",answer9:"Sí, aceptamos solicitudes de todos los antecedentes académicos. Valoramos su pasión y compromiso por aprender tanto como su experiencia previa.",question1:"¿Quién puede solicitar el programa de pasantías de Jina AI?",question10:"¿Es esta una pasantía remunerada?",question11:"¿Qué oportunidades tendré como pasante de Jina AI?",question2:"¿Dónde se realizará la pasantía?",question3:"¿Jina AI ayuda con los procesos de visa?",question4:"¿Jina AI proporciona asignaciones o beneficios para los pasantes?",question5:"¿Puedo trabajar en mi tesis de maestría durante la pasantía en Jina AI?",question6:"¿En qué consiste el proceso de solicitud?",question7:"¿Jina AI proporciona alguna carta de recomendación posterior a la pasantía?",question8:"¿Cuál es la duración de la pasantía?",question9:"¿Puedo presentar una solicitud si no tengo experiencia previa en IA?"},O={about_internship_program:"Acerca del programa de pasantías",about_internship_program_desc1:"Estamos emocionados de ofrecer esta oportunidad única para que personas talentosas se unan a nuestro equipo dinámico y contribuyan a proyectos innovadores en el campo de la Inteligencia Artificial. Esta pasantía está diseñada para brindarle una valiosa experiencia práctica, tutoría y exposición a tecnologías de vanguardia que están dando forma al futuro de la IA.",about_internship_program_desc2:"En Jina AI, entendemos la importancia de nutrir y aprovechar el talento joven. Reconocemos que los pasantes aportan nuevas perspectivas, entusiasmo y creatividad a la mesa, fortaleciendo a nuestro equipo con nuevas ideas y enfoques. Al proporcionar pasantías, nuestro objetivo es fomentar el crecimiento de los futuros líderes en la industria de la IA al tiempo que les ofrecemos una experiencia del mundo real en un entorno estimulante y de apoyo.",alumni:"ALUMNOS",alumni_network:"Nuestra próspera red de antiguos alumnos",application:"Solicitud",application_desc:"Embárquese en un viaje transformador con Jina AI. Nuestro completo programa de pasantías invita a todas las mentes apasionadas que aspiran a dar forma al futuro de la inteligencia artificial. Únase a nosotros para obtener experiencia en el mundo real, trabajar en proyectos desafiantes y colaborar con algunas de las mentes más brillantes de la industria de la IA.",apply:"Aplica ya",autumn:"Otoño",description:"Convocatoria mundial para estudiantes: Prácticas en investigación, ingeniería, marketing, ventas y más.",dev_rel_intern:"Pasante de Relaciones con Desarrolladores",enthusiastic:"ENTUSIASTA",explore_stories_from_our_interns:"Explore las historias de nuestros pasantes",explore_stories_from_our_interns1:"Inspírate con los viajes de nuestros pasantes",innovative:"INNOVADOR",intern_work1:"Modelos LLM ajustados para mejores incrustaciones",intern_work2:"Exploró el potencial de recuperación de generación aumentada",intern_work3:"Publicó un artículo sobre el tema de las incrustaciones de oraciones.",intern_work4:"Inyectar vitalidad juvenil continua al equipo.",intern_work5:"Técnicas de cuantificación comparadas para comprimir LLM",intern_work6:"Creación y promoción de campañas atractivas para PromptPerfect",intern_work7:"JinaColBERT V2 desarrollado y mejorado rápidamente",recruiting_and_administrative_intern:"Practicante de Reclutamiento y Administración",researcher_intern:"Investigador en prácticas",self_motivated:"AUTO MOTIVADO",software_engineer_intern:"Pasante de ingeniería de software",spring:"Primavera",submit_application:"Comienza tu aventura con Jina AI",subtitle:"Nuestro programa de pasantías de tiempo completo brinda experiencia laboral práctica a través de proyectos de pasantías bien diseñados en una amplia gama de alcances.",subtitle1:"Convocatoria mundial para estudiantes: Pasantes en investigación, ingeniería, marketing, ventas y más para ser pioneros juntos en la IA multimodal.",summer:"Verano",title:"Programa de pasantías",who_do_we_look_for:"¿A quién buscamos?",who_do_we_look_for_desc:"Valoramos la diversidad y alentamos a los solicitantes de diversos perfiles y antecedentes a unirse a nuestro Programa de pasantías. Las oportunidades de pasantías se ofrecen en varios departamentos, incluidos ingeniería, diseño, gestión de productos, gestión de ventas y cuentas, marketing y gestión comunitaria.",winter:"Invierno"},F={description:"Implemente un proyecto local como un servicio en la nube. Radicalmente fácil, sin sorpresas desagradables."},H={description:"Un perfeccionador experimental para LLM de código abierto"},Q={description:"Cree aplicaciones de IA multimodales en la nube"},V={description:"Más modalidad, más memoria, menos costo",example_1:"¿Quién eres?",example_2:"Soy un servicio de chat LLM creado por Jina AI"},W={add:"Agregar clave",add_key_explain:"Añade otra clave API a tu cuenta. Las claves añadidas se pueden gestionar, recargar o eliminar en cualquier momento.",add_shared_key:"Agregar a mis claves",add_success:"Se agregó exitosamente una clave {_key}.",advance_settings:"Abrir configuración avanzada",advanced_feature:"Función avanzada solo para clave premium.",auto_recharge_enable_success:"Se habilitó exitosamente la recarga automática para la clave {_key}.",auto_recharge_title:"¿Habilitar recarga automática?",auto_reminder:"Recordatorio de saldo bajo",auto_reminder_cancel_message:"¿Está seguro de que desea cancelar el recordatorio automático de esta clave?",auto_reminder_cancel_title:"Cancelar recordatorio automático",auto_reminder_description:"Reciba alertas automáticas por correo electrónico cuando el saldo de su token caiga por debajo de los límites establecidos. Puede configurar hasta tres límites.",auto_reminder_email:"Dirección de correo electrónico para recordatorios",auto_reminder_info:"La notificación se enviará a {_email} cuando el saldo de tokens caiga por debajo de {_threshold} tokens.",auto_reminder_threshold:"Recordar si",auto_reminder_threshold_error:"El umbral debe estar entre 1 y 1T.",auto_reminder_toggle:"Activar recordatorio automático. Tenga en cuenta que solo la clave premium puede habilitar esta función.",available_resources:"Tokens disponibles",balance:"Tokens disponibles",balance_primary_key:"Saldo de clave principal",cancel:"Cancelar",confirm:"Confirmar",copy:"Copiar clave",copy_share_link:"Copiar enlace",description:"Administre claves API para todos los servicios de Jina AI: incrustaciones, lector, reranker y más.",do_it_later:"Hazlo más tarde",email:"Correo electrónico",existing_key:"Clave existente",filter_by:"Filtrar por clave",free_key:"Llave gratis",generate_new_key:"Generar nueva clave",generate_new_key_tooltip:"Genera una nueva clave API con saldo vacío. Puedes recargar el saldo más tarde.",generate_success:"Se generó exitosamente una nueva clave {_key}.",get_free_key:"Crear clave API",ignore:"Ignorar",invalid_email:"Correo electrónico no válido",invalid_key:"Clave inválida",is_primary:"Tu clave API principal. Puedes cambiarla después de iniciar sesión.",last_used:"Último uso",last_used_at:"Última actividad",login:"Acceso",login_explain:"Administre múltiples claves API y realice un seguimiento del uso, todo en una sola cuenta.",login_explain_long:"Inicie sesión para almacenar y administrar de forma segura sus claves API. Realice un seguimiento del historial de uso, administre varias claves y nunca pierda el acceso a sus credenciales.",login_required:"Inicie sesión antes de agregar la clave compartida.",login_via:"Inició sesión a través de {_provider}",logout:"Finalizar la sesión",logout_message:"Tus claves API se almacenan de forma segura en tu cuenta. Inicia sesión en cualquier momento para administrarlas.",logout_success:"Se ha cerrado la sesión correctamente",no_key_title:"¿Necesita una clave API?",no_key_with_login:"Aún no has creado una clave API. Genera una ahora y obtén tokens gratis para comenzar.",no_key_without_login:"¿Ya tienes una cuenta? Inicia sesión para acceder a tus claves API o haz clic en '{_button}' para crear una nueva.",no_transferable_keys:"No hay otras claves disponibles para transferir, primero agregue una nueva clave.",ok:"DE ACUERDO",primary_key:"Establecer como clave principal",primary_key_set:"Se ha establecido correctamente {_apiKey} como su clave principal.",primary_key_set_caption:"Esta clave se utilizará en todas las demostraciones, ejemplos y áreas de juego en jina.ai.",purchase:"Comprar tokens",recharge_threshold_confirm_message:"¿Está seguro de que desea cambiar el umbral de recarga automática a {_threshold} tokens?",recharge_threshold_confirm_title:"Cambiar el umbral de recarga automática",remove:"Quitar clave",remove_explain:"Eliminar la clave de la lista no afectará al servicio, las operaciones dependientes ni a otros usuarios que la hayan almacenado. La clave seguirá funcionando y podrá volver a agregarse en cualquier momento.",remove_message:"¿Está seguro de que desea eliminar esta clave? La clave sigue funcionando y puede volver a agregarse en cualquier momento.",remove_primary_key:"Establezca otra clave como principal antes de eliminar la clave principal actual.",remove_success:"Se eliminó exitosamente la clave {_key}.",remove_title:"Quitar clave",revoke:"Revocar clave",revoke_error:"La clave ingresada no coincide con la clave que intenta revocar.",revoke_explain:"La revocación de una clave la desactivará inmediatamente para todos los usuarios que la hayan almacenado, y todo el saldo restante y las propiedades asociadas quedarán inutilizables de forma permanente. Esta acción no se puede deshacer.",revoke_label:"Por favor, confirme la revocación de esta clave escribiéndola a continuación",revoke_message:"¿Está seguro de que desea revocar esta clave? Una vez revocada, esta clave dejará de ser válida de forma permanente para todos los usuarios que la hayan almacenado. Todo el saldo restante y las propiedades asociadas quedarán inutilizables de forma permanente. Esta acción no se puede deshacer.",revoke_success:"Se revocó exitosamente la clave {_key}.",revoke_title:"Revocar clave",save:"Ahorrar",settings:"Ajustes",share:"Compartir clave",share_key_confirm_message:`El destinatario podrá ver, administrar y recargar el saldo de esta clave. Conservará las mismas funciones. 
Tenga en cuenta que el enlace caducará en 24 horas.`,share_key_confirm_title:"Compartir clave API",share_key_expired_at:"¡El enlace compartido expirará en {_time}!",share_key_expired_message:"El enlace compartido de la clave ha expirado. Solicite al propietario de la clave que la comparta nuevamente.",share_key_expired_title:"El enlace compartido ha expirado",share_key_message:"{_user} ha compartido una clave API contigo. Agrégala para administrar la clave y su saldo.",share_link_copied:"Compartir enlace copiado",shared_from:"Clave compartida por {_user}",shared_key:"Clave compartida",subscribed_key:"Clave Premium",title:"API de la Fundación Jina Search",to_dashboard:"Administrar claves",top_up:"Completar",total_keys:"Claves totales",transfer_before_revoke:"Transfiera los tokens pagados restantes antes de revocar la clave.",transfer_explain:"Transfiera sin problemas sus tokens pagados restantes a otra cuenta para disfrutar de una mayor flexibilidad y seguridad en la administración de sus recursos.",transfer_label:"Transferencia a",transfer_message:"¿Está seguro de que desea transferir sus tokens pagados restantes {_tokens} de {_source} a {_target}?",transfer_success:"Los tokens se transfirieron exitosamente de {_source} a {_target}.",transfer_title:"Tokens de transferencia",usage_history:"Historial de uso",usage_summary:"Últimos 7 días: {_usage} tokens"},K={GlobalQA:{description:"Presione la tecla '/' en cualquier página para abrir el cuadro de preguntas. Escriba su consulta y presione 'Entrar' para recibir respuestas directamente relacionadas con el contenido de la página. Esta característica está impulsada por PromptPerfect.",title:"RAG en la página"},Recommender:{description:"Abra el cuadro de recomendación en cualquier página de noticias con 'Shift+2'. Seleccione el modelo de reranker para descubrir los 5 artículos principales relacionados con esa página de noticias. Disfrute de esta función en tiempo real, impulsada por nuestra API Reranker.",title:"Artículo relacionado"},SceneXplainTooltip:{description:"Pase el cursor sobre cualquier imagen en las páginas de noticias o en nuestro catálogo de redacción para revelar la descripción de esa imagen. Las descripciones las calcula previamente SceneXplain y las incrustan en el atributo ALT de la imagen para mayor accesibilidad.",title:"Subtítulos de imágenes"},explain:"Descubra funciones ocultas en nuestro sitio web"},X={also_available_on:"También disponible en los mercados.",also_available_on1:"Disponible en los mercados de su nube empresarial",ask_how_your_question:"Por favor describe tu problema",autotune:"Ajuste fino automático",avatar:"Generador de avatares",badge:{"clip-v2":"¡Lanzamiento de clip-v2!","readerlm-v2":"¡Lanzamiento de ReaderLM-v2!",v2:"Lanzamiento v2!",v3:"¡Lanzamiento v3!"},browser_info_title:"Información del navegador",build_js:"Construir con JavaScript",build_python:"Construir con Python",ccbync:"Este modelo tiene licencia CC BY-NC 4.0. Úselo a través de la API o nuestra imagen oficial de AWS/Azure, o comuníquese con el departamento de ventas para una implementación local.",checkout_our_solution_for_you:"Descubra nuestra solución a su medida",classifier:"Clasificador",coming_soon:"Muy pronto",contact_sales:"Contacto",copied_to_clipboard:"Copiado al portapapeles",copy:"Copiar",developers:"Desarrolladores",developers_desc:"Libere todo el poder de la IA multimodal con tecnologías nativas de la nube de vanguardia e infraestructura de código abierto.",download_pdf:"Descargar PDF",embedding:"Incrustaciones",embedding_desc1:"Incrustaciones de contexto largo multilingües y multimodales de alto rendimiento para aplicaciones de búsqueda, RAG y agentes.",embedding_paper_desc:"Jina Embeddings constituye un conjunto de modelos de incrustación de oraciones de alto rendimiento, expertos en traducir varias entradas textuales en representaciones numéricas, capturando así la esencia semántica del texto. Si bien estos modelos no están diseñados exclusivamente para la generación de texto, se destacan en aplicaciones como la recuperación densa y la similitud textual semántica. Este documento detalla el desarrollo de Jina Embeddings, comenzando con la creación de un conjunto de datos de pares y triples de alta calidad. Subraya el papel crucial de la limpieza de datos en la preparación del conjunto de datos, brinda información detallada sobre el proceso de capacitación del modelo y concluye con una evaluación integral del rendimiento utilizando Massive Textual Embedding Benchmark (MTEB).",embedding_paper_title:"Jina Embeddings: un novedoso conjunto de modelos de incrustación de oraciones de alto rendimiento",embeddings:"Incrustaciones",enterprise:"Empresa",enterprise_desc:"Impulse su negocio con soluciones de IA multimodal escalables, seguras y personalizadas.",enterprise_desc_v2:"Pruebe nuestros modelos de integración de clase mundial para mejorar sus sistemas de búsqueda y RAG. ¡Empiece con una prueba gratuita!",enterprise_desc_v3:"Nuestros modelos de frontera forman la base de búsqueda para sistemas RAG y de búsqueda empresarial de alta calidad.",error:"Hubo un problema con la operación de búsqueda: {mensaje}",find_your_portal:"Encuentre su Portal",finding_faq:"Generando respuestas basadas en el conocimiento de las preguntas frecuentes a continuación",for:"Para",for_better_search:"Para una mejor búsqueda",for_developers:"Para desarrolladores",for_enterprise:"Para empresas",for_power_users:"Para usuarios avanzados",get_api_now:"API",get_started:"Empezar",go_to_product_homepage:"Ir a la página de inicio del producto",grounding:"Toma de tierra",how_to:"Cómo",include_experiment:"Incluye nuestros proyectos experimentales y archivados en la solución.",join_community:"Comunidad",key_manager:"Administrar clave API",learn_more_embeddings:"Más información sobre incrustaciones",learn_more_reader:"Más información sobre el lector",learn_more_reranker:"Más información sobre el cambio de rango",llm:"Modelos de inclusión LLM",llm_desc:"Proporcionamos una colección de modelos de incrustación de oraciones de alto rendimiento, con entre 35 millones y 6 mil millones de parámetros. Son excelentes para mejorar la búsqueda neuronal, la reclasificación, la similitud de oraciones, las recomendaciones, etc. ¡Prepárate para mejorar tu experiencia de IA!",mentioned_products:"Productos mencionados:",mmstack:"Stack multimodal",mmstack_desc:"A lo largo de los años, hemos desarrollado una variedad de software de código abierto para ayudar a los desarrolladores a crear mejores aplicaciones GenAI y de búsqueda más rápido.",models:"Modelos",more:"Más",multimodal:"Multimodal",multimodal_ai:"IA multimodal",new:"Nuevo",newsroom:"Sala de prensa",num_publications:"{_total} publicaciones en total.","on-prem-deploy":"Implementación local","on-premises":"En las instalaciones",opensource:"Código abierto",our_customer:"Nuestros clientes",our_customer_explain:"Empresas de todos los tamaños confían en la Fundación de búsqueda de Jina AI para potenciar sus herramientas y productos; usted también puede hacerlo.",our_publications:"Nuestras Publicaciones",parameters:"Parámetros",podcast:"Podcast",power_users:"Usuarios avanzados",power_users_desc:"Ingeniería automática rápida para su productividad diaria.",powered_by_promptperfect:'Desarrollado por la función "Optimización de solicitud" y "Solicitud como servicio" de PromptPerfect',pricing:"Precios",proposing_solution:"Proponiendo una solución basada en los productos Jina AI...",read_more:"Leer más",reader:"Lector",require_full_question:"Describe tu problema con más detalles.",reranker:"reclasificador",researcher_desc:"Comprenda cómo se entrenaron nuestros modelos de búsqueda de frontera desde cero; consulte nuestras últimas publicaciones. ¡Conozca a nuestro equipo en EMNLP, SIGIR, ICLR, NeurIPS e ICML!",researchers:"Investigadores",sdk:"SDK",sdk_desc:"¿Quiere crear aplicaciones AIGC de alto nivel con las API de PromptPerfect, SceneXplain, BestBanner, JinaChat y Rationale? ¡Le tenemos cubierto! Pruebe nuestro SDK fácil de usar y comience en minutos.",sdk_docs:"Leer documentos",sdk_example:"Ejemplo",search_foundation:"Fundación de búsqueda",source_code:"Código fuente",starter_kit:"Kit de inicio",supercharged1:"a todo motor!",tokenizer:"Segmentador",trusted_by:"DE CONFIANZA PARA",try_it_for_free:"¡Empiece de inmediato, sin necesidad de tarjeta de crédito ni registro!",try_our_saas:"Pruebe nuestra solución alojada, un reemplazo directo de la API integrada de OpenAI.",version_notify:"Estás viendo una versión anterior de este sitio web. Para ver las funciones más recientes, visita {_link}",view_browser_info:"Ver información del navegador",your_portal_to:"Tu Portal a",your_search_foundation1:"¡Tu base de búsqueda"},Y={description:"Aplicaciones Langchain en producción con Jina y FastAPI"},$={description:"Información legal, términos de servicio, política de privacidad y otros documentos importantes sobre los productos y servicios de Jina AI.",download_type1:"Descargar Certificación SOC 2 Tipo 1",download_type2:"Descargar Certificación SOC 2 Tipo 2",request_audit:"Solicitar informe de auditoría",title:"Información legal"},Z={api:"API de inteligencia artificial de Jina",browse_catalog:"Explorar catálogo",contact_sales_about_it:"Contacte con ventas al respecto",deploy_it_on:"Implementarlo en",description:"Hemos estado haciendo cambios en los modelos de búsqueda desde el primer día. Eche un vistazo a la evolución de nuestro modelo a continuación: pase el cursor o haga clic para descubrir cada hito.",find_on_hf:"Encuéntrelo en HuggingFace",search_for:"Buscalo en nuestro sitio",search_models:"Filtrar por nombre de modelo",title:"Nuestros modelos de base de búsqueda",use_it_via:"Úselo a través de"},ee={back_to_models:"Volver a modelos",comparison:{btn:"Comparar",select_models:"Elige modelos para comparar"},error:"No se pudo cargar el modelo",input_type:{"3d":"3D",audio:"Audio",code:"Código",document:"Documento",graph:"Gráfico",image:"Imagen","multi-vector":"Multi-vectorial",other:"Otro",ranking:"Clasificaciones",tabular:"Tabular",text:"Texto","text (code)":"Texto (Código)","text (document)":"Texto (Documento)","text (html)":"Texto (HTML)","text (json)":"Texto (JSON)","text (markdown)":"Texto (Markdown)","text (query)":"Texto (Consulta)",timeseries:"Serie temporal",vector:"Vector",video:"Video"},loading:"Cargando detalles del modelo...",metadata:{api_link:"API de Jina",arxiv:"Documento de ArXiv",aws_link:"AWS SageMaker",azure_link:"Microsoft Azure",deprecated_by:"Obsoleto por",gcp_link:"Nube de Google",huggingface_link:"Cara abrazada",input_type:"Aporte",license:"Licencia",license_link:"Licencia comercial",output_type:"Producción","reader-api_link":"API de Jina",related_models:"Modelos relacionados",release_blog:"Publicación de lanzamiento",release_date:"Fecha de lanzamiento"},search:{no_results:'No se encontraron modelos que coincidan con "{query}"',placeholder:"Buscar por nombre, etiquetas o tipo..."},sections:{availability:"Disponibilidad",blogs:"Blogs que mencionan este modelo",external_links:"Enlaces y recursos externos",guidance:{"ReaderLM-v2":"Se puede acceder al modelo a través de un notebook de Google Colab que muestra la conversión de HTML a Markdown, la extracción de JSON y el seguimiento de instrucciones. Para las tareas de HTML a Markdown, los usuarios pueden ingresar HTML sin instrucciones de prefijo, mientras que la extracción de JSON requiere un formato de esquema específico. La función auxiliar create_prompt facilita la creación de indicaciones para ambas tareas. Si bien el modelo funciona en el nivel de GPU T4 gratuito de Colab (que requiere vllm y triton), tiene limitaciones sin compatibilidad con bfloat16 o Flash Attention 2. Se recomienda RTX 3090/4090 para uso en producción. El modelo estará disponible en AWS SageMaker, Azure y GCP Marketplace, con licencia CC BY-NC 4.0 para uso no comercial.","jina-clip-v1":"Para implementar Jina CLIP v1 de manera eficaz, los equipos deben considerar tanto sus capacidades como sus requisitos de recursos. El modelo procesa imágenes en mosaicos de 224 x 224 píxeles, y cada mosaico consume 1000 tokens de capacidad de procesamiento. Para lograr un rendimiento óptimo, implemente un preprocesamiento de imágenes eficiente para que coincida con estas dimensiones. Si bien el modelo se destaca en el procesamiento de textos cortos y largos, actualmente solo admite la entrada en idioma inglés. Los equipos deben considerar cuidadosamente el uso de tokens: el texto requiere aproximadamente 1,1 tokens por palabra, mientras que las imágenes se procesan en mosaicos (por ejemplo, una imagen de 750 x 500 píxeles requiere 12 mosaicos, lo que consume 12 000 tokens). El modelo está disponible a través de la API Jina Embeddings y como una versión de código abierto en Hugging Face bajo la licencia Apache 2.0, lo que ofrece flexibilidad en las opciones de implementación. Para entornos de producción, considere usar las opciones de implementación de AWS Marketplace o Azure, que brindan configuraciones de infraestructura optimizadas.","jina-clip-v2":"Para una implementación óptima, los usuarios deben tener en cuenta varios factores clave. El modelo requiere hardware compatible con CUDA para un procesamiento eficiente, con requisitos de memoria que se escalan en función del tamaño del lote y la resolución de la imagen. Para optimizar los costos y el rendimiento de la API, cambie el tamaño de las imágenes a 512 x 512 píxeles antes del procesamiento: las imágenes más grandes se organizan automáticamente en mosaicos, lo que aumenta el uso de tokens y el tiempo de procesamiento. El modelo se destaca por hacer coincidir imágenes con texto descriptivo en todos los idiomas, pero puede tener dificultades con conceptos abstractos o contenido altamente especializado y específico del dominio. Es particularmente eficaz para la búsqueda de productos de comercio electrónico, sistemas de recomendación de contenido y aplicaciones de búsqueda visual, pero puede no ser adecuado para tareas que requieran un análisis de detalles visuales de grano fino o experiencia en el dominio altamente especializada. Al utilizar la función de representación Matryoshka, considere la compensación entre la reducción de dimensiones y el rendimiento: si bien las incrustaciones de 64 dimensiones mantienen un rendimiento sólido, las aplicaciones críticas pueden beneficiarse de dimensiones más altas.","jina-colbert-v1-en":"Para implementar Jina-ColBERT-v1-en de manera eficaz, los equipos deben considerar varios aspectos prácticos. El modelo requiere una GPU compatible con CUDA para un rendimiento óptimo, aunque es posible la inferencia de CPU para el desarrollo. Para el procesamiento de documentos, el límite de 8192 tokens se traduce en aproximadamente 6000 palabras, lo que lo hace adecuado para la mayoría de los tipos de documentos, incluidos artículos académicos, documentación técnica y contenido de formato largo. Los equipos deben implementar un preprocesamiento de documentos eficiente para manejar los límites de tokens y considerar el procesamiento por lotes para la indexación a gran escala. Si bien el modelo se destaca en contenido en inglés, no está diseñado para aplicaciones multilingües o recuperación en varios idiomas. Para implementaciones de producción, implemente estrategias adecuadas de fragmentación de documentos y considere el uso de índices de similitud de vectores (como FAISS) para una recuperación eficiente. El modelo es particularmente eficaz cuando se integra en canalizaciones RAG mediante marcos como RAGatouille, que simplifica la implementación de patrones de recuperación complejos.","jina-colbert-v2":"Para implementar Jina-ColBERT-v2 de manera eficaz, los equipos deben considerar varios aspectos prácticos. El modelo requiere hardware compatible con CUDA para un rendimiento óptimo y admite longitudes de documentos de hasta 8192 tokens (ampliables a 12 288) y limita las consultas a 32 tokens. Para la implementación en producción, el modelo está disponible a través de la API de Jina Search Foundation, el mercado de AWS y Azure, con una versión no comercial a la que se puede acceder a través de Hugging Face. Al implementar, los equipos deben especificar si están incorporando consultas o documentos, ya que el modelo utiliza codificación asimétrica. El modelo no está diseñado para el procesamiento en tiempo real de colecciones de documentos extremadamente grandes sin una indexación adecuada y, si bien se destaca en la recuperación multilingüe, puede mostrar un rendimiento ligeramente inferior en tareas especializadas específicas de dominio en comparación con los modelos ajustados para esos dominios específicos.","jina-embedding-b-en-v1":"Para una implementación óptima, el modelo requiere una GPU compatible con CUDA, aunque su tamaño moderado permite una inferencia eficiente en hardware estándar. El modelo acepta secuencias de entrada de hasta 512 tokens de longitud y es particularmente adecuado para entornos de producción donde la generación de incrustaciones consistentes y confiables es crucial. Funciona mejor con contenido en inglés y es ideal para aplicaciones como búsqueda semántica, comparación de similitudes de documentos y sistemas de recomendación de contenido. Los equipos deben considerar el uso de las versiones más nuevas v2 o v3 para nuevos proyectos, ya que ofrecen un rendimiento mejorado y un soporte de idiomas más amplio. El modelo no se recomienda para tareas que requieran comprensión multilingüe o conocimiento de dominio especializado fuera del texto en inglés general.","jina-embeddings-v2-base-code":"Para implementar de manera eficaz el código base de Jina Embeddings v2, los equipos deben considerar varios aspectos prácticos. El modelo se integra perfectamente con bases de datos vectoriales populares como MongoDB, Qdrant y Weaviate, lo que facilita la creación de sistemas de búsqueda de código escalables. Para lograr un rendimiento óptimo, implemente un preprocesamiento de código adecuado para manejar el límite de 8192 tokens, que generalmente admite la mayoría de las definiciones de funciones y clases. Si bien el modelo admite 30 lenguajes de programación, muestra el mejor rendimiento en los seis lenguajes principales: Python, JavaScript, Java, PHP, Go y Ruby. Los equipos deben considerar el uso del procesamiento por lotes para la indexación de código a gran escala para optimizar el rendimiento. La compatibilidad del modelo con RAG lo hace particularmente eficaz para la generación automatizada de documentación y las tareas de comprensión de código, aunque los equipos deben implementar estrategias de fragmentación adecuadas para bases de código muy grandes. Para implementaciones de producción, considere usar el punto de conexión de AWS SageMaker para la inferencia administrada e implemente estrategias de almacenamiento en caché adecuadas para optimizar el rendimiento de las consultas.","jina-embeddings-v2-base-de":"Para implementar Jina Embeddings v2 Base German de manera eficaz, las organizaciones deben considerar varios aspectos prácticos. El modelo se integra perfectamente con bases de datos vectoriales populares como MongoDB, Qdrant y Weaviate, lo que facilita la creación de sistemas de búsqueda bilingües escalables. Para un rendimiento óptimo, implemente un preprocesamiento de texto adecuado para manejar el límite de 8192 tokens de manera eficaz; esto generalmente admite alrededor de 15 a 20 páginas de texto. Si bien el modelo se destaca tanto en contenido en alemán como en inglés, es particularmente eficaz cuando se lo utiliza para tareas de recuperación en varios idiomas donde los idiomas de consulta y documento pueden diferir. Las organizaciones deben considerar la implementación de estrategias de almacenamiento en caché para el contenido al que se accede con frecuencia y utilizar el procesamiento por lotes para la indexación de documentos a gran escala. La integración del modelo con AWS SageMaker proporciona una ruta confiable para la implementación de producción, aunque los equipos deben monitorear el uso de tokens e implementar una limitación de velocidad adecuada para aplicaciones de alto tráfico. Al utilizar el modelo para aplicaciones RAG, considere implementar la detección de idioma para optimizar la construcción de indicaciones en función del idioma de entrada.","jina-embeddings-v2-base-en":"Para implementar Jina Embeddings v2 Base English de manera eficaz, los equipos deben considerar varios aspectos prácticos. El modelo requiere hardware compatible con CUDA para un rendimiento óptimo, aunque su arquitectura eficiente significa que puede ejecutarse en GPU de nivel de consumidor. Está disponible a través de múltiples canales: descarga directa desde Hugging Face, implementación de AWS Marketplace o la API de Jina AI con 1 millón de tokens gratuitos. Para implementaciones de producción, AWS SageMaker en la región us-east-1 ofrece la solución más escalable. El modelo se destaca en el análisis de texto de propósito general, pero puede no ser la mejor opción para terminología científica altamente especializada o jerga específica del dominio sin un ajuste fino. Al procesar documentos largos, considere dividirlos en fragmentos semánticos significativos en lugar de divisiones arbitrarias para mantener la integridad del contexto. Para obtener resultados óptimos, implemente un preprocesamiento de texto adecuado y asegúrese de que los datos de entrada estén limpios y bien formateados.","jina-embeddings-v2-base-es":"Para utilizar este modelo de manera eficaz, las organizaciones deben garantizar el acceso a una infraestructura de GPU compatible con CUDA para lograr un rendimiento óptimo. El modelo se integra perfectamente con las principales bases de datos vectoriales y marcos RAG, incluidos MongoDB, Qdrant, Weaviate y Haystack, lo que lo hace fácilmente implementable en entornos de producción. Se destaca en aplicaciones como búsqueda de documentos bilingües, sistemas de recomendación de contenido y análisis de documentos en varios idiomas. Si bien el modelo muestra una versatilidad impresionante, está particularmente optimizado para escenarios bilingües español-inglés y puede no ser la mejor opción para aplicaciones monolingües o escenarios que involucran otros pares de idiomas. Para obtener resultados óptimos, los textos de entrada deben tener el formato adecuado en español o inglés, aunque el modelo maneja contenido en varios idiomas de manera eficaz. El modelo admite el ajuste fino para aplicaciones específicas del dominio, pero esto debe abordarse considerando cuidadosamente la calidad y distribución de los datos de entrenamiento.","jina-embeddings-v2-base-zh":"El modelo requiere 322 MB de almacenamiento y se puede implementar a través de múltiples canales, incluidos AWS SageMaker (región us-east-1) y la API de Jina AI. Si bien la aceleración de GPU no es obligatoria, puede mejorar significativamente la velocidad de procesamiento para cargas de trabajo de producción. El modelo se destaca en varias aplicaciones, incluido el análisis de documentos, la búsqueda multilingüe y la recuperación de información en varios idiomas, pero los usuarios deben tener en cuenta que está optimizado específicamente para escenarios bilingües chino-inglés. Para obtener resultados óptimos, el texto de entrada debe estar segmentado correctamente y, si bien el modelo puede manejar hasta 8192 tokens, se recomienda dividir los documentos extremadamente largos en fragmentos semánticamente significativos para un mejor rendimiento. El modelo puede no ser adecuado para tareas que requieran el procesamiento en tiempo real de textos muy breves, donde los modelos especializados de menor latencia podrían ser más apropiados.","jina-embeddings-v3":"Para implementar Jina Embeddings v3 de manera eficaz, los equipos deben considerar su caso de uso específico para seleccionar el adaptador de tareas adecuado: retrieval.query y retrieval.passage para aplicaciones de búsqueda, separación para tareas de agrupamiento, clasificación para categorización y coincidencia de texto para similitud semántica. El modelo requiere hardware compatible con CUDA para un rendimiento óptimo, aunque su arquitectura eficiente significa que necesita significativamente menos memoria de GPU que las alternativas más grandes. Para la implementación de producción, la integración de AWS SageMaker proporciona una ruta optimizada hacia la escalabilidad. El modelo se destaca en aplicaciones multilingües, pero puede requerir una evaluación adicional para idiomas con bajos recursos. Si bien admite documentos largos de hasta 8192 tokens, se logra un rendimiento óptimo con la función de fragmentación tardía para textos muy largos. Los equipos deben evitar usar el modelo para tareas que requieran generación en tiempo real o razonamiento complejo: está diseñado para la incrustación y la recuperación, no para la generación de texto o la respuesta directa a preguntas.","jina-reranker-v1-base-en":"El modelo requiere hardware compatible con CUDA para un rendimiento óptimo y es accesible a través de puntos finales de API y opciones de implementación de AWS SageMaker. Si bien puede procesar secuencias extremadamente largas, los usuarios deben considerar la compensación entre la longitud del contexto y el tiempo de procesamiento: la latencia del modelo aumenta notablemente con documentos más largos, de 156 ms para 256 tokens a 7068 ms para 4096 tokens con una consulta de 512 tokens. Para implementaciones de producción, se recomienda implementar una canalización de dos etapas donde la búsqueda vectorial proporcione candidatos iniciales para la reclasificación. El modelo está optimizado específicamente para contenido en inglés y es posible que no funcione de manera óptima en documentos multilingües o con mucho código. Al integrarse con sistemas RAG, los usuarios deben ajustar cuidadosamente la cantidad de documentos enviados para la reclasificación en función de sus requisitos de latencia; 100 a 200 documentos generalmente brindan un buen equilibrio entre calidad y rendimiento.","jina-reranker-v1-tiny-en":"Para implementar este modelo de manera eficaz, las organizaciones deben priorizar los escenarios en los que la velocidad de procesamiento y la eficiencia de los recursos son consideraciones fundamentales. El modelo es particularmente adecuado para implementaciones de computación de borde, aplicaciones móviles y sistemas de búsqueda de alto rendimiento donde los requisitos de latencia son estrictos. Si bien funciona excepcionalmente bien en la mayoría de las tareas de reclasificación, es importante tener en cuenta que para las aplicaciones que requieren el nivel más alto de precisión de clasificación, el modelo base aún puede ser preferible. El modelo requiere una infraestructura de GPU compatible con CUDA para un rendimiento óptimo, aunque su arquitectura eficiente significa que puede ejecutarse de manera eficaz en hardware menos potente que sus contrapartes más grandes. Para la implementación, el modelo se integra perfectamente con las principales bases de datos vectoriales y marcos RAG, y está disponible a través de la API Reranker y AWS SageMaker. Al realizar ajustes para dominios específicos, los usuarios deben equilibrar cuidadosamente la calidad de los datos de entrenamiento con la arquitectura compacta del modelo para mantener sus características de rendimiento.","jina-reranker-v1-turbo-en":"El modelo requiere hardware compatible con CUDA para un rendimiento óptimo y se puede implementar a través de AWS SageMaker o acceder a él a través de puntos finales de API. Para las implementaciones de producción, las organizaciones deben implementar una canalización de dos etapas donde la búsqueda de vectores proporcione candidatos iniciales para la reclasificación. Si bien el modelo admite 8192 tokens, los usuarios deben considerar el impacto de latencia de secuencias más largas: el tiempo de procesamiento aumenta con la longitud del documento. El punto óptimo para la mayoría de las aplicaciones es la reclasificación de 100 a 200 candidatos por consulta, lo que equilibra la calidad y la velocidad. El modelo está optimizado específicamente para contenido en inglés y es posible que no funcione de manera óptima en documentos multilingües. Los requisitos de memoria son significativamente menores que el modelo base, ya que generalmente requieren solo 150 MB de memoria de GPU en comparación con 550 MB, lo que lo hace adecuado para la implementación en instancias más pequeñas y permite un ahorro de costos significativo en entornos de nube.","jina-reranker-v2-base-multilingual":"Para una implementación óptima, el modelo requiere una GPU compatible con CUDA y se puede acceder a él a través de múltiples canales, incluida la API Reranker, los principales marcos RAG como Haystack y LangChain, o implementarse de forma privada a través de mercados en la nube. El modelo se destaca en escenarios que requieren una comprensión precisa de las barreras lingüísticas y los tipos de datos, lo que lo hace ideal para empresas globales que trabajan con contenido multilingüe, documentación de API o repositorios de código. Su amplia ventana de contexto de 524 288 tokens permite el procesamiento de documentos grandes o bases de código completas en una sola pasada. Los equipos deben considerar el uso de este modelo cuando necesiten mejorar la precisión de la búsqueda en todos los idiomas, requieran capacidades de llamada de funciones para sistemas RAG con agentes o quieran mejorar la funcionalidad de búsqueda de código en bases de código multilingües. El modelo es particularmente eficaz cuando se utiliza junto con sistemas de búsqueda vectorial, donde puede mejorar significativamente la clasificación final de los documentos recuperados.","reader-lm-05b":"Para implementar Reader LM 0.5B de manera eficaz, las organizaciones deben asegurarse de que su infraestructura pueda manejar los requisitos CUDA del modelo, aunque su arquitectura eficiente significa que puede ejecutarse en GPU de nivel de consumidor. El modelo funciona mejor con una entrada HTML sin procesar y no requiere prefijos ni instrucciones especiales. Para un rendimiento óptimo, implemente el mecanismo de detección de repetición provisto para evitar posibles bucles de tokens en la generación de salida. Si bien el modelo admite varios idiomas y varias estructuras HTML, está diseñado específicamente para la extracción de contenido y la conversión de Markdown; no debe usarse para tareas como generación de texto, resumen o respuesta directa a preguntas. El modelo está disponible a través de AWS SageMaker para la implementación de producción, y se proporciona un cuaderno de Google Colab para pruebas y experimentación. Los equipos deben tener en cuenta que, si bien el modelo puede manejar documentos extremadamente largos de hasta 256 000 tokens, el procesamiento de entradas tan grandes puede requerir estrategias de administración de memoria adicionales.","reader-lm-15b":"Para implementar Reader LM 1.5B de manera eficaz, las organizaciones deben centrarse en escenarios que impliquen el procesamiento complejo de documentos HTML donde la precisión y la eficiencia son primordiales. El modelo requiere una infraestructura de GPU compatible con CUDA para un rendimiento óptimo, aunque su arquitectura eficiente significa que puede ejecutarse de manera eficaz en hardware más modesto en comparación con alternativas más grandes. Para implementaciones de producción, el modelo está disponible a través de AWS SageMaker y Azure Marketplace, que ofrecen opciones de integración flexibles. Si bien el modelo se destaca en la conversión de HTML a Markdown, es importante tener en cuenta que está optimizado específicamente para esta tarea y puede no ser adecuado para la generación de texto de propósito general u otras tareas de NLP. Al procesar documentos extremadamente largos (que se acercan a los 512 000 tokens), los usuarios deben tener en cuenta que el rendimiento puede degradarse a medida que esto exceda los parámetros de entrenamiento del modelo. Para obtener resultados óptimos, implemente los mecanismos de detección de repeticiones provistos y considere usar la búsqueda contrastiva durante la inferencia para mantener la calidad de salida.",title:"Guía"},image_size:"Tamaño de la imagen de entrada",language:"Soporte de idioma",methods:{"ReaderLM-v2":"Desarrollado sobre la base de Qwen2.5-1.5B-Instruction, el entrenamiento de ReaderLM-v2 implicó un conjunto de datos html-markdown-1m de un millón de documentos HTML, con un promedio de 56.000 tokens cada uno. El proceso de entrenamiento incluyó: 1) preentrenamiento de contexto largo utilizando atención ring-zag y RoPE para expandir el contexto de 32 000 a 256 000 tokens, 2) ajuste fino supervisado con conjuntos de datos refinados, 3) optimización de preferencia directa para alineación de salida y 4) ajuste de refuerzo de reproducción automática. La preparación de los datos siguió un proceso de tres pasos (borrador, refinamiento y crítica) impulsado por Qwen2.5-32B-Instruction, con modelos especializados entrenados para tareas específicas antes de fusionarlos mediante interpolación de parámetros lineales.","jina-clip-v1":"La arquitectura del modelo representa una innovación significativa en el diseño de IA multimodal, ya que combina un codificador de texto Jina BERT v2 adaptado con el codificador de imágenes de vanguardia EVA-02 de la Academia de Inteligencia Artificial de Beijing. El codificador de texto admite secuencias de hasta 12 288 tokens (más de 100 veces más largas que el límite de 77 tokens del CLIP original), mientras que el codificador de imágenes procesa de manera eficiente 16 tokens de parche. El proceso de entrenamiento sigue un novedoso enfoque de tres pasos: primero, alinear pares de imágenes y subtítulos mientras se mantiene la comprensión del texto mediante el entrenamiento de pares de texto intercalados; segundo, incorporar descripciones de texto más largas de las imágenes generadas por IA; y finalmente, usar tripletes de texto negativos duros para mejorar las capacidades de distinción semántica. Esta metodología de entrenamiento única permite que el modelo mantenga un alto rendimiento tanto en subtítulos cortos como en descripciones textuales detalladas, al tiempo que preserva una sólida comprensión visual.","jina-clip-v2":"En esencia, Jina CLIP v2 emplea una sofisticada arquitectura de codificador dual que combina un codificador de texto Jina XLM-RoBERTa (561 millones de parámetros) con un codificador de visión EVA02-L14 (304 millones de parámetros). El codificador de texto procesa contenido en 89 idiomas con una enorme ventana de contexto de 696.320 tokens, mientras que el codificador de visión maneja imágenes de alta resolución de hasta 512x512 píxeles. El modelo presenta un innovador aprendizaje de representación Matryoshka, que permite un ajuste dinámico de la dimensión de incrustación desde 1024 hasta 64 dimensiones, preservando el rendimiento. Esta arquitectura procesa tanto el texto como las imágenes a través de sus respectivos codificadores, proyectándolos en un espacio semántico compartido donde los conceptos similares se alinean independientemente de su modalidad o idioma original.","jina-colbert-v1-en":"El modelo emplea una innovadora arquitectura de interacción tardía que cambia radicalmente el funcionamiento de la recuperación de documentos. En lugar de comparar documentos enteros a la vez, procesa consultas y documentos de forma independiente hasta la etapa final de comparación, utilizando una versión adaptada del enfoque ColBERT. La arquitectura combina dos componentes clave: un codificador de documentos que procesa texto de hasta 8192 tokens (más de 16 veces más que los transformadores estándar) y un codificador de consultas que crea representaciones precisas a nivel de token. Cada token, tanto en la consulta como en el documento, obtiene su propio vector de incrustación de 128 dimensiones, lo que preserva la información semántica de grano fino que se perdería en los modelos de un solo vector. El mecanismo de interacción tardía permite entonces una comparación eficiente token por token entre consultas y documentos, utilizando operaciones de agrupación máxima y suma para calcular las puntuaciones de relevancia finales sin necesidad de costosas comparaciones de todos a todos.","jina-colbert-v2":"El modelo se basa en la arquitectura ColBERT, introduciendo un sofisticado mecanismo de interacción tardía que cambia fundamentalmente la forma en que se relacionan las consultas y los documentos. En esencia, utiliza una estructura principal XLM-RoBERTa modificada con 560M parámetros, mejorada mediante incrustaciones de posición rotatoria y optimizada con atención flash. El proceso de entrenamiento implica dos etapas clave: preentrenamiento inicial con diversos datos débilmente supervisados de varios idiomas, seguido de un ajuste fino con datos de tripletes etiquetados y destilación supervisada. Lo que hace que este enfoque sea único es la implementación del aprendizaje de representación Matryoshka, que permite que el modelo produzca incrustaciones en múltiples dimensiones (128, 96 o 64) a partir de un único proceso de entrenamiento, lo que permite la optimización dinámica del almacenamiento sin reentrenamiento.","jina-embedding-b-en-v1":"El modelo emplea una arquitectura basada en un codificador T5 mejorado con agrupamiento de medias para generar representaciones de longitud fija. Entrenado en el conjunto de datos Linnaeus-Clean cuidadosamente seleccionado, que contiene 385 millones de pares de oraciones de alta calidad filtrados de los 1.600 millones de pares iniciales, el modelo se sometió a un proceso de entrenamiento de dos fases. La primera fase utilizó aprendizaje contrastivo con pérdida de InfoNCE en pares de texto, mientras que la segunda fase incorporó entrenamiento de tripletes para refinar la capacidad del modelo para distinguir entre contenido similar y diferente. Este innovador enfoque de entrenamiento, combinado con un riguroso filtrado de datos que incluye detección de idioma y verificación de consistencia, permite que el modelo capture relaciones semánticas matizadas de manera efectiva.","jina-embeddings-v2-base-code":"El modelo logra su impresionante rendimiento a través de una arquitectura especializada diseñada específicamente para la comprensión del código. En esencia, utiliza una red neuronal basada en transformadores con 161 millones de parámetros, entrenada en diversos conjuntos de datos de lenguajes de programación con énfasis en seis lenguajes principales: Python, JavaScript, Java, PHP, Go y Ruby. Lo que hace que esta arquitectura sea única es su ventana de contexto extendida de 8192 tokens, lo que le permite procesar funciones completas o múltiples archivos a la vez mientras mantiene la comprensión semántica. El modelo genera incrustaciones densas de 768 dimensiones que capturan tanto la estructura sintáctica como el significado semántico del código, lo que le permite comprender las relaciones entre diferentes segmentos de código incluso cuando utilizan diferentes patrones de programación o sintaxis para lograr el mismo objetivo.","jina-embeddings-v2-base-de":"El modelo logra sus impresionantes capacidades bilingües a través de una arquitectura innovadora que procesa textos en alemán e inglés dentro de un espacio de incrustación unificado de 768 dimensiones. En esencia, emplea una red neuronal basada en transformadores con 161 millones de parámetros, cuidadosamente entrenada para comprender las relaciones semánticas en ambos idiomas. Lo que hace que esta arquitectura sea particularmente efectiva es su enfoque de minimización de sesgos, diseñado específicamente para evitar el error común de favorecer las estructuras gramaticales en inglés, un problema identificado en investigaciones recientes con modelos multilingües. La ventana de contexto extendida del modelo de 8192 tokens le permite procesar documentos completos o múltiples páginas de texto en una sola pasada, manteniendo la coherencia semántica en el contenido de formato largo en ambos idiomas.","jina-embeddings-v2-base-en":"La arquitectura del modelo combina una estructura básica BERT Small con un innovador mecanismo ALiBi (Atención con sesgos lineales) bidireccional simétrico, lo que elimina la necesidad de incrustaciones posicionales tradicionales. Esta elección arquitectónica permite al modelo extrapolar mucho más allá de su longitud de entrenamiento de 512 tokens, manejando secuencias de hasta 8192 tokens sin degradación del rendimiento. El proceso de entrenamiento implicó dos fases clave: preentrenamiento inicial en el conjunto de datos C4, seguido de refinamiento en la colección curada de más de 40 conjuntos de datos especializados de Jina AI. Estos diversos datos de entrenamiento, que incluyen ejemplos negativos desafiantes y pares de oraciones variados, garantizan un rendimiento sólido en diferentes dominios y casos de uso. El modelo produce vectores densos de 768 dimensiones que capturan relaciones semánticas matizadas, logradas con unos parámetros relativamente modestos de 137 millones.","jina-embeddings-v2-base-es":"En el corazón de este modelo se encuentra una arquitectura innovadora basada en ALiBi (Atención con sesgos lineales) bidireccional simétrico, un enfoque sofisticado que permite el procesamiento de secuencias de hasta 8192 tokens sin incrustaciones posicionales tradicionales. El modelo utiliza una arquitectura BERT modificada con 161 millones de parámetros, que incorpora unidades lineales controladas (GLU) y técnicas de normalización de capas especializadas. El entrenamiento sigue un proceso de tres etapas: preentrenamiento inicial en un corpus de texto masivo, seguido de un ajuste fino con pares de texto cuidadosamente seleccionados y, finalmente, entrenamiento negativo duro para mejorar la discriminación entre contenido similar pero semánticamente distinto. Este enfoque, combinado con incrustaciones de 768 dimensiones, permite que el modelo capture relaciones semánticas matizadas al tiempo que mantiene la eficiencia computacional.","jina-embeddings-v2-base-zh":"La arquitectura del modelo combina una estructura básica basada en BERT con ALiBi (Atención con sesgos lineales) bidireccional simétrico, lo que permite un procesamiento eficiente de secuencias largas sin la limitación tradicional de 512 tokens. El proceso de entrenamiento sigue un enfoque de tres fases cuidadosamente orquestado: preentrenamiento inicial con datos bilingües de alta calidad, seguido de etapas de ajuste primario y secundario. Esta estrategia de entrenamiento metódica, junto con los 161 millones de parámetros del modelo y la salida de 768 dimensiones, logra una eficiencia notable al tiempo que mantiene un rendimiento equilibrado en ambos idiomas. El mecanismo ALiBi bidireccional simétrico representa una innovación significativa, que permite al modelo manejar documentos de hasta 8192 tokens de longitud, una capacidad que anteriormente estaba limitada a soluciones propietarias.","jina-embeddings-v3":"La arquitectura del modelo representa una innovación significativa en la tecnología de incrustación, construida sobre una base de jina-XLM-RoBERTa con 24 capas y mejorada con adaptadores de adaptación de bajo rango (LoRA) específicos de la tarea. Los adaptadores LoRA son componentes de red neuronal especializados que optimizan el modelo para diferentes tareas como recuperación, clasificación o agrupamiento sin aumentar significativamente el recuento de parámetros: agregan menos del 3% a los parámetros totales. El modelo incorpora aprendizaje de representación Matryoshka (MRL), lo que permite reducir de manera flexible las incrustaciones de 1024 a tan solo 32 dimensiones, al tiempo que se preserva el rendimiento. El entrenamiento implicó un proceso de tres etapas: preentrenamiento inicial en texto multilingüe de 89 idiomas, ajuste fino en textos emparejados para la calidad de incrustación y entrenamiento especializado de adaptadores para la optimización de tareas. El modelo admite longitudes de contexto de hasta 8192 tokens a través de incrustaciones de posición rotatoria (RoPE), con una innovadora técnica de ajuste de frecuencia base que mejora el rendimiento tanto en textos cortos como largos.","jina-reranker-v1-base-en":"El modelo emplea una arquitectura de atención cruzada basada en BERT que difiere fundamentalmente de los enfoques tradicionales basados en incrustaciones. En lugar de comparar incrustaciones de documentos calculadas previamente, realiza interacciones dinámicas a nivel de token entre consultas y documentos, lo que le permite capturar matices contextuales que las métricas de similitud simples pasan por alto. Los 137 millones de parámetros de la arquitectura están cuidadosamente estructurados para permitir una comprensión semántica profunda al tiempo que se mantiene la eficiencia computacional. Una innovación destacada es su capacidad para manejar secuencias de hasta 262.144 tokens (mucho más allá de las limitaciones típicas del modelo), lograda mediante técnicas de optimización sofisticadas que mantienen velocidades de inferencia rápidas a pesar de la ventana de contexto aumentada.","jina-reranker-v1-tiny-en":"El modelo emplea una arquitectura optimizada de cuatro capas basada en JinaBERT con ALiBi (Atención con sesgos lineales) bidireccional simétrico, lo que permite un procesamiento eficiente de secuencias largas. Su desarrollo aprovecha un enfoque avanzado de destilación de conocimientos en el que un modelo docente más grande y de alto rendimiento (jina-reranker-v1-base-en) guía el proceso de entrenamiento, lo que permite que el modelo más pequeño aprenda comportamientos de clasificación óptimos sin requerir datos de entrenamiento extensos del mundo real. Esta innovadora metodología de entrenamiento, combinada con optimizaciones arquitectónicas como capas ocultas reducidas y mecanismos de atención eficientes, permite que el modelo mantenga clasificaciones de alta calidad al tiempo que reduce significativamente los requisitos computacionales. El resultado es un modelo que logra una eficiencia notable sin comprometer su capacidad para comprender relaciones complejas entre documentos.","jina-reranker-v1-turbo-en":"El modelo logra su eficiencia a través de una innovadora arquitectura de seis capas que comprime las sofisticadas capacidades de reclasificación de su contraparte más grande en solo 37,8 millones de parámetros, una reducción drástica de los 137 millones del modelo base. Este diseño optimizado emplea la destilación de conocimiento, donde el modelo base más grande actúa como un maestro, entrenando a la variante turbo para que coincida con su comportamiento mientras usa menos recursos. La arquitectura mantiene el mecanismo de atención cruzada basado en BERT central para interacciones a nivel de token entre consultas y documentos, pero lo optimiza para la velocidad a través de un recuento de capas reducido y una asignación de parámetros eficiente. El modelo admite secuencias de hasta 8192 tokens, lo que permite un análisis integral de documentos al tiempo que mantiene velocidades de inferencia rápidas a través de técnicas de optimización sofisticadas.","jina-reranker-v2-base-multilingual":"El modelo emplea una arquitectura de codificador cruzado mejorada con Flash Attention 2, lo que permite una comparación directa entre consultas y documentos para una evaluación de relevancia más precisa. El modelo, que se entrena mediante un proceso de cuatro etapas, primero establece capacidades en inglés y luego incorpora progresivamente datos multilingües y translingüísticos, antes del refinamiento final con ejemplos negativos duros. Este innovador enfoque de entrenamiento, combinado con la implementación de Flash Attention 2, permite que el modelo procese secuencias de hasta 524 288 tokens manteniendo una velocidad excepcional. La eficiencia de la arquitectura le permite manejar tareas complejas de reclasificación en varios idiomas con un rendimiento seis veces mayor en comparación con su predecesor, al tiempo que garantiza una evaluación de relevancia precisa a través de la interacción directa entre la consulta y el documento.","reader-lm-05b":'El modelo emplea una arquitectura innovadora "superficial pero amplia" optimizada específicamente para operaciones de copia selectiva en lugar de generación creativa de texto. Construido sobre una base de solo decodificador con 24 capas y 896 dimensiones ocultas, el modelo utiliza mecanismos de atención especializados con 14 cabezas de consulta y 2 cabezas de clave-valor para procesar eficientemente las secuencias de entrada. El proceso de entrenamiento implicó dos etapas distintas: primero con HTML más corto y simple (32K tokens) para aprender patrones básicos de conversión, luego con HTML complejo del mundo real (128K tokens) para manejar casos desafiantes. El modelo incorpora búsqueda contrastiva durante el entrenamiento e implementa un mecanismo de detección de repeticiones para evitar problemas de degeneración como bucles de tokens. Un aspecto único de su arquitectura es el mecanismo de atención de anillo en zigzag, que permite que el modelo maneje secuencias extremadamente largas de hasta 256K tokens mientras mantiene un rendimiento estable.',"reader-lm-15b":'El modelo emplea una arquitectura innovadora "superficial pero amplia" que desafía los enfoques de escalamiento tradicionales en el diseño de modelos de lenguaje. En su núcleo hay 28 capas de transformador configuradas con 12 encabezados de consulta y 2 encabezados de clave-valor, lo que crea un equilibrio único que optimiza las operaciones de copia selectiva al tiempo que mantiene una comprensión semántica profunda. La arquitectura presenta un tamaño oculto de 1536 y un tamaño intermedio de 8960, cuidadosamente ajustados para manejar secuencias de hasta 256K tokens. El proceso de entrenamiento implicó dos etapas distintas: primero, se centró en HTML corto y simple con secuencias de tokens de 32K, luego avanzó a HTML largo y duro con tokens de 128K, implementando atención de anillo en zigzag para un procesamiento eficiente. Este enfoque, combinado con búsqueda contrastiva y mecanismos especializados de detección de repeticiones, permite que el modelo evite problemas comunes como la degeneración y los bucles aburridos que suelen afectar a los modelos de lenguaje más pequeños que manejan tareas complejas de procesamiento de documentos.',title:"Métodos"},model_comparison:"Comparación de modelos",model_details:"Detalles del modelo",model_io_graph:"Gráfico de E/S {_number}",model_name:"Nombre",output_dimension:"Dimensión de salida",overview:{"ReaderLM-v2":'ReaderLM-v2 es un modelo de lenguaje de parámetros 1.5B que convierte HTML sin formato en Markdown o JSON, manejando hasta 512K tokens de longitud de entrada/salida combinada con soporte para 29 idiomas. A diferencia de su predecesor que trataba la conversión de HTML a Markdown como una tarea de "copia selectiva", v2 lo aborda como un proceso de traducción, lo que permite un manejo superior de elementos complejos como cercas de código, listas anidadas, tablas y ecuaciones LaTeX. El modelo mantiene un rendimiento consistente en diferentes longitudes de contexto e introduce capacidades de generación directa de HTML a JSON con esquemas predefinidos.',"jina-clip-v1":"Jina CLIP v1 revoluciona la IA multimodal al ser el primer modelo que se destaca por igual en tareas de recuperación de texto a texto y de texto a imagen. A diferencia de los modelos CLIP tradicionales que tienen dificultades con escenarios de solo texto, este modelo logra un rendimiento de vanguardia en todas las combinaciones de recuperación, al tiempo que mantiene un tamaño de parámetro notablemente compacto de 223M. El modelo aborda un desafío crítico de la industria al eliminar la necesidad de modelos separados para el procesamiento de texto e imágenes, lo que reduce la complejidad del sistema y la sobrecarga computacional. Para los equipos que crean sistemas de búsqueda, motores de recomendación o herramientas de análisis de contenido, Jina CLIP v1 ofrece una solución única y eficiente que maneja tanto el contenido de texto como el visual con una precisión excepcional.","jina-clip-v2":"Jina CLIP v2 revoluciona la inteligencia artificial multimodal al reducir la brecha entre la comprensión visual y textual en 89 idiomas. Este modelo resuelve desafíos críticos en el comercio electrónico global, la gestión de contenido y la comunicación intercultural al permitir una correspondencia precisa entre imágenes y textos independientemente de las barreras lingüísticas. Para las empresas que se expanden internacionalmente o que gestionan contenido multilingüe, elimina la necesidad de modelos separados por idioma o de procesos de traducción complejos. El modelo se destaca particularmente en escenarios que requieren una búsqueda visual precisa a través de las fronteras lingüísticas, como el descubrimiento de productos en el mercado global o la gestión de activos digitales multilingües.","jina-colbert-v1-en":"Jina-ColBERT-v1-en revoluciona la búsqueda de texto al resolver un desafío crítico en la recuperación de información: lograr una alta precisión sin sacrificar la eficiencia computacional. A diferencia de los modelos tradicionales que comprimen documentos enteros en vectores únicos, este modelo mantiene una comprensión precisa a nivel de token mientras requiere solo 137 millones de parámetros. Para los equipos que crean aplicaciones de búsqueda, sistemas de recomendación o plataformas de descubrimiento de contenido, Jina-ColBERT-v1-en elimina el tradicional equilibrio entre la calidad de la búsqueda y el rendimiento del sistema. El modelo se destaca particularmente en escenarios donde la comprensión matizada del texto es crucial, como la búsqueda de documentación técnica, la recuperación de artículos académicos o cualquier aplicación donde capturar relaciones semánticas sutiles puede marcar la diferencia entre encontrar la información correcta y perder contenido crítico.","jina-colbert-v2":"Jina-ColBERT-v2 es un innovador modelo de recuperación de información multilingüe que resuelve el desafío crítico de una búsqueda eficiente y de alta calidad en varios idiomas. Como el primer modelo multilingüe similar a ColBERT que genera incrustaciones compactas, aborda la creciente necesidad de soluciones de búsqueda multilingües escalables y rentables en aplicaciones globales. Las organizaciones que trabajan con contenido multilingüe, desde plataformas de comercio electrónico hasta sistemas de gestión de contenido, pueden aprovechar este modelo para proporcionar resultados de búsqueda precisos en 89 idiomas y, al mismo tiempo, reducir significativamente los costos de almacenamiento y computación a través de sus innovadoras capacidades de reducción de dimensiones.","jina-embedding-b-en-v1":"Jina Embedding B v1 es un modelo de incrustación de texto especializado diseñado para transformar texto en inglés en representaciones numéricas de alta dimensión, manteniendo al mismo tiempo el significado semántico. El modelo aborda la necesidad crítica de incrustaciones de texto eficientes y precisas en entornos de producción, especialmente valiosas para organizaciones que requieren un equilibrio entre la eficiencia computacional y la calidad de incrustación. Con sus 110 millones de parámetros que generan incrustaciones de 768 dimensiones, sirve como una solución práctica para equipos que implementan búsqueda semántica, agrupación de documentos o sistemas de recomendación de contenido sin requerir recursos computacionales extensos.","jina-embeddings-v2-base-code":"Jina Embeddings v2 Base Code aborda un desafío crítico en el desarrollo de software moderno: navegar y comprender de manera eficiente bases de código grandes. Para los equipos de desarrollo que tienen dificultades con el descubrimiento y la documentación de código, este modelo transforma la forma en que los desarrolladores interactúan con el código al permitir la búsqueda en lenguaje natural en 30 lenguajes de programación. A diferencia de las herramientas de búsqueda de código tradicionales que se basan en la coincidencia exacta de patrones, este modelo comprende el significado semántico detrás del código, lo que permite a los desarrolladores encontrar fragmentos de código relevantes mediante descripciones en inglés simple. Esta capacidad es particularmente valiosa para los equipos que mantienen bases de código heredadas de gran tamaño, los desarrolladores que se incorporan a nuevos proyectos o las organizaciones que buscan mejorar las prácticas de documentación y reutilización de código.","jina-embeddings-v2-base-de":"Jina Embeddings v2 Base German aborda un desafío crítico en los negocios internacionales: cerrar la brecha lingüística entre los mercados alemán e inglés. Para las empresas alemanas que se expanden a territorios de habla inglesa, donde un tercio de las empresas generan más del 20 % de sus ventas globales, la comprensión bilingüe precisa es esencial. Este modelo transforma la forma en que las organizaciones manejan el contenido en varios idiomas al permitir la comprensión y recuperación de textos sin problemas tanto en alemán como en inglés, lo que lo hace invaluable para las empresas que implementan sistemas de documentación internacionales, plataformas de soporte al cliente o soluciones de gestión de contenido. A diferencia de los enfoques tradicionales basados en la traducción, este modelo asigna directamente significados equivalentes en ambos idiomas al mismo espacio de incrustación, lo que permite operaciones bilingües más precisas y eficientes.","jina-embeddings-v2-base-en":"Jina Embeddings v2 Base English es un innovador modelo de incrustación de texto de código abierto que resuelve el desafío crítico de procesar documentos extensos manteniendo una alta precisión. Las organizaciones que tienen dificultades para analizar documentos legales extensos, artículos de investigación o informes financieros encontrarán este modelo particularmente valioso. Se destaca por manejar documentos de hasta 8192 tokens de longitud (16 veces más largos que los modelos tradicionales) y al mismo tiempo igualar el rendimiento de las soluciones patentadas de OpenAI. Con un tamaño compacto de 0,27 GB y un uso eficiente de los recursos, ofrece una solución accesible para los equipos que buscan implementar un análisis avanzado de documentos sin una sobrecarga computacional excesiva.","jina-embeddings-v2-base-es":"Jina Embeddings v2 Base Spanish es un innovador modelo de incrustación de texto bilingüe que aborda el desafío crítico de la recuperación y el análisis de información entre contenido en español e inglés. A diferencia de los modelos multilingües tradicionales que a menudo muestran un sesgo hacia idiomas específicos, este modelo ofrece un rendimiento verdaderamente equilibrado tanto en español como en inglés, lo que lo hace indispensable para las organizaciones que operan en mercados de habla hispana o que manejan contenido bilingüe. La característica más notable del modelo es su capacidad de generar incrustaciones alineadas geométricamente: cuando los textos en español e inglés expresan el mismo significado, sus representaciones vectoriales se agrupan naturalmente en el espacio de incrustación, lo que permite una búsqueda y un análisis sin problemas entre idiomas.","jina-embeddings-v2-base-zh":"Jina Embeddings v2 Base Chinese es una innovación al ser el primer modelo de código abierto que maneja sin problemas textos en chino e inglés con una longitud de contexto de 8192 tokens sin precedentes. Esta potente herramienta bilingüe aborda un desafío crítico en los negocios globales: la necesidad de un procesamiento preciso y extenso de documentos en contenido chino e inglés. A diferencia de los modelos tradicionales que tienen dificultades para la comprensión interlingüe o requieren modelos separados para cada idioma, este modelo asigna significados equivalentes en ambos idiomas al mismo espacio de incrustación, lo que lo hace invaluable para las organizaciones que se expanden globalmente o que administran contenido multilingüe.","jina-embeddings-v3":"Jina Embeddings v3 es un innovador modelo de incrustación de texto multilingüe que transforma la forma en que las organizaciones manejan la comprensión y recuperación de texto en todos los idiomas. En esencia, resuelve el desafío crítico de mantener un alto rendimiento en varios idiomas y tareas, al mismo tiempo que mantiene los requisitos computacionales manejables. El modelo se destaca particularmente en entornos de producción donde la eficiencia es importante: logra un rendimiento de vanguardia con solo 570 millones de parámetros, lo que lo hace accesible para equipos que no pueden permitirse la sobrecarga computacional de modelos más grandes. Las organizaciones que necesitan crear sistemas de búsqueda escalables y multilingües o analizar contenido a través de barreras lingüísticas encontrarán este modelo especialmente valioso.","jina-reranker-v1-base-en":"Jina Reranker v1 Base English revoluciona el refinamiento de los resultados de búsqueda al abordar una limitación crítica en los sistemas de búsqueda vectorial tradicionales: la incapacidad de capturar relaciones matizadas entre consultas y documentos. Si bien la búsqueda vectorial con similitud de coseno proporciona resultados iniciales rápidos, a menudo omite señales de relevancia sutiles que los usuarios humanos entienden intuitivamente. Este reranker cierra esa brecha al realizar un análisis sofisticado a nivel de token tanto de las consultas como de los documentos, lo que ofrece una notable mejora del 20 % en la precisión de la búsqueda. Para las organizaciones que tienen dificultades con la precisión de la búsqueda o que implementan sistemas RAG, este modelo ofrece una solución poderosa que mejora significativamente la calidad de los resultados sin requerir una revisión completa de la infraestructura de búsqueda existente.","jina-reranker-v1-tiny-en":"Jina Reranker v1 Tiny English representa un gran avance en el refinamiento de búsquedas eficiente, diseñado específicamente para organizaciones que requieren un reranking de alto rendimiento en entornos con recursos limitados. Este modelo aborda el desafío crítico de mantener la calidad de la búsqueda al mismo tiempo que reduce significativamente la sobrecarga computacional y los costos de implementación. Con solo 33 millones de parámetros (una fracción de los tamaños típicos de reranker), ofrece un rendimiento notablemente competitivo a través de técnicas innovadoras de destilación de conocimiento. La característica más sorprendente del modelo es su capacidad de procesar documentos casi cinco veces más rápido que los modelos básicos, al tiempo que mantiene más del 92 % de su precisión, lo que hace que el refinamiento de búsqueda de nivel empresarial sea accesible para aplicaciones donde los recursos computacionales son limitados.","jina-reranker-v1-turbo-en":"Jina Reranker v1 Turbo English aborda un desafío crítico en los sistemas de búsqueda de producción: el equilibrio entre la calidad de los resultados y la eficiencia computacional. Si bien los rerankers tradicionales ofrecen una precisión de búsqueda mejorada, sus demandas computacionales a menudo los hacen poco prácticos para aplicaciones en tiempo real. Este modelo rompe esa barrera al ofrecer el 95 % de la precisión del modelo base mientras procesa documentos tres veces más rápido y utiliza un 75 % menos de memoria. Para las organizaciones que luchan con la latencia de búsqueda o los costos computacionales, este modelo ofrece una solución convincente que mantiene un refinamiento de búsqueda de alta calidad al tiempo que reduce significativamente los requisitos de infraestructura y los costos operativos.","jina-reranker-v2-base-multilingual":"Jina Reranker v2 Base Multilingual es un modelo de codificador cruzado diseñado para mejorar la precisión de búsqueda en diferentes barreras lingüísticas y tipos de datos. Este reranker aborda el desafío crítico de la recuperación precisa de información en entornos multilingües, especialmente valioso para empresas globales que necesitan refinar los resultados de búsqueda en diferentes idiomas y tipos de contenido. Con soporte para más de 100 idiomas y capacidades únicas en llamadas de funciones y búsqueda de código, sirve como una solución unificada para equipos que requieren un refinamiento preciso de la búsqueda en contenido internacional, documentación de API y bases de código multilingües. El diseño compacto de 278M de parámetros del modelo lo hace particularmente atractivo para las organizaciones que buscan equilibrar el alto rendimiento con la eficiencia de los recursos.","reader-lm-05b":"Reader LM 0.5B es un modelo de lenguaje especializado diseñado para resolver el complejo desafío de convertir documentos HTML en texto Markdown limpio y estructurado. Este modelo aborda una necesidad crítica en los procesos de procesamiento de datos modernos: transformar de manera eficiente contenido web desordenado en un formato ideal para LLM y sistemas de documentación. A diferencia de los modelos de lenguaje de propósito general que requieren recursos computacionales masivos, Reader LM 0.5B logra un procesamiento HTML de nivel profesional con solo 494 millones de parámetros, lo que lo hace accesible para equipos con recursos computacionales limitados. Las organizaciones que se ocupan del procesamiento de contenido web, la automatización de la documentación o la creación de aplicaciones impulsadas por LLM encontrarán este modelo particularmente valioso para agilizar sus flujos de trabajo de preparación de contenido.","reader-lm-15b":"Reader LM 1.5B representa un gran avance en el procesamiento eficiente de documentos, ya que aborda el desafío crítico de convertir contenido web complejo en formatos limpios y estructurados. Este modelo de lenguaje especializado aborda un problema fundamental en los procesos de IA modernos: la necesidad de procesar y limpiar de manera eficiente el contenido HTML para tareas posteriores sin depender de sistemas frágiles basados en reglas o modelos de lenguaje grandes que consumen muchos recursos. Lo que hace que este modelo sea realmente notable es su capacidad de superar a modelos 50 veces más grandes y, al mismo tiempo, mantener una huella de parámetros sorprendentemente compacta de 1.54B. Las organizaciones que se ocupan del procesamiento de contenido web a gran escala, la automatización de la documentación o los sistemas de gestión de contenido encontrarán este modelo particularmente valioso por su capacidad de manejar documentos extremadamente largos y, al mismo tiempo, ofrecer una precisión superior en la conversión de HTML a Markdown.",title:"Descripción general"},parameter_size:"Parámetros",performance:{"ReaderLM-v2":"En pruebas comparativas exhaustivas, ReaderLM-v2 supera a modelos más grandes como Qwen2.5-32B-Instruct y Gemini2-flash-expr en tareas de conversión de HTML a Markdown. Para la extracción de contenido principal, logra un ROUGE-L de 0,84, un Jaro-Winkler de 0,82 y una distancia de Levenshtein significativamente menor (0,22) en comparación con los competidores. En tareas de conversión de HTML a JSON, mantiene un rendimiento competitivo con puntajes F1 de 0,81 y una tasa de aprobación del 98 %. El modelo procesa a 67 tokens/s de entrada y 36 tokens/s de salida en una GPU T4, con problemas de degeneración significativamente reducidos a través del entrenamiento de pérdida de contraste.","jina-clip-v1":"Jina CLIP v1 demuestra mejoras notables con respecto al CLIP original de OpenAI en todos los puntos de referencia. En la recuperación de solo texto, logra un aumento del rendimiento del 165% con una puntuación de 0,429 en comparación con el 0,162 de CLIP. Para las tareas relacionadas con imágenes, muestra mejoras consistentes: 2% mejor en la recuperación de texto a imagen (0,899), 6% en la recuperación de imagen a texto (0,803) y 12% en la recuperación de imagen a imagen (0,916). El modelo se destaca particularmente en las tareas de clasificación visual de disparo cero, categorizando imágenes con éxito sin entrenamiento previo en dominios específicos. Cuando se evalúa en puntos de referencia estándar como MTEB para recuperación de texto, CIFAR-100 para tareas de imagen y Flickr8k/30k y MSCOCO Captions para rendimiento intermodal, supera constantemente a los modelos monomodales especializados, al tiempo que mantiene un rendimiento competitivo en tareas intermodales.","jina-clip-v2":"El modelo alcanza un rendimiento de vanguardia con una precisión del 98,0 % en las tareas de recuperación de imágenes a texto de Flickr30k, superando tanto a su predecesor como a NLLB-CLIP-SigLIP. En escenarios multilingües, demuestra una mejora de hasta un 4 % sobre NLLB-CLIP-SigLIP en las tareas de recuperación de imágenes en varios idiomas, a pesar de tener menos parámetros que su mayor competidor. El modelo mantiene un rendimiento sólido incluso cuando se comprimen las incrustaciones: al reducir las dimensiones en un 75 %, aún conserva más del 99 % del rendimiento en tareas de texto, imágenes y multimodales. En los completos puntos de referencia multilingües de MTEB, alcanza un 69,86 % en la recuperación y un 67,77 % en las tareas de similitud semántica, con un rendimiento competitivo con los modelos de incrustación de texto especializados.","jina-colbert-v1-en":"Jina-ColBERT-v1-en demuestra mejoras notables con respecto a los modelos de referencia en varios puntos de referencia. En la colección de conjuntos de datos BEIR, logra un rendimiento superior en múltiples categorías: 49,4 % en Arguana (frente al 46,5 % de ColBERTv2), 79,5 % en FEVER (frente al 78,8 %) y 75,0 % en TREC-COVID (frente al 72,6 %). Lo más impresionante es que muestra una mejora drástica en el punto de referencia LoCo para la comprensión del contexto largo, con una puntuación del 83,7 % en comparación con el 74,3 % de ColBERTv2. El modelo se destaca particularmente en escenarios que requieren una comprensión semántica detallada, superando a los modelos de incrustación tradicionales al tiempo que mantiene la eficiencia computacional a través de su innovador enfoque de interacción tardía. Estas mejoras se logran manteniendo el recuento de parámetros del modelo en unos modestos 137 millones, lo que lo hace potente y práctico para implementaciones de producción.","jina-colbert-v2":"En pruebas del mundo real, Jina-ColBERT-v2 demuestra capacidades excepcionales en múltiples puntos de referencia. Logra una mejora del 6,5 % sobre el ColBERT-v2 original en tareas de inglés, con una puntuación media de 0,521 en 14 puntos de referencia BEIR. Más impresionante aún, supera a los métodos de recuperación tradicionales basados en BM25 en todos los idiomas probados en los puntos de referencia MIRACL, mostrando una fortaleza particular en escenarios multilingües. El modelo mantiene este alto rendimiento incluso cuando se utilizan dimensiones de incrustación reducidas: la reducción de 128 a 64 dimensiones da como resultado una disminución del rendimiento de solo el 1,5 % y reduce a la mitad los requisitos de almacenamiento. Esto se traduce en un importante ahorro de costes en producción: por ejemplo, almacenar 100 millones de documentos con vectores de 64 dimensiones cuesta 659,62 USD al mes en AWS, en comparación con los 1319,24 USD para 128 dimensiones.","jina-embedding-b-en-v1":"En evaluaciones del mundo real, Jina Embedding B v1 demuestra capacidades impresionantes, particularmente en tareas de similitud textual semántica. El modelo logra un rendimiento de vanguardia en STS12 con una puntuación de 0,751, superando a modelos establecidos como all-mpnet-base-v2 y all-minilm-l6-v2. Muestra un sólido rendimiento en varios puntos de referencia, al tiempo que mantiene tiempos de inferencia eficientes. Sin embargo, los usuarios deben tener en cuenta que el modelo está optimizado específicamente para contenido en idioma inglés y es posible que no funcione de manera óptima en tareas multilingües o específicas del código. Desde entonces, el modelo ha sido reemplazado por jina-embeddings-v2-base-en y jina-embeddings-v3, que ofrecen un rendimiento mejorado en una gama más amplia de casos de uso.","jina-embeddings-v2-base-code":"En pruebas reales, Jina Embeddings v2 Base Code demuestra capacidades excepcionales, liderando el campo en nueve de quince puntos de referencia cruciales de CodeNetSearch. En comparación con modelos de gigantes de la industria como Microsoft y Salesforce, logra un rendimiento superior al mismo tiempo que mantiene una huella más eficiente. El modelo se destaca particularmente en la comprensión de código entre lenguajes, al hacer coincidir con éxito fragmentos de código funcionalmente equivalentes en diferentes lenguajes de programación. Su ventana de contexto de 8192 tokens resulta particularmente valiosa para funciones grandes y archivos de código complejos, superando significativamente a los modelos tradicionales que generalmente manejan solo unos pocos cientos de tokens. La eficiencia del modelo es evidente en su tamaño compacto de 307 MB (sin cuantificar), lo que permite una inferencia rápida al tiempo que mantiene una alta precisión en tareas de similitud y búsqueda de código.","jina-embeddings-v2-base-de":"En pruebas reales, Jina Embeddings v2 Base German demuestra una eficiencia y precisión excepcionales, en particular en tareas de recuperación de texto en varios idiomas. El modelo supera al modelo base E5 de Microsoft a pesar de tener menos de un tercio de su tamaño, e iguala el rendimiento del modelo grande E5 a pesar de ser siete veces más pequeño. En los principales puntos de referencia, incluidos WikiCLIR para la recuperación de texto de inglés a alemán, STS17 y STS22 para la comprensión bidireccional del lenguaje, y BUCC para la alineación precisa de texto bilingüe, el modelo demuestra constantemente capacidades superiores. Su tamaño compacto de 322 MB permite la implementación en hardware estándar a la vez que mantiene un rendimiento de vanguardia, lo que lo hace particularmente eficiente para entornos de producción donde los recursos computacionales son una consideración.","jina-embeddings-v2-base-en":"En pruebas reales, Jina Embeddings v2 Base English demuestra capacidades excepcionales en múltiples puntos de referencia. Supera a text-embedding-ada-002 de OpenAI en varias métricas clave: clasificación (73,45 % frente a 70,93 %), reclasificación (85,38 % frente a 84,89 %), recuperación (56,98 % frente a 56,32 %) y resumen (31,6 % frente a 30,8 %). Estas cifras se traducen en ventajas prácticas en tareas como la clasificación de documentos, donde el modelo muestra una capacidad superior para categorizar textos complejos, y en aplicaciones de búsqueda, donde comprende mejor las consultas de los usuarios y encuentra documentos relevantes. Sin embargo, los usuarios deben tener en cuenta que el rendimiento puede variar cuando se trabaja con contenido altamente especializado y específico del dominio que no está representado en los datos de entrenamiento.","jina-embeddings-v2-base-es":"En evaluaciones comparativas exhaustivas, el modelo demuestra capacidades excepcionales, en particular en tareas de recuperación en varios idiomas, donde supera a modelos multilingües significativamente más grandes como E5 y BGE-M3 a pesar de tener solo un 15-30 % de su tamaño. El modelo logra un rendimiento superior en tareas de recuperación y agrupamiento, mostrando una fortaleza particular en la búsqueda de coincidencias de contenido semánticamente equivalente en varios idiomas. Cuando se prueba en el benchmark MTEB, exhibe un rendimiento sólido en varias tareas, incluidas la clasificación, el agrupamiento y la similitud semántica. La ventana de contexto extendida de 8192 tokens resulta especialmente valiosa para el procesamiento de documentos largos, mostrando un rendimiento constante incluso con documentos que abarcan varias páginas, una capacidad de la que carecen la mayoría de los modelos de la competencia.","jina-embeddings-v2-base-zh":"En las pruebas comparativas de la tabla de clasificación de MTEB chino (C-MTEB), el modelo demuestra un rendimiento excepcional entre los modelos de menos de 0,5 GB, y se destaca especialmente en las tareas en chino. Supera significativamente al text-embedding-ada-002 de OpenAI en aplicaciones específicas en chino, al tiempo que mantiene un rendimiento competitivo en las tareas en inglés. Una mejora notable en esta versión es la distribución refinada de la puntuación de similitud, que aborda los problemas de inflación de la puntuación presentes en la versión preliminar. El modelo ahora proporciona puntuaciones de similitud más claras y lógicas, lo que garantiza una representación más precisa de las relaciones semánticas entre textos. Esta mejora es particularmente evidente en las pruebas comparativas, donde el modelo muestra una discriminación superior entre contenido relacionado y no relacionado en ambos idiomas.","jina-embeddings-v3":"El modelo demuestra una relación eficiencia-rendimiento excepcional en pruebas del mundo real, superando tanto a las alternativas de código abierto como a las soluciones propietarias de OpenAI y Cohere en tareas en inglés, a la vez que se destaca en escenarios multilingües. Lo más sorprendente es que logra mejores resultados que e5-mistral-7b-instruct, que tiene 12 veces más parámetros, lo que resalta su notable eficiencia. En las evaluaciones de referencia de MTEB, logra una puntuación promedio de 65,52 en todas las tareas, con un desempeño particularmente sólido en Precisión de clasificación (82,58) y Similitud de oraciones (85,80). El modelo mantiene un rendimiento constante en todos los idiomas, con una puntuación de 64,44 en tareas multilingües. Al utilizar MRL para la reducción de dimensiones, mantiene un rendimiento sólido incluso en dimensiones más bajas; por ejemplo, mantiene el 92 % de su rendimiento de recuperación en 64 dimensiones en comparación con las 1024 dimensiones completas.","jina-reranker-v1-base-en":"En los benchmarks integrales, el modelo demuestra mejoras excepcionales en las métricas clave, logrando un aumento del 8% en la tasa de aciertos y un aumento del 33% en la clasificación recíproca media en comparación con la búsqueda de vectores de referencia. En el benchmark BEIR, logra una puntuación media de 0,5588, superando a otros rerankers de BGE (0,5032), BCE (0,4969) y Cohere (0,5141). Particularmente impresionante es su desempeño en el benchmark LoCo, donde obtiene una puntuación media de 0,873, significativamente por delante de los competidores en la comprensión de la coherencia local y la clasificación consciente del contexto. El modelo muestra una fortaleza particular en la evaluación de contenido técnico, logrando puntuaciones de 0,996 en tareas qasper_abstract y 0,962 en el análisis de informes gubernamentales, aunque muestra un rendimiento relativamente inferior (0,466) en tareas de resumen de reuniones.","jina-reranker-v1-tiny-en":"En evaluaciones comparativas exhaustivas, el modelo demuestra capacidades excepcionales que desafían el equilibrio convencional entre tamaño y rendimiento. En la prueba comparativa BEIR, logra una puntuación NDCG-10 de 48,54, lo que le permite conservar el 92,5 % del rendimiento del modelo base a pesar de tener solo una cuarta parte de su tamaño. Aún más impresionante es que, en las pruebas comparativas RAG de LlamaIndex, mantiene una tasa de aciertos del 83,16 %, casi igualando a los modelos más grandes y procesando documentos significativamente más rápido. El modelo se destaca particularmente en rendimiento, ya que procesa documentos casi cinco veces más rápido que el modelo base y utiliza un 13 % menos de memoria que incluso la variante turbo. Estas métricas se traducen en un rendimiento en el mundo real que rivaliza o supera a modelos mucho más grandes como mxbai-rerank-base-v1 (184 millones de parámetros) y bge-reranker-base (278 millones de parámetros).","jina-reranker-v1-turbo-en":"En las pruebas comparativas exhaustivas, la variante turbo demuestra una eficiencia notable sin sacrificar la precisión de manera significativa. En la prueba comparativa BEIR, logra una puntuación NDCC-10 de 49,60, manteniendo el 95 % del rendimiento del modelo base (52,45) y superando a muchos competidores más grandes como bge-reranker-base (47,89, 278M parámetros). En las aplicaciones RAG, mantiene una impresionante tasa de aciertos del 83,51 % y una tasa de recuperación de datos de 0,6498, lo que demuestra una fortaleza particular en las tareas de recuperación prácticas. Las mejoras de velocidad del modelo son aún más sorprendentes: procesa documentos tres veces más rápido que el modelo base, con un escalamiento del rendimiento casi lineal con un recuento de parámetros reducido. Sin embargo, los usuarios deben notar un rendimiento ligeramente inferior en las tareas de clasificación extremadamente matizadas, donde el recuento completo de parámetros de los modelos más grandes proporciona ventajas marginales.","jina-reranker-v2-base-multilingual":"En evaluaciones del mundo real, el modelo demuestra capacidades excepcionales en diversos puntos de referencia. Alcanza un rendimiento de vanguardia en la clasificación de AirBench para sistemas RAG y muestra sólidos resultados en tareas multilingües, incluido el conjunto de datos MKQA que cubre 26 idiomas. El modelo se destaca particularmente en tareas de datos estructurados, logrando altos puntajes de recuperación tanto en llamadas de funciones (punto de referencia ToolBench) como en coincidencias de esquemas SQL (punto de referencia NSText2SQL). Lo más impresionante es que ofrece estos resultados mientras procesa documentos 15 veces más rápido que modelos comparables como bge-reranker-v2-m3, lo que lo hace práctico para aplicaciones en tiempo real. Sin embargo, los usuarios deben tener en cuenta que el rendimiento óptimo requiere una GPU compatible con CUDA para la inferencia.","reader-lm-05b":"En pruebas reales, Reader LM 0.5B demuestra impresionantes relaciones eficiencia-rendimiento en múltiples métricas. El modelo alcanza una puntuación ROUGE-L de 0,56, lo que indica una fuerte conservación del contenido, y mantiene una baja tasa de error de token de 0,34, lo que muestra una alucinación mínima. En evaluaciones cualitativas en 22 fuentes HTML diversas, incluidos artículos de noticias, publicaciones de blogs y páginas de comercio electrónico en varios idiomas, muestra una fortaleza particular en la conservación de la estructura y el uso de la sintaxis de Markdown. El modelo se destaca en el manejo de páginas web modernas complejas donde los CSS y scripts en línea pueden expandirse a cientos de miles de tokens, un escenario en el que los enfoques tradicionales basados en reglas a menudo fallan. Sin embargo, es importante señalar que, si bien el modelo funciona excepcionalmente bien en tareas sencillas de conversión de HTML a Markdown, puede requerir procesamiento adicional para páginas altamente dinámicas o con mucho JavaScript.","reader-lm-15b":"En evaluaciones comparativas exhaustivas, Reader LM 1.5B demuestra capacidades excepcionales que desafían los estándares de la industria. El modelo logra una puntuación ROUGE-L de 0,72 y una tasa de error de token de 0,19, superando significativamente a modelos más grandes como GPT-4 (0,43 ROUGE-L, 0,50 TER) y Gemini-1.5-Pro (0,42 ROUGE-L, 0,48 TER) en tareas de conversión de HTML a Markdown. Su rendimiento se destaca particularmente en evaluaciones cualitativas en cuatro dimensiones clave: extracción de encabezado, extracción de contenido principal, conservación de estructura enriquecida y uso de sintaxis de Markdown. El modelo mantiene constantemente una alta precisión en diversos tipos de documentos, desde artículos de noticias y publicaciones de blogs hasta páginas de destino y publicaciones en foros, en varios idiomas, incluidos inglés, alemán, japonés y chino. Este rendimiento se logra al procesar documentos de hasta 256 000 tokens de longitud, lo que elimina la necesidad de costosas operaciones de fragmentación que generalmente se requieren con modelos más grandes.",title:"Actuación"},performance_metrics:"Métricas de rendimiento",publications:"Publicaciones",tags:"Etiquetas",token_length:"Longitud del token de entrada",usage_requirements:"Uso y requisitos",using_model:"Disponible a través de"},select_model:"Seleccione un modelo de la lista para ver los detalles",sort:{direction:{asc:"Ascendente",desc:"Descendiendo",name:"Dirección"},label:"Clasificar",name:"Nombre",parameter_size:"Tamaño",release_date:"Fecha"},title:"{_modelName} - Búsqueda de modelos de la Fundación",warnings:{deprecated:"Este modelo está obsoleto por los modelos más nuevos."}},ae={back_to_newsroom:"Volver a la sala de prensa",categories:"Categorías",copy_link:"Copia el enlace a esta sección.",in_this_article:"En este articulo",learn_more:"Aprende más",news_not_found:"Artículo no encontrado",redirect_to_news:"Redirigiendo a la sala de redacción en 5 segundos..."},oe={academic:"Académico",academic_research:"Publicaciones Académicas",author:"Filtrar por autor",description:"Lea las últimas noticias y actualizaciones de Jina AI.",description1:"Creando innovaciones en IA, palabra a palabra.",engineering_group:"Grupo de Ingeniería",engineering_group_date:"31 mayo, 2021",minutes_read:"minutos de lectura",most_recent_articles:"Artículos más recientes",news_description:'Para Jina 2.0, escuchamos a la comunidad. Verdaderamente, profundamente escuchado. "¿Cuáles son tus puntos débiles?" preguntamos, esperando ansiosamente comentarios valiosos',news_title:"Buscar todas las cosas: estamos organizando un concurso MEME para Jina 2.0",photos:"Fotos",product:"Filtrar por producto",search:"Buscar por título",tech_blog:"Blog de tecnología",title:"Sala de prensa",top_stories:"Historias destacadas"},ne='🎉 ¡Nuestro primer libro, "Búsqueda neuronal: del prototipo a la producción con Jina" sale oficialmente hoy!',ie={description:"Una oportunidad exclusiva para obtener una visión privilegiada de Jina AI.",engage:"Recomendamos encarecidamente un diálogo interactivo durante todo el día. El intercambio de pensamientos y perspectivas es invaluable para nosotros. Las colaboraciones potenciales derivadas de estas discusiones podrían contribuir significativamente a un futuro más integrado e innovador.",engage_title:"Participa con nosotros",experience:"Hemos organizado un recorrido inmersivo de tres horas para nuestros huéspedes, disponible en alemán, inglés, francés, español, chino y ruso. El recorrido cubre una mirada en profundidad a nuestros avances en IA multimodal, nuestra perspectiva sobre el panorama de la IA, seguido de un examen detallado de proyectos específicos. Concluiremos con una discusión grupal para facilitar el intercambio de ideas y puntos de vista. Una opción de almuerzo también está disponible bajo petición.",experience_title:"El viaje de un iniciado",group_size:"Número estimado de visitantes",impact:"Comprenda cómo nuestras contribuciones a la comunidad de código abierto y nuestro trabajo en tecnología de IA multimodal están estableciendo a Jina AI como un jugador influyente en la innovación de IA. Nuestro objetivo es desempeñar un papel importante en los procesos de toma de decisiones, asegurando que el avance de la tecnología de IA beneficie a todos.",impact_title:"Impacto e influencia",introduction:"Jina AI se complace en abrir nuestras puertas a entidades y organizaciones estimadas interesadas en el progreso y el futuro de la Inteligencia Artificial. Extendemos esta oportunidad exclusiva para aquellos en la política, las ONG, las OSFL y los sectores de inversión para obtener una visión interna de nuestras operaciones y visiones aquí en nuestra sede de Berlín.",motivation_min_length_v1:"Proporcione una motivación más detallada.",motivation_placeholder_v2:"Compartir tus motivaciones nos ayudará a mejorar tu experiencia.",motivation_to_attend_v2:"¿Por qué te interesa nuestra jornada de puertas abiertas?",one_hour:"1 hora",organization:"Organización",organization_website:"Sitio web de la organización",organization_website_placeholder:"URL de la página de inicio de su organización o del perfil de LinkedIn",preferred_date:"Fecha preferida",preferred_language:"Idioma preferido del tour",preferred_products:"¿En qué productos estás interesado?",subtitle:"Un vistazo al futuro de la IA multimodal",title:"Dia abierto",tutor_subtitle:"Un recorrido de tres horas cuidadosamente seleccionado que lo acerca al corazón del trabajo innovador de Jina AI en tecnología de IA multimodal.",tutor_title:"Una inmersión profunda exclusiva en",vision:"Únase a nosotros para obtener una descripción general completa del panorama de la IA tal como lo vemos. Nuestra discusión se centrará en el potencial de los modelos de lenguaje grande, la IA multimodal y el impacto de la tecnología de código abierto en la configuración del futuro de la innovación global.",vision_title:"Nuestra visión para el futuro"},se={answer1:"Ofrecemos tours en alemán, inglés, francés, español, chino y ruso.",answer2:"El recorrido suele durar aproximadamente tres horas.",answer3:"El almuerzo es opcional y se puede organizar bajo petición.",answer4:"Nuestra Jornada de Puertas Abiertas está diseñada principalmente para grupos profesionales, como políticos, ONG, OSFL e inversores. Sin embargo, ocasionalmente hacemos excepciones según el perfil de la persona.",answer5:"Podemos acomodar una variedad de tamaños de grupo. Indique el tamaño de su grupo en el formulario de registro y le confirmaremos los detalles.",answer6:"Hay una sección en el formulario de registro donde puede especificar sus áreas de interés o cualquier solicitud especial. Haremos todo lo posible para adaptar el recorrido de acuerdo a sus necesidades.",answer7:"En este momento, solo ofrecemos tours en nuestra sede de Berlín ubicada en Kreuzberg. Nuestras oficinas de Beijing y Shenzhen no están abiertas actualmente para visitas.",question1:"¿Qué idiomas ofrecen para el tour?",question2:"¿Cuál es la duración del recorrido?",question3:"¿Se proporciona almuerzo?",question4:"¿Pueden registrarse personas para la Jornada de Puertas Abiertas?",question5:"¿De cuántas personas puede estar formado un grupo para la Jornada de Puertas Abiertas?",question6:"¿Cómo puedo especificar áreas de interés para el tour?",question7:"¿Hay recorridos disponibles en sus oficinas de Beijing o Shenzhen?"},re={description:"Un marco de servicio nativo de la nube de código abierto de grandes modelos multimodales"},te={commercial_licence:{chip_label:"Exclusivo para Pequeñas Empresas",company_size_note:"Exclusivo para empresas con menos de 50 empleados o $500K de ingresos",cta_button:"Empezar",download_title:"Descargar Licencia Comercial",feature_api_desc:"Pruebe antes de comprar",feature_api_title:"Acceso gratuito a pruebas de API",feature_consulting:"Dos horas de asesoramiento con nuestros expertos en modelos",feature_consulting_desc:"Dos (2) horas de servicios de consultoría técnica por Período de Licencia.",feature_future_support:"Acceso a futuros modelos CC BY-NC sin permiso",feature_future_support_desc:"Cualquier modelo nuevo publicado por el Licenciante bajo CC-BY-NC-4.0 durante el Período de la Licencia.",feature_models:"Uso comercial ilimitado de nuestros modelos CC BY-NC",feature_models_desc:"Utilizar los Modelos para fines comerciales, incluido el uso interno o la incorporación en aplicaciones orientadas al cliente.",price_amount:"$1,000",price_period:"/ cuarto",read_the_terms:"Revisar los términos de la licencia",read_the_terms_btn:"Términos",read_the_terms_desc:"Revise los derechos y limitaciones de la licencia comercial antes de comprar",subtitle:"Todos los modelos que necesitas para una mejor búsqueda",test_before_purchase:"Pruébelo antes de comprarlo",test_before_purchase_desc:"Obtenga 1 millón de tokens API gratuitos o use nuestro modelo Hugging Face para validar el rendimiento",title:"Licencia de equipo",try_api:"Pruebe primero la API"},free_hour_consult:"Consulta gratuita de 1 hora",free_hour_consult_description:"Una hora de consulta gratuita con nuestros equipos de productos e ingeniería para analizar las mejores prácticas para su caso de uso.",full_commercial:"Uso comercial sin restricciones",full_commercial_description:"Puede utilizar la API para fines comerciales sin ninguna restricción.",higher_limit:"Límite de tasa mucho más alto",higher_limit_description:"Obtén hasta 1000 RPM para r.jina.ai y 100 RPM para s.jina.ai; más detalles en la sección de límite de velocidad.",key_manager:"Gestión básica de claves",key_manager_description:"Administre múltiples claves API en una cuenta, realice un seguimiento del historial de uso y recargue tokens.",no_commercial:"Sólo para uso no comercial (CC-BY-NC)",no_commercial_description:"Puede utilizar la API únicamente con fines no comerciales. Para uso comercial, recargue su clave API.",on_prem:"Con licencia comercial para uso local",on_prem_explain:"Compre una licencia comercial para utilizar nuestros modelos en sus instalaciones.",premium_key:"Clave premium con límites de velocidad mucho más altos",premium_key_description:"Obtenga límites de velocidad mucho más altos y acceso a funciones premium, consulte la tabla de límites de velocidad para obtener detalles.",premium_key_manager:"Gestión avanzada de claves",premium_key_manager_description:"Funciones básicas y avanzadas como recordatorio automático, revocación y transferencia de token.",priority_support:"Soporte técnico prioritario",priority_support_description:"Respuesta garantizada por correo electrónico sobre problemas técnicos e incidentes dentro de las 24 horas.",secured_by_stripe:"Pago seguro a través de Stripe",standard_key:"Llave estándar",standard_key_description:"Acceso a todos los productos API de Jina Search Foundation con un límite de tarifa estándar.",via_api:"Con la API de Jina Search Foundation",via_api_explain:"La forma más sencilla de acceder a todos nuestros productos. Recarga tokens a medida que avanzas."},de="Energizado por",ce="Imprimir",le={archived:"Archivado",cloud_native:"Nativo de la nube",core:"Centro",data_structure:"Estructura de datos",embedding_serving:"Incrustar servicio",embedding_tuning:"Ajuste de incrustación",graduated:"Graduado",incubating:"incubando",kubernetes:"Kubernetes",large_size_model:"Modelo de gran tamaño",linux_foundation:"Fundación Linux",llm1:"LLMOps",mid_size_model:"Modelo de tamaño mediano",model_serving:"Servicio modelo",model_tuning:"Ajuste del modelo",observability:"Observabilidad",orchestration:"Orquestación",prompt_serving:"Servicio rápido",prompt_tuning:"Sintonización rápida",rag1:"TRAPO",sandbox:"Salvadera",small_size_model:"Modelo de tamaño pequeño",vector_database:"Base de datos de vectores",vector_store:"Tienda de vectores"},ue={description:"Herramienta principal para ingeniería rápida",image_model:"Modelos de imagen",intro:"Herramienta principal para ingeniería rápida",intro1:"La principal herramienta para una ingeniería rápida",optimized:"Su tarea es ser mi compañero de lluvia de ideas y proporcionar ideas y sugerencias creativas para un tema o problema determinado. Su respuesta debe incluir ideas originales, únicas y relevantes que puedan ayudar a resolver el problema o explorar más a fondo el tema de una manera interesante. Tenga en cuenta que su respuesta también debe tener en cuenta los requisitos o limitaciones específicos de la tarea.",optimized_title:"Mensaje optimizado",original:"Tu papel es ser mi compañero de intercambio de ideas.",original_title:"Aviso original",text_model:"Modelos de texto"},me={features:[{description:"Cambie fácilmente entre generación de contenido y optimización rápida, lleve la calidad de su contenido al siguiente nivel.",name:"Asistente",title:"Dosis diaria de productividad."},{description:"¿No sabes cómo escribir una instrucción eficaz? Simplemente introduzca su idea, con un clic, obtenga una mejor instrucción.",name:"Optimización inmediata",title:"Mejores insumos, mejores resultados"},{description:"Comprenda la vibra de cada modelo de IA comparando su resultado del mismo mensaje.",name:"Comparar modelos",title:"Comparación de modelos lado a lado."},{description:"Quizás la forma más sencilla de implementar sus indicaciones como API para la integración.",name:"Implementar indicaciones",title:"Sin operaciones, solo despliegue."},{description:"Personalice sus propios agentes LLM e inicie una simulación de múltiples agentes. Vea cómo colaboran o compiten en un entorno virtual para alcanzar la meta.",name:"Multiagente",title:"Explora cómo colaboran los agentes"}],get_started:"Comience con PromptPerfect"},pe={api_key:"Clave API recargada",free_key:"Clave API gratuita",generation:"¡Tu clave API está lista!",generation_caption:"¡Su clave API se generó a las {_purchasedTime} y está lista para usar!",success:"Gracias por su compra!",success_caption:"Tu pedido se completó a las {_purchasedTime}. ¡Tu clave API se recargó y está lista para usar!"},ge="Comprar ahora",ve={batch_explain:"Esta API admite operaciones por lotes, lo que permite hasta 512 documentos por solicitud, cada uno de los cuales contiene hasta 8192 tokens. El uso inteligente de las operaciones por lotes puede reducir significativamente la cantidad de solicitudes y mejorar el rendimiento.",classifier:"Entrenar un clasificador usando ejemplos etiquetados",classifier_few_shot:"Clasifique las entradas utilizando un clasificador de pocos disparos entrenado",classifier_few_shot_token_counting:"Los tokens se cuentan como: input_tokens",classifier_latency:"El tiempo de respuesta varía según el tamaño de la entrada.",classifier_token_counting:"Los tokens se cuentan como: input_tokens × num_iters",classifier_zero_shot:"Clasificar las entradas utilizando la clasificación de disparo cero",classifier_zero_shot_token_counting:"Los tokens se cuentan como: input_tokens + label_tokens",deepsearch:"Razonar, buscar e iterar para encontrar la mejor respuesta.",depends:"depende del tamaño de entrada",description:"Descripción",embeddings:"Convertir texto/imágenes en vectores de longitud fija",endpoint:"Punto final de API",explain:"Los límites de velocidad se controlan de dos maneras: <b>RPM</b> (solicitudes por minuto) y <b>TPM</b> (tokens por minuto). Los límites se aplican por clave de API o IP y se pueden alcanzar en función del umbral que se alcance primero (RPM o TPM). Tenga en cuenta que, cuando se proporciona la clave de API en la solicitud, los límites de velocidad se controlan por clave, no por dirección IP.",gjinaai:"Fundamentar una declaración con conocimiento web",input_token_counting:"Cuente la cantidad de tokens en la solicitud de entrada.",latency:"Latencia media",no_token_counting:"El token no se cuenta como uso.",output_token_counting:"Cuente la cantidad de tokens en la respuesta de salida.",premium_rate:"Con potencial para límites de tarifas más altos",product:"Producto",requestType:"Solicitud Permitida",reranker:"Clasificar documentos por consulta",rjinaai:"Convertir URL a texto compatible con LLM",sjinaai:"Busque en la web y convierta los resultados en texto compatible con LLM",tbd:"Por determinar",title:"Límite de velocidad",tokenCounting:"Recuento de uso de tokens",tokenizer:"Tokenizar y segmentar textos largos",total_token_counting:"Cuente el número total de tokens en todo el proceso.",understanding:"Entender el límite de velocidad",understanding_description:"Los límites de velocidad son la cantidad máxima de solicitudes que se pueden realizar a una API en un minuto por dirección IP/clave API (RPM). Obtenga más información sobre los límites de velocidad para cada producto y nivel a continuación.",wAPIkey:"con clave API",wPremium:"con clave API Premium",woAPIkey:"Sin clave API"},be={decision:"Decisión",description:"Las mejores herramientas de toma de decisiones de IA",intro:"Ver las dos caras de la moneda, tomar decisiones racionales"},_e={beta:"Experimental",better_input:"Mejore la calidad de la entrada desde el principio",better_input_description:"¿Tiene problemas con la salida de su agente o del sistema RAG? Puede deberse a una mala calidad de entrada.",check_price_table:"Consulte la tabla de precios",copy:"Copiar",demo:{advanced_parameter_explain:"Parámetros específicos que solo se utilizan para {_product}.",advanced_parameters:"Específico",advanced_usage:"Uso avanzado",ask_llm:"Pregunte a LLM sin y con base de búsqueda",ask_llm_directly:"Pregúntele a LLM directamente",ask_llm_with_search_grounding:"Pregúntele a LLM con base de búsqueda",ask_question:"Haz una pregunta",ask_question_hint:"Ingrese una pregunta y combínela con el contenido obtenido para que LLM genere una respuesta.",basic_usage:"Uso básico",basic_usage1:"Leer una URL",basic_usage2:"Buscar una consulta",basic_usage3:"Toma de tierra",common_parameter_explain:"Parámetros comunes que se pueden utilizar para {_product1}, {_product2} y {_product3}.",common_parameters:"Común",copy:"Copiar",fetch:"Obtener contenido",get_response:"Obtener una respuesta",grounding_result_false:"Esta afirmación es falsa.",grounding_result_true:"Esta afirmación es verdadera.",headers:{auth_token:"Agregue clave API para un límite de tasa más alto",auth_token_explain:"Ingrese su clave API de Jina para acceder a un límite de tasa más alto. Para obtener la información más reciente sobre el límite de tarifas, consulte la siguiente tabla.",auto:"Auto",auto_explain:"Selecciona automáticamente el motor óptimo para la URL.",base:"Resolución de redireccionamiento",base_explain:"Seleccione si desea resolver la URL de destino final después de seguir todas las redirecciones. Habilite esta opción para seguir la cadena de redireccionamiento completa.",browser:"Por defecto",browser_explain:"El motor más compatible que ofrece un buen equilibrio entre calidad y velocidad.",browser_locale:"Configuración regional del navegador",browser_locale_explain:"Controla la configuración regional del navegador para mostrar la página. Muchos sitios web ofrecen contenido diferente según la configuración regional.",custom_script:"Ejecutar previamente JavaScript personalizado",custom_script_explain:"Ejecuta código JavaScript de preprocesamiento y acepta una cadena de código en línea o un punto final de URL de script remoto",deepdive:"Análisis profundo de fuentes",deepdive_explain:"Busca más fuentes y lee documentos completos para verificar los datos en profundidad. Es un poco más lento, pero más preciso y tiene más referencias.",default:"Por defecto",default_explain:"La canalización predeterminada optimizada para la mayoría de los sitios web y la entrada LLM.",direct:"La velocidad es lo primero",direct_explain:"El motor más rápido, pero puede tener dificultades con algunos sitios web que utilizan mucho JavaScript.",engine:"Leer motor",engine_explain:"Seleccione el motor que se utilizará para analizar el contenido de la URL indicada. Esto afecta la calidad, la velocidad y la compatibilidad del resultado.",file:"Archivo PDF/HTML local",file_explain:"Utilice Reader en sus archivos PDF y HTML locales cargándolos. Solo se admiten archivos PDF y HTML.",html:"HTML",html_explain:"Devuelve documentElement.outerHTML.",image_caption:"Captura de imagen",image_caption_explain:"Subtitula todas las imágenes en la URL especificada, agregando 'Imagen [idx]: [caption]' como etiqueta alternativa para aquellas que no tienen una. Esto permite que los LLM posteriores interactúen con las imágenes en actividades como razonar y resumir.",images_summary:"Reúna todas las imágenes al final",images_summary_all:"Resumen (Todas las imágenes)",images_summary_all_explain:"Se incluye un resumen de las imágenes recopiladas, sin filtrar duplicados.",images_summary_explain:'Se creará una sección de "Imágenes" al final. Esto brinda a los LLM posteriores una descripción general de todos los elementos visuales de la página, lo que puede mejorar el razonamiento.',images_summary_true:"Resumen (filtrado)",images_summary_true_explain:"Se incluye un resumen de las imágenes recopiladas, pero se filtran las imágenes duplicadas.",instruction_explain:"Extraer información por instrucción",invalid_json:"Esquema JSON no válido",json_response:"Respuesta JSON",json_response_explain:"La respuesta estará en formato JSON y contendrá la URL, el título, el contenido y la marca de tiempo (si está disponible). En el modo de búsqueda, devuelve una lista de cinco entradas, cada una de las cuales sigue la estructura JSON descrita.",json_schema_explain:"Extracción de HTML a JSON con esquema JSON",links_summary:"Reúna todos los enlaces al final",links_summary_all:"Resumen (Todos los enlaces)",links_summary_all_explain:"Se incluye un resumen de los enlaces recopilados, sin filtrar los duplicados.",links_summary_explain:'Al final se creará una sección de "Botones y enlaces". Esto ayuda a los LLM posteriores o agentes web a navegar por la página o realizar más acciones.',links_summary_no:"Sin resumen (predeterminado)",links_summary_no_explain:"No se creará ninguna sección de resumen al final.",links_summary_true:"Resumen (filtrado)",links_summary_true_explain:"Se incluye un resumen de los enlaces recopilados, pero se filtran los enlaces duplicados.",markdown:"Reducción",markdown_explain:"Devuelve el markdown directamente desde el HTML, omitiendo el filtrado de legibilidad.",mode:"Modo de lectura o búsqueda",mode_explain:"El modo de lectura sirve para acceder al contenido de una URL, mientras que el modo de búsqueda le permite buscar una consulta en la web, aplicando el modo de lectura a cada URL de resultado de búsqueda.",no_cache:"Omitir el caché",no_cache_explain:"Nuestro servidor API almacena en caché los contenidos del modo Lectura y Búsqueda durante un cierto período de tiempo. Para omitir este caché, establezca este encabezado en verdadero.",no_gfm:"Desactivado",no_gfm_explain:"Funciones de GFM (Github Flavored Markdown) deshabilitadas.",no_gfm_table:"Sin mesa GFM",no_gfm_table_explain:"Excluir la tabla GFM pero conservar los elementos HTML de la tabla en respuesta.",opt_out_gfm:"Markdown con sabor a Github",opt_out_gfm_explain:"Funciones de inclusión/exclusión voluntaria de GFM (Github Flavored Markdown).",pageshot:"Captura de página",pageshot_explain:"Devuelve la URL de la imagen de la captura de pantalla de la página completa (con el máximo esfuerzo).",post_with_url:"Usar el método POST",post_with_url_explain:"Utilice POST en lugar del método GET con una URL pasada en el cuerpo. Útil para crear SPA con enrutamiento basado en hash.",proxy_server:"Utilice un servidor proxy",proxy_server_explain:"Nuestro servidor API puede utilizar su proxy para acceder a las URL, lo cual resulta útil para páginas a las que solo se puede acceder a través de servidores proxy específicos.",references:"Referencias",references_explain:"Lista separada por comas de referencias proporcionadas por el usuario (URL)",remove_all_images:"Eliminar todas las imágenes",remove_all_images_explain:"Eliminar todas las imágenes de la respuesta.",remove_selector:"Selector excluido",remove_selector_explain:"Proporciona una lista de selectores CSS para eliminar los elementos especificados de la página. Resulta útil cuando se quieren excluir partes específicas de la página, como encabezados, pies de página, etc.",respond_with:"Utilice ReaderLM-v2",respond_with_explain:"Utiliza ReaderLM-v2 para la conversión de HTML a Markdown y ofrece resultados de alta calidad para sitios web con estructuras y contenidos complejos. ¡Cuesta el triple de tokens!",result_count:"Límite de resultados",result_count_explain:"El número de resultados de búsqueda a devolver.",return_format:"Formato del contenido",return_format_explain:"Puede controlar el nivel de detalle de la respuesta para evitar el filtrado excesivo. La canalización predeterminada está optimizada para la mayoría de los sitios web y las entradas de LLM.",screenshot:"Captura de pantalla",screenshot_explain:"Devuelve la URL de la imagen de la primera pantalla.",search_engine:"Motor de búsqueda",search_engine_explain:"Seleccione el motor que se utilizará para la búsqueda. Afecta la calidad, la velocidad y la compatibilidad del resultado.",set_cookie:"Cookie de reenvío",set_cookie_explain:"Nuestro servidor API puede reenviar su configuración de cookies personalizada al acceder a la URL, lo cual es útil para páginas que requieren autenticación adicional. Tenga en cuenta que las solicitudes con cookies no se almacenarán en caché.",site_selector:"Búsqueda en el sitio",site_selector_explain:"Devuelve los resultados de búsqueda solo del sitio web o dominio especificado. Por defecto busca en toda la web.",stream_mode:"Modo de transmisión",stream_mode_explain:"El modo de transmisión es beneficioso para páginas de destino grandes, ya que permite más tiempo para que la página se represente por completo. Si el modo estándar genera contenido incompleto, considere usar el modo Stream.",target_selector:"Selector de objetivos",target_selector_explain:"Proporciona una lista de selectores CSS para centrarse en partes más específicas de la página. Resulta útil cuando el contenido deseado no se muestra con la configuración predeterminada.",text:"Texto",text_explain:"Devuelve documento.body.innerText.",token_budget:"Presupuesto de tokens",token_budget_explain:"Limita la cantidad máxima de tokens que se pueden usar para esta solicitud. Si se excede este límite, la solicitud fallará.",viewport:"Configuración de la ventana gráfica",viewport_explain:"Configurar las dimensiones de la ventana gráfica del navegador para una representación responsiva",vlm:"VLM",vlm_explain:"Ideal para páginas cortas con contenido multimedia enriquecido y diseños complejos.",wait_for_selector:"Esperar al selector",wait_for_selector_explain:"Proporciona una lista de selectores CSS para esperar a que aparezcan elementos específicos antes de volver. Resulta útil cuando el contenido deseado no se muestra con la configuración predeterminada.",with_gfm:"Activado",with_gfm_explain:"Funciones de GFM (Github Flavored Markdown) habilitadas.",with_iframe:"Habilitar la extracción de iframe",with_iframe_explain:"Extrae y procesa contenido de todos los iframes incrustados dentro del árbol DOM",with_shadow_dom:"Habilitar la extracción de Shadow DOM",with_shadow_dom_explain:"Recorre y extrae contenido de todas las raíces Shadow DOM en el documento",x_timeout:"Se acabó el tiempo",x_timeout_explain:"Tiempo máximo de espera para que se cargue la página web. Tenga en cuenta que NO es el tiempo total de toda la solicitud de principio a fin."},how_to_stream:"Para procesar el contenido a medida que esté disponible, configure el encabezado de la solicitud en modo de transmisión. Esto minimiza el tiempo hasta que se recibe el primer byte. Ejemplo en curl:",how_to_use1:"Agregue https://r.jina.ai/ a cualquier URL en su código o herramienta donde se espera acceso LLM. Esto devolverá el contenido principal de la página en un texto limpio y compatible con LLM.",how_to_use2:"Agregue https://s.jina.ai/ a su consulta. Esto llamará al motor de búsqueda y arrojará los 5 primeros resultados con sus URL y contenidos, cada uno en texto limpio y compatible con LLM.",how_to_use3:"Agregue https://g.jina.ai/ a su declaración. Esto llamará al motor de juicio y devolverá un porcentaje de veracidad, un valor booleano que indica si la declaración es verdadera o falsa, un resumen de la razón y una lista de referencias.",key_required:"Se requiere una clave API para utilizar este punto final",learn_more:"Aprende más",open:"Abrir en una nueva pestaña",params_classification:"Parámetros",raw_html:"HTML sin formato",reader_output:"Salida del lector",reader_response:"La respuesta del lector",reader_search_hint:"Si utiliza esta URL en el código, no olvide codificar la URL.",reader_url:"URL del lector",reader_url_hint:"Haga clic a continuación para obtener el contenido a través de nuestra Reader API",requires_post_method:"Esta función requiere el método POST. Al cargar el archivo local, el método POST se activará automáticamente.",search_params:"Parámetros de búsqueda/encabezados",search_query_rewrite:"Tenga en cuenta que, a diferencia de la demostración que se muestra arriba, en la práctica no busca la pregunta original en la web para fundamentarse. Lo que la gente suele hacer es reescribir la pregunta original o utilizar preguntas de múltiples saltos. Leen los resultados recuperados y luego generan consultas adicionales para recopilar más información según sea necesario antes de llegar a una respuesta final.",select_mode:"Seleccionar modo",show_read_demo:"Vea cómo Reader lee una URL",show_search_demo:"Vea cómo Reader busca en la web",slow_warning:"Esto puede tardar hasta 30 segundos y cuesta hasta 300.000 tokens por solicitud.",standard_usage:"Uso estándar",stream_mode:"Modo de transmisión",stream_mode_explain:"El modo de transmisión es útil cuando la página de destino es grande para representar. Si encuentra que el modo estándar le proporciona contenido incompleto, pruebe el modo de transmisión.",stream_mode_explain1:"El modo Streaming es útil cuando descubre que el modo estándar proporciona un resultado incompleto. Esto se debe a que el modo de transmisión esperará un poco más hasta que la página se represente por completo. Utilice el encabezado de aceptación para alternar el modo de transmisión:",tagline:"Pruebe la demostración",try_demo:"Manifestación",use_headers:"El comportamiento de la API Reader se puede controlar con encabezados de solicitud. Aquí hay una lista completa de encabezados compatibles.",waiting_for_reader:"Esperando primero el resultado de Reader API...",warn_grounding_message:"Este proceso puede tardar hasta 30 segundos y consumir hasta 300 000 tokens por solicitud de conexión a tierra. Algunos navegadores pueden finalizar la solicitud debido a la larga latencia, por lo que recomendamos copiar el código y ejecutarlo desde su terminal.",warn_grounding_title:"Alta latencia y uso de tokens",your_query:"Ingresa tu consulta",your_query_hint:"Escriba una pregunta que requiera la información más reciente o conocimiento mundial.",your_statement:"Su declaración de verificación de hechos",your_url:"Introduce tu URL",your_url_hint:"Haga clic a continuación para obtener el código fuente de la página directamente"},description:"Lea las URL y busque en la web para obtener una base más sólida para su LLM.",dont_panic_api_key_is_free:"¡No entrar en pánico! ¡Cada nueva clave API contiene un millón de tokens gratis!",faq_v1:{answer1:"La API Reader es gratuita y no requiere una clave API. Simplemente anteponga 'https://r.jina.ai/' a su URL.",answer10:"No, la API Reader solo puede procesar contenido de URL de acceso público.",answer11:"Si solicita la misma URL en un plazo de 5 minutos, la API de Reader devolverá el contenido almacenado en caché.",answer12:"Lamentablemente no.",answer13:"Sí, puede utilizar la compatibilidad con PDF nativo del Reader (https://r.jina.ai/https://arxiv.org/pdf/2310.19923v4) o utilizar la versión HTML de arXiv (https:// r.jina.ai/https://arxiv.org/html/2310.19923v4)",answer14:"Reader subtitula todas las imágenes en la URL especificada y agrega `Imagen [idx]: [caption]` como etiqueta alt (si inicialmente carecen de una). Esto permite a los LLM posteriores interactuar con las imágenes para razonar, resumir, etc.",answer15:"La API Reader está diseñada para ser altamente escalable. Se escala automáticamente en función del tráfico en tiempo real y las solicitudes de concurrencia máxima ahora son de alrededor de 4000. Lo mantenemos activamente como uno de los productos principales de Jina AI. Así que siéntete libre de usarlo en producción.",answer16:"Encuentre la información más reciente sobre el límite de tarifas en la siguiente tabla. Tenga en cuenta que estamos trabajando activamente para mejorar el límite de velocidad y el rendimiento de Reader API; la tabla se actualizará en consecuencia.",answer17:"Reader-LM es un nuevo modelo de lenguaje pequeño (SLM) diseñado para la extracción y limpieza de datos de la web abierta. Convierte HTML sin formato y con ruido en Markdown limpio, inspirándose en Jina Reader. Con un enfoque en la rentabilidad y el tamaño pequeño del modelo, Reader-LM es práctico y potente. Actualmente está disponible en los mercados de AWS, Azure y GCP. Si tiene requisitos específicos, comuníquese con nosotros a sales AT jina.ai.",answer2:"La API Reader utiliza un proxy para recuperar cualquier URL y representar su contenido en un navegador para extraer contenido principal de alta calidad.",answer3:"Sí, la API Reader es de código abierto y está disponible en el repositorio GitHub de Jina AI.",answer4:"La API Reader generalmente procesa las URL y devuelve el contenido en 2 segundos, aunque las páginas complejas o dinámicas pueden requerir más tiempo.",answer5:"El scraping puede ser complicado y poco confiable, particularmente con páginas complejas o dinámicas. Reader API proporciona una salida optimizada y confiable de texto limpio y listo para LLM.",answer6:"La API Reader devuelve contenido en el idioma original de la URL. No proporciona servicios de traducción.",answer7:"Si tiene problemas de bloqueo, comuníquese con nuestro equipo de soporte para obtener ayuda y resolución.",answer8:"Si bien está diseñada principalmente para páginas web, la API Reader puede extraer contenido de archivos PDF vistos en formato HTML en sitios web como arXiv, pero no está optimizada para la extracción general de PDF.",answer9:"Actualmente, Reader API no procesa contenido multimedia, pero futuras mejoras incluirán subtítulos de imágenes y resúmenes de videos.",question1:"¿Cuáles son los costos asociados con el uso de Reader API?",question10:"¿Es posible utilizar la API de Reader en archivos HTML locales?",question11:"¿Reader API almacena en caché el contenido?",question12:"¿Puedo usar Reader API para acceder al contenido tras un inicio de sesión?",question13:"¿Puedo utilizar la API de Reader para acceder a PDF en arXiv?",question14:"¿Cómo funciona el título de imagen en Reader?",question15:"¿Cuál es la escalabilidad del Reader? ¿Puedo usarlo en producción?",question16:"¿Cuál es el límite de velocidad de la API Reader?",question17:"¿Qué es Reader-LM? ¿Cómo puedo utilizarlo?",question2:"¿Cómo funciona la API Reader?",question3:"¿La API Reader es de código abierto?",question4:"¿Cuál es la latencia típica de la API Reader?",question5:"¿Por qué debería utilizar Reader API en lugar de raspar la página yo mismo?",question6:"¿La API Reader admite varios idiomas?",question7:"¿Qué debo hacer si un sitio web bloquea la API de Reader?",question8:"¿Puede la API Reader extraer contenido de archivos PDF?",question9:"¿Puede la API Reader procesar contenido multimedia de páginas web?",title:"Preguntas comunes relacionadas con los lectores"},fast:"Rápido",fast_stream:"Transmisión de datos inmediata",fast_stream_description:"¿Necesita datos rápidamente? Nuestra API Reader puede transmitir datos para minimizar la latencia.",free:"Siempre libre",free_description:"¡La API del lector es gratuita! No requiere tarjeta de crédito ni secreto de API. No consumirá su cuota de tokens.",is_free:"¿La mejor parte? ¡Es gratis!",is_free_description:"Reader API está disponible de forma gratuita y ofrece límites de tarifas y precios flexibles. Construido sobre una infraestructura escalable, ofrece alta accesibilidad, simultaneidad y confiabilidad. Nos esforzamos por ser su solución de conexión a tierra preferida para sus LLM.",lm_v2_description:"ReaderLM-v2 es un modelo de lenguaje de parámetros 1500 millones especializado en la conversión de HTML a Markdown y la extracción de HTML a JSON. Admite documentos de hasta 512 000 tokens en 29 idiomas y ofrece un 20 % más de precisión en comparación con su predecesor.",lm_v2_title:"ReaderLM v2: un pequeño modelo de lenguaje para convertir HTML a Markdown y JSON",open:"Abrir en una pestaña nueva",original_pdf:"PDF original",rate_limit:"Límite de tarifa",read_grounding_release_note:"Leer la nota de lanzamiento",reader_also_read_images:"Las imágenes de la página web se subtitulan automáticamente utilizando un modelo de lenguaje de visión en el lector y se formatean como etiquetas alternativas de imagen en la salida. Esto le brinda a su LLM posterior suficientes sugerencias para incorporar esas imágenes en sus procesos de razonamiento y resumen. Esto significa que puede hacer preguntas sobre las imágenes, seleccionar imágenes específicas o incluso reenviar sus URL a un VLM más potente para un análisis más profundo.",reader_description:"Convierte una URL en una entrada compatible con LLM, simplemente agregando <code>r.jina.ai</code> al frente.",reader_do_grounding:"Lector para verificación de datos",reader_do_grounding_explain:"El nuevo punto de conexión ofrece una experiencia de verificación de hechos de principio a fin casi en tiempo real. Toma una afirmación determinada, la fundamenta utilizando resultados de búsqueda web en tiempo real y devuelve una puntuación de veracidad y las referencias exactas utilizadas. Puede fundamentar fácilmente las afirmaciones para reducir las alucinaciones de LLM o mejorar la integridad del contenido escrito por humanos.",reader_do_pdf_explain:"Sí, Reader admite de forma nativa la lectura de PDF. Es compatible con la mayoría de los archivos PDF, incluidos aquellos con muchas imágenes, ¡y es ultrarrápido! Combinado con un LLM, puede crear fácilmente un ChatPDF o una IA de análisis de documentos en poco tiempo.",reader_do_search:"Lector para bases de búsqueda.",reader_do_search_explain:"Los LLM tienen un límite de conocimientos, lo que significa que no pueden acceder a los conocimientos mundiales más recientes. Esto conduce a problemas como desinformación, respuestas obsoletas, alucinaciones y otras cuestiones objetivas. La conexión a tierra es absolutamente esencial para las aplicaciones GenAI. Reader le permite basar su LLM con la información más reciente de la web. Simplemente anteponga https://s.jina.ai/ a su consulta y Reader buscará en la web y devolverá los cinco resultados principales con sus URL y contenidos, cada uno en texto limpio y compatible con LLM. De esta manera, siempre podrá mantener actualizado su LLM, mejorar su factibilidad y reducir las alucinaciones.",reader_reads_images:"¡El lector también lee imágenes!",reader_reads_pdf:"¡El lector también lee archivos PDF!",reader_result:"Resultado del lector",table:{td_1_0:"Leer una URL y devolver su contenido, útil para comprobar la conexión a tierra",td_1_1:"20 rpm",td_1_2:"200 rpm",td_1_3:"Basado en los tokens de salida",td_1_4:"3 segundos",td_1_5:"3 segundos",td_2_0:"La búsqueda en la web arroja los 5 primeros resultados, lo que resulta útil para la base de búsqueda",td_2_1:"5 RPM",td_2_2:"40 rpm",td_2_3:"Basado en los tokens de salida para los 5 resultados de búsqueda",td_2_4:"10 segundos",td_2_5:"10 segundos",th0:"Punto final",th1:"Descripción",th2:"Límite de velocidad sin clave API",th3:"Límite de tasa con clave API",th4:"Esquema de conteo de tokens",th5:"Latencia media",th6:"Latencia media"},title:"API de lector",usage:"Uso",usage_details_false:"Mostrar solo usos básicos",usage_details_null:"Mostrar usos básicos y avanzados",usage_details_true:"Mostrar solo usos avanzados",want_higher_rate_limit:"¿Quiere un límite de velocidad más alto, hasta 1000 RPM? ¡Podemos apoyarte!",what_is1:"¿Qué es el lector?",what_is_answer_long:"Introducir información web en los LLM es un paso importante para la puesta a tierra, pero puede ser un desafío. El método más simple es raspar la página web y alimentar el HTML sin formato. Sin embargo, el scraping puede ser complejo y a menudo bloqueado, y el HTML sin formato está lleno de elementos extraños como marcas y scripts. Reader API aborda estos problemas extrayendo el contenido principal de una URL y convirtiéndolo en texto limpio y compatible con LLM, lo que garantiza una entrada de alta calidad para su agente y sus sistemas RAG.",what_is_desc:"Un proxy que accede a cualquier URL y transforma el contenido principal en texto sin formato optimizado para LLM."},fe={confirm_message:"A su clave API le quedan {_leftTokens} tokens. Enviar el texto completo de los artículos {_numArticles} a la API de Reranker, utilizando el modelo {_selectedReranker} para descubrir artículos relacionados para la página actual, reducirá significativamente el recuento de tokens de su clave API {_APIKey}. Quieres proceder?",confirm_title:"Advertencia: uso elevado de tokens",out_of_quota:"Esta clave API se ha quedado sin tokens. Recargue su cuenta o utilice una clave API diferente.",recommend:"Consigue el top 5",recommended_articles:"Los 5 mejores artículos similares"},qe={benchmark:{description0:"LlamaIndex evaluó varias combinaciones de incrustaciones y reordenadores para RAG y realizó un estudio de replicación que midió el rango recíproco medio. Los hallazgos destacan la mejora significativa de la calidad de búsqueda de Jina Reranker, un beneficio que es independiente de las incrustaciones específicas utilizadas.",description1:"BIER (Benchmarking IR) evalúa la efectividad de la recuperación de un modelo, incluida la relevancia y NDCG. Una puntuación BIER más alta se correlaciona con coincidencias y clasificaciones de resultados de búsqueda más precisas.",description2:"A través del punto de referencia LoCo, medimos la comprensión de un modelo sobre la coherencia y el contexto local, junto con la clasificación específica de la consulta. Una puntuación más alta de LoCo refleja una mejor capacidad para identificar y priorizar información relevante.",description3:"El MTEB (Parámetro de referencia de incrustación de texto multilingüe), en general, prueba las capacidades de un modelo en la incrustación de texto, incluida la agrupación, clasificación, recuperación y otras métricas. Sin embargo, para nuestra comparación, solo utilizamos las tareas de Reranking del MTEB.",title:"Punto de referencia",title0:"LlamaIndex",title1:"BEIR",title2:"Locomotora",title3:"MTEB"},benchmark_description:"A modo de comparación, incluimos otros tres rerankers líderes de BGE (BAAI), BCE (Netease Youdao) y Cohere en el índice de referencia. Como lo muestran los resultados a continuación, Jina Reranker tiene el puntaje promedio más alto en todas las categorías relevantes para el reranking, lo que lo convierte en un claro líder entre sus pares.",benchmark_title:"Punto de referencia de rendimiento",choose_turbo:"Obtenga una aceleración de hasta 5 veces con reranker-turbo",choose_turbo_description:"También ofrecemos dos nuevos modelos de reranker de código abierto: jina-reranker-v1-turbo-en y jina-reranker-v1-tiny-en; este último tiene solo 30 millones de parámetros y cuatro capas. Estos dos nuevos reordenadores disfrutan de una velocidad de inferencia 5 veces más rápida que el modelo base con un costo de calidad muy pequeño. Son perfectos para aplicaciones que requieren reclasificación en tiempo real. Lea el punto de referencia a continuación.",customize_urself:"¡Cámbialo y verás cómo cambia la respuesta!",customize_urself_pl:"¡Cámbialos y observa cómo cambia la respuesta!",description:"Recuperador neuronal de clase mundial para maximizar la relevancia de la búsqueda.",description_rich:"Maximice la relevancia de la búsqueda y la precisión de RAG con nuestra API de reranking de vanguardia.",example_input_document:"Ejemplos de documentos candidatos para clasificar",example_input_query:"Consulta de ejemplo",faq_v1:{answer1:"El precio de la API Reranker está alineado con nuestra estructura de precios de API integrada. Comienza con 1 millón de tokens gratis por cada nueva clave API. Más allá de los tokens gratuitos, hay diferentes paquetes disponibles para su compra. Para obtener más detalles, visite nuestra sección de precios.",answer10:"Sí, nuestros servicios están disponibles en los mercados de AWS, Azure y GCP. Si tiene requisitos específicos, comuníquese con nosotros a sales AT jina.ai.",answer11:"Si está interesado en un reranker ajustado y adaptado a datos de dominio específicos, comuníquese con nuestro equipo de ventas. Nuestro equipo responderá a su consulta con prontitud.",answer3:"<code>jina-reranker-v2-base-multilingual</code> se destaca por su compatibilidad con varios idiomas, superando a <code>bge-reranker-v2-m3</code> y ofreciendo un rendimiento 15 veces más rápido que <code>jina-reranker-v1-base-en</code>. También admite tareas de agente y recuperación de código. <code>jina-colbert-v2</code> mejora a <code>ColBERTv2</code>, ya que ofrece un rendimiento de recuperación un 6,5 % mejor y agrega compatibilidad con varios idiomas para 89 idiomas. Cuenta con tamaños de incrustación controlados por el usuario para lograr una eficiencia y precisión óptimas.",answer4:"Sí, tanto <code>jina-reranker-v2-base-multilingual</code> como <code>jina-colbert-v2</code> son de código abierto y están disponibles bajo la licencia CC-BY-NC 4.0. Puedes usar, compartir y adaptar libremente los modelos para fines no comerciales.",answer5:"Sí, tanto <code>jina-reranker-v2-base-multilingual</code> como <code>jina-colbert-v2</code> admiten más de 100 idiomas, incluidos inglés, chino y otros idiomas globales importantes. Están optimizados para tareas multilingües y superan a los modelos anteriores.",answer6:"La longitud máxima del token de consulta es 512. No hay límite de token para los documentos.",answer7:"Puede reclasificar hasta 2048 documentos por consulta.",answer8:"No existe un concepto de tamaño de lote a diferencia de nuestra API de incrustación. Puede enviar solo una tupla de documento de consulta por solicitud, pero la tupla puede incluir hasta 2048 documentos candidatos.",answer9:"La latencia varía de 100 milisegundos a 7 segundos, dependiendo en gran medida de la longitud de los documentos y de la consulta. Por ejemplo, reclasificar 100 documentos de 256 tokens cada uno con una consulta de 64 tokens lleva unos 150 milisegundos. Aumentar la longitud del documento a 4096 tokens aumenta el tiempo a 3,5 segundos. Si la longitud de la consulta aumenta a 512 tokens, el tiempo aumenta aún más a 7 segundos.",question1:"¿Cuánto cuesta la API de Reranker?",question10:"¿Es posible alojar sus puntos finales de forma privada en AWS, Azure o GCP?",question11:"¿Ofrecen un reranker ajustado en datos específicos del dominio?",question3:"¿Cuál es la diferencia entre los dos rerankers?",question4:"¿Los Jina Rerankers son de código abierto?",question5:"¿Los rerankers admiten varios idiomas?",question6:"¿Cuál es la extensión máxima para consultas y documentos?",question7:"¿Cuál es la cantidad máxima de documentos que puedo reclasificar por consulta?",question8:"¿Cuál es el tamaño del lote y cuántas tuplas de documentos de consulta puedo enviar en una solicitud?",question9:"¿Qué latencia puedo esperar al reclasificar 100 documentos?",title:"Preguntas comunes relacionadas con el reranker"},feature_on_premises_description2:"Implemente Jina Reranker en AWS Sagemaker y pronto en Microsoft Azure y Google Cloud Services, o comuníquese con nuestro equipo de ventas para obtener implementaciones personalizadas de Kubernetes para su nube privada virtual y servidores locales.",feature_on_premises_description3:"Implemente Jina Reranker en AWS Sagemaker y Microsoft Azure y pronto en Google Cloud Services, o comuníquese con nuestro equipo de ventas para obtener implementaciones personalizadas de Kubernetes para su nube privada virtual y servidores locales.",feature_solid_description:"Desarrollado a partir de nuestra investigación académica de vanguardia y probado rigurosamente con los reclasificadores SOTA para garantizar un rendimiento incomparable.",how_it_works:"Así es como funciona:",how_it_works_v1:{description1:"Un sistema de búsqueda utiliza embeddings/BM25 para encontrar un amplio conjunto de documentos potencialmente relevantes en función de la consulta del usuario.",description2:"Luego, el reclasificador toma estos resultados y los analiza a un nivel más granular, considerando los matices de cómo los términos de consulta interactúan con el contenido del documento.",description3:"Reordena los resultados de la búsqueda, colocando en la parte superior los que considera más relevantes, en base a este análisis más profundo.",title1:"Recuperación inicial",title2:"Reclasificación",title3:"Resultados mejorados"},improve_performance:"Mejora garantizada sobre la búsqueda de vectores",improve_performance_description:"Nuestras evaluaciones demostraron mejoras en los sistemas de búsqueda que emplean Jina Reranker con un +8 % en la tasa de aciertos y un +33 % en la clasificación recíproca media.",learning1:"Aprendiendo sobre Reranker",learning1_description:"¿Qué es un reranker? ¿Por qué no es suficiente la búsqueda de vectores o la similitud de cosenos? Aprenda sobre los rerankers desde cero con nuestra guía completa.",read_more_about_benchmark:"Leer más sobre el punto de referencia",read_more_about_turbo:"Lea más sobre los modelos turbo y tiny",read_more_about_v2:"Jina Reranker v2 es el mejor reranker de su clase lanzado el 25 de junio de 2024; está diseñado para Agentic RAG. Cuenta con soporte de llamada de funciones, recuperación multilingüe para más de 100 idiomas, capacidades de búsqueda de códigos y ofrece una velocidad 6 veces mayor que la v1. Lea más sobre el modelo v2.",reranker_description:"Pruebe nuestra API de reranker de vanguardia para maximizar la relevancia de su búsqueda y la precisión de RAG. ¡Empezando gratis!",show_v2benchmark:"Mostrar punto de referencia para el modelo v2 (más reciente)",table:{number_token_document:"Número de tokens en cada documento",number_token_query:"Número de tokens en la consulta",title:"A continuación se muestra el costo de tiempo de reclasificar una consulta y 100 documentos en milisegundos:"},title:"API de reclasificación",top_n:"Número de documentos devueltos",top_n_explain:"El número de documentos más relevantes que se devolverán para la consulta.",try_embedding:"Pruebe incorporar API de forma gratuita",try_reranker:"Pruebe la API de reranker gratis",v2_features:{description1:"Reranker v2 permite la recuperación de documentos en más de 100 idiomas, independientemente del idioma de consulta.",description2:"Reranker v2 clasifica fragmentos de código y firmas de funciones en función de consultas en lenguaje natural, ideal para aplicaciones Agentic RAG.",description3:"Reranker v2 clasifica las tablas más relevantes basándose en consultas en lenguaje natural, lo que ayuda a ordenar diferentes esquemas de tablas e identificar el más relevante antes de generar una consulta SQL.",title1:"Recuperación multilingüe",title2:"Llamada de funciones y búsqueda de códigos",title3:"Soporte de datos tabulares y estructurados"},v2benchmark:{descBeir:"Puntuaciones NDCG 10 reportadas para diferentes modelos de reclasificación para el conjunto de datos de Beir",descCodeSearchNet:"Puntuaciones de MRR 10 informadas para diferentes modelos de reclasificación para el conjunto de datos CodeSearchNet",descMKQA:"Recuerde 10 puntuaciones reportadas para diferentes modelos de reclasificación para el conjunto de datos MKQA",descNSText2SQL:"Recuerde 3 puntuaciones reportadas para diferentes modelos de reclasificación para el conjunto de datos NSText2SQL",descRTX4090:"Puntuaciones de rendimiento (documentos recuperados en 50 ms) informadas para diferentes modelos de reclasificación en una GPU RTX 4090.",descToolBench:"Recuerde 3 puntuaciones reportadas para diferentes modelos de reclasificación para el conjunto de datos de ToolBench",titleBeir:"BEIR (Parámetro de referencia heterogéneo en diversas tareas de RI)",titleCodeSearchNet:"CódigoSearchNet. El punto de referencia es una combinación de consultas en formatos de cadena de documentación y lenguaje natural, con segmentos de código etiquetados relevantes para las consultas.",titleMKQA:"MKQA (Preguntas y respuestas sobre conocimientos multilingües)",titleNSText2SQL:"NSText2SQL",titleRTX4090:"Rendimiento de Jina Reranker v2 en RTX4090",titleToolBench:"Banco de herramientas. El punto de referencia recopila más de 16 mil API públicas y las correspondientes instrucciones generadas sintéticamente para usarlas en configuraciones de API única y múltiple."},vs_table:{col0:"reclasificador",col0_1:"Precisión y relevancia de búsqueda mejoradas",col0_2:"Filtrado inicial y rápido",col0_3:"Recuperación de texto general en consultas de amplio alcance",col1:"Búsqueda de vectores",col1_1:"Detallado: subdocumento y segmento de consulta",col1_2:"Amplio: documentos completos",col1_3:"Intermedio: varios segmentos de texto",col2:"BM25",col2_1:"Alto",col2_2:"Medio",col2_3:"Bajo",col3_1:"No requerido",col3_2:"Alto",col3_3:"Bajo, utiliza índice prediseñado",col4_1:"Alto",col4_2:"Alto",col4_3:"No requerido",col5_1:"Superior para consultas matizadas",col5_2:"Equilibrado entre eficiencia y precisión",col5_3:"Consistente y confiable para un amplio conjunto de consultas",col6_1:"Altamente preciso con una profunda comprensión contextual.",col6_2:"Rápido y eficiente, con precisión moderada.",col6_3:"Altamente escalable, con eficacia establecida",col7_1:"Uso intensivo de recursos con implementación compleja",col7_2:"Es posible que no capture el contexto o los matices de la consulta profunda",col7_3:"Puede tener un rendimiento inferior en búsquedas muy específicas o contextuales",header0:"Mejor para",header1:"Granularidad",header2:"Complejidad del tiempo de consulta",header3:"Complejidad del tiempo de indexación",header4:"Complejidad del tiempo de entrenamiento",header5:"Calidad de búsqueda",header6:"Fortalezas",header7:"Debilidades",subtitle:"La siguiente tabla proporciona una comparación completa de Reranker, Vector/Inbeddings Search y BM25, destacando sus fortalezas y debilidades en varias categorías.",title:"Comparación de Reranker, Vector Search y BM25"},what_is:"¿Qué es un reranker?",what_is_answer_long:`El objetivo de un sistema de búsqueda es encontrar los resultados más relevantes de forma rápida y eficaz. Tradicionalmente, se han utilizado métodos como BM25 o tf-idf para clasificar los resultados de búsqueda según la concordancia de palabras clave. En muchas bases de datos vectoriales se han implementado métodos recientes, como la similitud de coseno basada en incrustación. Estos métodos son sencillos, pero a veces pueden pasar por alto las sutilezas del lenguaje y, lo más importante, la interacción entre los documentos y la intención de una consulta.

Aquí es donde brilla el "reranker". Un reranker es un modelo de IA avanzado que toma el conjunto inicial de resultados de una búsqueda (a menudo proporcionado por una búsqueda incrustada/basada en tokens) y los reevalúa para garantizar que se alineen más estrechamente con la intención del usuario. Mira más allá de la coincidencia superficial de términos para considerar la interacción más profunda entre la consulta de búsqueda y el contenido de los documentos.`,what_is_answer_long_ending:"El reclasificador puede mejorar significativamente la calidad de la búsqueda porque opera a nivel de subdocumento y subconsulta, lo que significa que analiza las palabras y frases individuales, sus significados y cómo se relacionan entre sí dentro de la consulta y los documentos. Esto da como resultado un conjunto de resultados de búsqueda más preciso y contextualmente relevante.",what_is_desc:"Un reranker es un modelo de IA que refina los resultados de la búsqueda a partir de una búsqueda vectorial o un modelo de recuperación denso. Leer más."},ye={caption_image_desc:"Generar una descripción textual de la imagen.",caption_image_title:"Imagen de título",description:"Explore la narración de imágenes más allá de los píxeles",example1:"Este vídeo parece ser un metraje de la naturaleza que muestra un encantador conejito blanco y una mariposa en un campo de hierba. Se ve al conejito interactuando con la mariposa de diferentes maneras, mostrando su relación única. El entorno natural proporciona un telón de fondo pintoresco que realza la belleza de esta escena sencilla pero cautivadora.",generate_story_desc:"Elabora una historia inspirada en la imagen, que a menudo incluye diálogos o monólogos de sus personajes.",generate_story_title:"Generar historia",intro1:"Solución líder de IA para subtítulos de imágenes y resúmenes de vídeos",json_image_desc:"Genere un formato JSON estructurado a partir de la imagen utilizando un esquema predefinido. Esto permite la extracción de datos específicos de la imagen.",json_image_title:"Extraer JSON de la imagen",summarize_video_desc:"Genere un resumen conciso del vídeo, destacando los eventos clave.",summarize_video_title:"Resumir vídeo",visual_q_a_desc:"Responda una consulta basada en el contenido de la imagen.",visual_q_a_title:"Preguntas y respuestas visuales"},he={ask_on_current_page:"Pregunte a la página actual sobre...",find_solution:"Generar una solución para...",hint:"Busque productos, noticias y sus preguntas.",hotkey:"Presione la tecla / para buscar en esta página",hotkey1:"Prensa",hotkey2:"para alternar",hotkey_long1:"En cualquier momento, presione",hotkey_long3:"para abrir la barra de búsqueda",more_results:"{_numMore} más resultados",placeholder:"Haga cualquier pregunta en esta página",proposing_solution:"Generando respuesta basada en el contenido de la página...",required:"Describe tu pregunta con más detalles.",results:"resultados"},Ae={description:"Navegue, interactúe, perfeccione: vuelva a imaginar el descubrimiento de productos"},je={description:"Cerrar la brecha semántica en su infraestructura de búsqueda existente"},ze={"Hacker News":"Noticias de piratas informáticos",LinkedIn:"LinkedIn",facebook:"Facebook",reddit:"Reddit",rss:"RSS Feed",share_btn:"Compartir",twitter:"X (Twitter)"},xe={click_to_learn_more:"Haz click para aprender mas",contextualization:"Contextualización",contextualization_desc:"Los rerankers ajustan los resultados de búsqueda iniciales en función de una profunda relevancia contextual. consulta. Esto refina la clasificación para que coincida mejor con lo que los usuarios probablemente encuentren útil.",coreInfra:"Infraestructura central",coreInfra_desc:"Core Infra proporciona una capa nativa de la nube para desarrollar, implementar y orquestar modelos básicos de búsqueda tanto en la nube pública como en las instalaciones, lo que permite que los servicios aumenten y disminuyan sin esfuerzo.",embedding_serving:"Incrustar servicio",embedding_serving_description:"Entrega de incorporaciones a través de un microservicio robusto y escalable que utiliza tecnologías nativas de la nube.",embedding_tech:"Incrustaciones",embedding_tech_description:`En Jina AI, aprovechamos el poder de la tecnología integrada para revolucionar diversas aplicaciones de IA. Esta tecnología sirve como un método unificado para representar y comprimir de manera eficiente varios tipos de datos, garantizando que no se pierda información crítica. Nuestro objetivo es transformar conjuntos de datos complejos en un formato de incrustación universalmente comprensible, lo cual es esencial para un análisis de IA preciso y revelador.

Las incrustaciones son fundamentales, especialmente en aplicaciones como el reconocimiento preciso de imágenes y voz, donde ayudan a discernir detalles y matices finos. En el procesamiento del lenguaje natural, las incorporaciones mejoran la comprensión del contexto y el sentimiento, lo que lleva a herramientas de traducción de idiomas e inteligencia artificial conversacional más precisas. También son cruciales para desarrollar sistemas de recomendación sofisticados que requieren una comprensión profunda de las preferencias del usuario en diferentes formas de contenido, como texto, audio y video.`,embedding_tuning:"Ajuste de incrustación",embedding_tuning_description:"Optimización de incorporaciones de alta calidad mediante la integración de experiencia en el dominio para mejorar el rendimiento de tareas específicas.",embeddings:"Incrustaciones",embeddings_desc:"Las incrustaciones son la piedra angular del sistema de búsqueda moderno y representan datos multimodales en vectores de números. Este proceso permite una comprensión más matizada y contextual del contenido, mucho más allá de la simple coincidencia de palabras clave.",for_developers:"Para desarrolladores",for_enterprise:"Para Empresas",for_power_users:"Para usuarios avanzados",grounding:"Toma de tierra",grounding_desc:"Lector refinando entradas y resultados a través de LLM. Mejoran la calidad, legibilidad y factibilidad de la respuesta final.",model_serving:"Servicio modelo",model_serving_description:"La implementación de modelos ajustados en un entorno de producción, que generalmente requiere recursos sustanciales, como el alojamiento de GPU. MLOps, enfatizando el servicio de modelos medianos a grandes de una manera escalable, eficiente y confiable.",model_tuning:"Ajuste del modelo",model_tuning_description:"También conocido como ajuste fino, implica ajustar los parámetros de un modelo previamente entrenado en un nuevo conjunto de datos, a menudo específico de una tarea, para mejorar su rendimiento y adaptarlo a una aplicación específica.",personalization:"Personalización",personalization_desc:"Uso de datos sintéticos guiados por las instrucciones del usuario para entrenar automáticamente un modelo de incrustación y reclasificación específico del dominio.",preprocessing:"Preprocesamiento",preprocessing_desc:"El preprocesamiento implica limpiar, normalizar y transformar datos sin procesar en un formato que el sistema de búsqueda pueda digerir.",promptOps:"Operaciones inmediatas",promptOps_desc:"Prompt Ops mejora la entrada y salida del sistema de búsqueda, incluidas las utilizadas en la expansión de consultas, la entrada de LLM y la reescritura de resultados. Esto garantiza que la búsqueda se comprenda mejor y obtenga mejores resultados.",prompt_serving:"Servicio rápido",prompt_serving_description:"Envolviendo y entregando avisos a través de una API, sin alojar modelos pesados. La API llama a un servicio de modelo de lenguaje grande público y maneja la orquestación de entradas y salidas en una cadena de operaciones.",prompt_tech:"Ingeniería rápida y de agentes",prompt_tech_description:`En Jina AI, reconocemos que la ingeniería rápida es vital para interactuar con grandes modelos de lenguaje (LLM). A medida que estos modelos avanzan, la complejidad de las indicaciones aumenta, abarcando razonamiento y lógica intrincados. Este avance subraya el crecimiento entrelazado de los LLM y la sofisticación inmediata.

Prevemos un futuro en el que los LLM actuarán como compiladores y los mensajes se convertirán en el nuevo lenguaje de programación. Este cambio sugiere que el dominio tecnológico futuro puede centrarse más en el dominio rápido que en la codificación tradicional. Nuestro compromiso en Jina AI es liderar esta área transformadora, haciendo que la IA avanzada sea accesible y práctica para el uso diario al dominar este "lenguaje" emergente.`,prompt_tuning:"Sintonización rápida",prompt_tuning_description:"El proceso de elaborar y refinar las indicaciones de entrada para guiar su salida hacia respuestas específicas y deseadas.",representation:"Representación",representation_desc:"Las incrustaciones transforman datos multimodales en un formato vectorizado uniforme. Esto permite que el sistema de búsqueda comprenda y categorice el contenido más allá de simples palabras clave.",rerankers:"reclasificador",rerankers_desc:"Los rerankers toman los resultados iniciales de las incrustaciones y los refinan, asegurando que se presenten los resultados más relevantes al usuario. Esto es crucial para ofrecer resultados de búsqueda de alta calidad que cumplan con la intención del usuario."},Pe={care_most:"¿Qué es lo que más te importa?",care_most_options:{accuracy:"Exactitud",cost:"Costo",other:"Otro",scalability:"Escalabilidad",speed:"Velocidad"},care_most_required:"A la hora de elegir un servicio, ¿qué es lo que más te importa?",company_size:"¿Cuál es el tamaño de su empresa?",company_size_required:"Cuéntanos el tamaño de tu empresa nos ayuda a dar un mejor servicio",company_url:"¿Cuál es el sitio web de su empresa?",company_url_required:"Cuéntanos que la web de tu empresa nos ayuda a dar un mejor servicio",contactName:"Su nombre",contactName_required:"¿Cómo deberíamos dirigirnos a usted?",contactTitle:"¿Cuál es su profesión?",contactTitle_required:"Su título de trabajo es requerido",contact_us:"Contáctenos",domain_required:"Cuéntanos tu dominio de trabajo nos ayuda a dar un mejor servicio",email:"Correo electrónico",email_contact:"Tu correo electrónico de contacto",email_invalid:"el correo electrónico es invalido",email_required:"correo electronico es requerido",fine_tuned_embedding:"¿Está interesado en incorporaciones optimizadas y adaptadas a sus datos y caso de uso? ¡Vamos a discutir!",fine_tuned_reranker:"¿Está interesado en reclasificadores ajustados y adaptados a sus datos y caso de uso? ¡Vamos a discutir!",full_survey:"Responda la encuesta completa y obtenga una respuesta más rápida de nuestro equipo",get_new_key:"Obtenga su clave API",get_update_blog_posts:"Obtenga las últimas actualizaciones de las publicaciones del blog.",get_update_embeddings:"Obtenga las últimas actualizaciones para las incorporaciones",send:"Enviar",sign_up:"Inscribirse",subscribe:"Suscribir",tell_domain:"Cuéntanos tu dominio",usage_type:"¿Qué tipo de uso te describe mejor?",usage_type_options:{other:"Otro",poc:"Prueba de concepto",production:"Producción",research:"Investigación"},usage_type_required:"Díganos que su tipo de uso nos ayuda a brindar un mejor servicio.",used_product:"¿Qué modelo estás usando?",used_product_required:"Selecciona el modelo que estás utilizando o te interesa"},ke={description:"Técnicas de agentes para aumentar su LLM y llevarlo más allá de sus límites"},Ee="Tabla de contenido",Ie={advance_usage:"Utilice la solicitud POST para obtener más funciones",basic_usage:"Utilice la solicitud GET para contar tokens",basic_usage_explain:"Simplemente puede enviar una solicitud GET para contar la cantidad de tokens en su texto.",change_content:"Cambie el 'contenido' y vea el resultado en vivo",chars:"personajes",chinese:"Chino",chunk:"Pedazo",chunk_all:"Todos los trozos",chunking:"¡Agrupe documentos largos a la velocidad del rayo!",chunking_explain:"También puede utilizar Segmenter API para dividir documentos largos en fragmentos más pequeños, lo que facilita su procesamiento en incrustaciones o reclasificadores. Aprovechamos las señales estructurales comunes y creamos un conjunto de reglas y heurísticas que funcionan bien en diversos tipos de contenido, por ejemplo, lenguajes Markdown, HTML, LaTeX y CJK.",chunking_short:"Fragmentación",chunks_in_total:"{_numChunks} fragmentos en total",count_tokens_hint:"<b>{_numTokens}</b> tokens, {_numChars} caracteres.",description:"Corta el texto largo en fragmentos y haz tokenización.",description_long:"Nuestra API Segmenter es fundamental para ayudar a los LLM a gestionar la entrada dentro de los límites del contexto y optimizar el rendimiento del modelo. Permite a los desarrolladores contar tokens y extraer segmentos de texto relevantes, lo que garantiza un procesamiento de datos eficiente y una gestión de costos.",description_long1:"API gratuita para segmentar texto largo en fragmentos y tokenizarlo.",english:"Inglés",explain:"Un segmentador es un componente crucial que convierte el texto en tokens o fragmentos, que son las unidades básicas de datos que procesa un modelo de incrustación/reclasificación o LLM. Los tokens pueden representar palabras completas, partes de palabras o incluso caracteres individuales.",faq_v1:{answer1:"La API de Segmenter es de uso gratuito. Si proporciona su clave API, podrá acceder a un límite de velocidad más alto y no se le cobrará por su clave.",answer10:"Además de los idiomas occidentales, la fragmentación también funciona bien con el chino, el japonés y el coreano.",answer2:"Sin una clave API, puede acceder a la API de Segmenter con un límite de velocidad de 20 RPM.",answer3:"Con una clave API, puede acceder a la API de Segmenter con un límite de velocidad de 200 RPM. Para los usuarios pagos premium, el límite de velocidad es de 1000 RPM.",answer4:"No, su clave API solo se utiliza para acceder a un límite de velocidad más alto.",answer5:"Sí, la API de Segmenter es multilingüe y admite más de 100 idiomas.",answer6:"Las solicitudes GET se utilizan únicamente para contar la cantidad de tokens en un texto, lo que le permite integrarlo fácilmente como un contador en su aplicación. Las solicitudes POST admiten más parámetros y funciones, como devolver los primeros/últimos N tokens.",answer7:"Puede enviar hasta 64k caracteres por solicitud.",answer8:"La función de fragmentación segmenta documentos largos en fragmentos más pequeños según señales estructurales comunes, lo que garantiza una segmentación precisa del texto en fragmentos significativos. Básicamente, se trata de un patrón de expresiones regulares (¡grande!) que segmenta el texto según ciertas características sintácticas que suelen coincidir con los límites semánticos, como los finales de las oraciones, los saltos de párrafo, la puntuación y ciertas conjunciones. No se trata de fragmentación semántica. Esta expresión regular (grande) es tan potente como puede serlo dentro de las limitaciones de las expresiones regulares. Equilibra la complejidad y el rendimiento. Si bien la verdadera comprensión semántica no es posible con las expresiones regulares, se aproxima bien al contexto mediante señales estructurales comunes.",answer9:"Si la entrada contiene tokens especiales, nuestra API Segmenter los colocará en el campo 'special_tokens'. Esto le permite identificarlos fácilmente y manejarlos como corresponde para sus tareas posteriores, por ejemplo, eliminarlos antes de introducir el texto en un LLM para evitar ataques de inyección.",question1:"¿Cuánto cuesta la API Segmenter?",question10:"¿La función de chunking admite otros idiomas además del inglés?",question2:"Si no proporciono una clave API, ¿cuál es el límite de velocidad?",question3:"Si proporciono una clave API, ¿cuál es el límite de velocidad?",question4:"¿Cobrarás los tokens de mi clave API?",question5:"¿La API de Segmenter admite varios idiomas?",question6:"¿Cuál es la diferencia entre las solicitudes GET y POST?",question7:"¿Cuál es la longitud máxima que puedo tokenizar por solicitud?",question8:"¿Cómo funciona la función de fragmentación? ¿Se trata de fragmentación semántica?",question9:"¿Cómo se manejan tokens especiales como 'endoftext' en la API de Segmenter?",title:"Preguntas frecuentes relacionadas con el segmentador"},free_api:"La API de Segmenter es de uso gratuito. Si proporciona su clave API, podrá acceder a un límite de velocidad más alto y no se le cobrará por la clave.",input_text:"Texto de entrada",is_free:"¡La API de Segmenter es gratuita!",is_free_description:"Al proporcionar su clave API, podrá acceder a un límite de tarifa más alto y no se le cobrará su clave.",japanese:"japonés",korean:"coreano",parameters:{auth_token:"Agregar clave API para límite de velocidad más alto",auth_token_explain:"Ingresa tu clave API de Jina para acceder a un límite de velocidad más alto. Para obtener la información más actualizada sobre el límite de velocidad, consulta la siguiente tabla.",head:"Devuelve los primeros N tokens",head_explain:"Devuelve los primeros N tokens del contenido indicado. Excluye límites. No se puede utilizar con 'tail'.",learn_more:"Más información",max_chunk_length:"Longitud máxima de cada fragmento",max_chunk_length_explain:"Número máximo de caracteres en cada fragmento. En la práctica, la longitud del fragmento puede ser menor que este valor, si existe un límite natural en el texto.",return_chunks:"Devolver los trozos",return_chunks_explain:"Dividir la entrada en segmentos semánticamente significativos mientras se maneja una amplia variedad de tipos de texto y casos extremos basados en señales estructurales comunes.",return_tokens:"Devolver las fichas",return_tokens_explain:"Devuelve los tokens y sus identificadores correspondientes en la respuesta. Activa o desactiva esta opción para ver la visualización del resultado.",tail:"Devuelve los últimos N tokens",tail_explain:"Devuelve los últimos N tokens del contenido indicado. Excluye límites. No se puede utilizar con 'head'.",type:"Segmentador",type_explain:"Seleccione el tokenizador a utilizar.",used_by_models:"Utilizado en {_usedBy}."},remove_boundary_cues:"Eliminar saltos de línea",remove_boundary_cues_explain:"Elimine todos los saltos de línea (las principales señales de límite) de la entrada, esto hace que el problema sea más desafiante y observe cómo cambia la respuesta.",show_space:"Mostrar espacios iniciales y finales",table:{td_1_0:"Tokeniza textos, cuenta y obtén los primeros/últimos N tokens.",td_1_1:"20 RPM",td_1_2:"200 RPM",td_1_3:"1000 RPM",td_1_4:"Sin cargo",td_1_5:"800 ms"},title:"API de segmentación",token_index:"Índice de token: {_index}",usage:"Uso",visualization:"Visualización",what_is:"¿Qué es un segmentador?"},Le={cta:"Traducir al código {_lang}",select_language:"Idioma"},Ce={description:"Una base de datos vectorial de Python que solo necesita, ni más ni menos"},Se="zzz",we={PRODUCT_DESCRIPTION:e,SEO_TAG_LINE:a,about_us_page:o,api_general_faq:n,autotune:i,avatar:s,best_banner:r,beta:t,billing_general_faq:d,blog_tags:c,book2024:l,cclicence:u,classifier:m,clip_as_service:p,cloud:g,contact_us_page:v,copy:b,copy_to_clipboard_success:_,dalle_flow:f,deepsearch:q,"dev-gpt":{description:"Tu equipo de desarrollo virtual"},disco_art:y,doc_array:h,download:A,embedding:j,embeddings:z,estimator:x,faq:P,faq_button:k,farewell:E,finetuner:I,finetuner_plus:L,finetuning:C,footer:S,get_new_key:w,github:M,grounding:R,header:T,hub:D,huggingface:N,impact_snapshots:B,inference:U,integrations:J,internship_faq:G,internship_page:O,jcloud:F,jerboa:H,jina:Q,jina_chat:V,key_manager:W,lab_dialog:K,landing_page:X,langchain_serve:Y,legal_page:$,model_graph:Z,models:ee,news_page:ae,newsroom_page:oe,notice:ne,open_day:ie,open_day_faq:se,open_gpt:re,paywall:te,powered_by:de,print:ce,project_status:le,prompt_perfect:ue,promptperfect:me,purchase:pe,purchase_now:ge,rate_limit:ve,rationale:be,reader:_e,recommender:fe,reranker:qe,scenex:ye,searchbar:he,searchscape:Ae,semantic:je,share:ze,spectrum:xe,subscribe_system:Pe,think_gpt:ke,toc:Ee,tokenizer:Ie,translator:Le,vectordb:Ce,zzz:Se};export{e as PRODUCT_DESCRIPTION,a as SEO_TAG_LINE,o as about_us_page,n as api_general_faq,i as autotune,s as avatar,r as best_banner,t as beta,d as billing_general_faq,c as blog_tags,l as book2024,u as cclicence,m as classifier,p as clip_as_service,g as cloud,v as contact_us_page,b as copy,_ as copy_to_clipboard_success,f as dalle_flow,q as deepsearch,we as default,y as disco_art,h as doc_array,A as download,j as embedding,z as embeddings,x as estimator,P as faq,k as faq_button,E as farewell,I as finetuner,L as finetuner_plus,C as finetuning,S as footer,w as get_new_key,M as github,R as grounding,T as header,D as hub,N as huggingface,B as impact_snapshots,U as inference,J as integrations,G as internship_faq,O as internship_page,F as jcloud,H as jerboa,Q as jina,V as jina_chat,W as key_manager,K as lab_dialog,X as landing_page,Y as langchain_serve,$ as legal_page,Z as model_graph,ee as models,ae as news_page,oe as newsroom_page,ne as notice,ie as open_day,se as open_day_faq,re as open_gpt,te as paywall,de as powered_by,ce as print,le as project_status,ue as prompt_perfect,me as promptperfect,pe as purchase,ge as purchase_now,ve as rate_limit,be as rationale,_e as reader,fe as recommender,qe as reranker,ye as scenex,he as searchbar,Ae as searchscape,je as semantic,ze as share,xe as spectrum,Pe as subscribe_system,ke as think_gpt,Ee as toc,Ie as tokenizer,Le as translator,Ce as vectordb,Se as zzz};

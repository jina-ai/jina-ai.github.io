<!DOCTYPE html><html translate="no" dir="ltr" lang="en-US"><head><title>A Tale of Two Worlds: EMNLP 2023 at Sentosa</title><meta charset="utf-8"><meta name="title" content="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><meta name="description" content="Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa"><meta property="og:title" content="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><meta property="og:description" content="Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas."><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa"><meta property="twitter:title" content="A Tale of Two Worlds: EMNLP 2023 at Sentosa"><meta property="twitter:description" content="Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas."><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png">  <script type="module" crossorigin="" src="/assets/index.25bd83bc.js"></script>
  <link rel="stylesheet" href="/assets/index.9fe6336f.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/register.1f690289.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip.2d92bc4f.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine.6c8d5c1e.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/selection.cef1371f.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard.5b586f3f.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index.25bd83bc.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout.e2da99dd.js"><link rel="stylesheet" href="/assets/MainLayout.891b970f.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpace.798e5d0c.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSelect.0903ee00.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip.30a3065d.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel.82f7d6fd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu.ee0c2b67.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/format.988231da.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/embedding.d6432bbd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList.0562c458.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown.fdf840f2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnGroup.a610ebc8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver.ae83a352.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QImg.3f84753e.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem.6c67256a.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollArea.3241795d.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan.8f7e30de.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch.3df10340.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResponsive.1ea7a0cb.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup.188122ef.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/chunk.17bfede8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/isArrayLike.a2a00cef.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/isObject.cef35763.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/toNumber.476d0739.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage.7fbda9f9.js"><link rel="stylesheet" href="/assets/NewsPage.eb622b00.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage.84e85e37.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs.cb0b975e.js"><link rel="stylesheet" href="/assets/blogs.b3aadae9.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/plugin-vue_export-helper.21dcd24c.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge.08b6da84.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard.e2eed9bd.js"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><link prerender-ignore rel=preconnect href=//app.usercentrics.eu><link prerender-ignore rel=preconnect href=//api.usercentrics.eu><link prerender-ignore rel=preconnect href=//privacy-proxy.usercentrics.eu><link prerender-ignore rel=preload href=//app.usercentrics.eu/browser-ui/latest/loader.js as=script><link prerender-ignore rel=preload href=//privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js as=script><script prerender-ignore id=usercentrics-cmp data-settings-id=w5v6v2pJsC3wdR src=https://app.usercentrics.eu/browser-ui/latest/loader.js async></script><script prerender-ignore src=https://privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js></script><script prerender-ignore>// (optional) additional configs for the Smart Data Protector
      uc.reloadOnOptIn('BJz7qNsdj-7'); // reload page on YouTube opt-in</script><style prerender-ignore>.uc-embedding-container {
      min-height: 100% !important;
    }

    .uc-embedding-wrapper p {
      color: black;
    }</style><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top text-white lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none" role="toolbar"><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button" style="padding: 24px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-icons" aria-hidden="true" role="img">reorder</i></span></button><hr class="q-separator q-separator--vertical q-separator--dark" aria-orientation="vertical"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" href="/" style="font-size: 32px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></span></span></a><div class="q-space"></div><label class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--float q-field--dense q-field--dark" for="f_9a45e0f5-ecfc-4dc5-86d9-b5509a765958"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__prepend q-field__marginal row no-wrap items-center"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation">language</i></div><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span>English</span><input class="q-select__focus-target" id="f_9a45e0f5-ecfc-4dc5-86d9-b5509a765958" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_9a45e0f5-ecfc-4dc5-86d9-b5509a765958_lb"></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon fas fa-caret-down q-select__dropdown-icon" aria-hidden="true" role="presentation"> </i></div></div></div></label></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-left" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--left q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(-300px);"><div class="q-drawer__content fit scroll"><div class="q-scrollarea q-scrollarea--dark fit"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark q-list--padding"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-avatar"><div class="q-avatar__content row flex-center overflow-hidden"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation">notifications</i></div></div></div><div class="q-item__section column q-item__section--main justify-center">News</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="c275e199-ccac-4765-bec8-76c8c1000cb8" aria-label="Expand &quot;For Enterprises&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">For Enterprises</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon fas fa-chevron-down q-expansion-item__toggle-icon" aria-hidden="true" role="presentation"> </i></div></div><div class="q-expansion-item__content relative-position" id="c275e199-ccac-4765-bec8-76c8c1000cb8" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Embeddings</div><div class="q-item__label q-item__label--caption text-caption">Our world-class embeddings for your search and RAG systems</div></div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="2da1a177-b764-42da-9389-a58074412b90" aria-label="Expand &quot;For Power Users&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">For Power Users</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon fas fa-chevron-down q-expansion-item__toggle-icon" aria-hidden="true" role="presentation"> </i></div></div><div class="q-expansion-item__content relative-position" id="2da1a177-b764-42da-9389-a58074412b90" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://promptperfect.jina.ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://promptperfect.jina.ai/PromptPerfect-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">PromptPerfect</div><div class="q-item__label q-item__label--caption text-caption">Premier tool for prompt engineering</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://scenex.jina.ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://scenex.jina.ai/SceneX - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">SceneXplain</div><div class="q-item__label q-item__label--caption text-caption">Leading AI solution for image captions and video summaries</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://bestbanner.jina.ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://bestbanner.jina.ai/bestbanner-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">BestBanner</div><div class="q-item__label q-item__label--caption text-caption">Blog to banner, without the prompts!</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://chat.jina.ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://chat.jina.ai/JinaChat - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JinaChat</div><div class="q-item__label q-item__label--caption text-caption">More modality, longer memory, less cost</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://rationale.jina.ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://rationale.jina.ai/Rationale-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Rationale</div><div class="q-item__label q-item__label--caption text-caption">Ultimate AI decision-making tools</div></div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="02d09e2a-91a0-4b16-b075-6ee75938c9f0" aria-label="Expand &quot;For Developers&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">For Developers</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon fas fa-chevron-down q-expansion-item__toggle-icon" aria-hidden="true" role="presentation"> </i></div></div><div class="q-expansion-item__content relative-position" id="02d09e2a-91a0-4b16-b075-6ee75938c9f0" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/docarray/docarray"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/doc-array.35372518.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DocArray</div><div class="q-item__label q-item__label--caption text-caption">The data structure for multimodal data</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/jina"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/core.99751891.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Jina</div><div class="q-item__label q-item__label--caption text-caption">Build multimodal AI applications on the cloud</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/finetuner"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/finetuner.c62eaafa.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Finetuner</div><div class="q-item__label q-item__label--caption text-caption">Fine-tune embeddings on domain specific data for better search quality</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/clip-as-service"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/clip-as-service.f454ca2a.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">CLIP-as-service</div><div class="q-item__label q-item__label--caption text-caption">Embed images and sentences into fixed-length vectors with CLIP</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/jcloud"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/jcloud.669910ba.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JCloud</div><div class="q-item__label q-item__label--caption text-caption">Deploy a local project as a cloud service. Radically easy, no nasty surprises.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/langchain-serve"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/langchain-serve.8cf53254.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">LangChain-Serve</div><div class="q-item__label q-item__label--caption text-caption">Langchain apps on production with Jina &amp; FastAPI</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/vectordb"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/VectorDB.46be6cc1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">VectorDB</div><div class="q-item__label q-item__label--caption text-caption">A Python vector database you just need - no more, no less</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://cloud.jina.ai/executors"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/hub.5e89f942.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Executor Hub</div><div class="q-item__label q-item__label--caption text-caption">Share and discover building blocks for multimodal AI applications</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/dalle-flow"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/dall-e-flow.ea199b2d.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DALL-E Flow</div><div class="q-item__label q-item__label--caption text-caption">A human-in-the-Loop workflow for creating HD images from text</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/discoart"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/disco-art.f21a267f.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DiscoArt</div><div class="q-item__label q-item__label--caption text-caption">Create compelling Disco Diffusion artworks in one line of code</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/thinkgpt"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/think-gpt.0a671280.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">ThinkGPT</div><div class="q-item__label q-item__label--caption text-caption">Agent techniques to augment your LLM and push it beyond its limits</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/dev-gpt"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/dev-gpt.a3e55036.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DevGPT</div><div class="q-item__label q-item__label--caption text-caption">Your virtual development team</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/rungpt"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/run-gpt.5571707e.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">RunGPT</div><div class="q-item__label q-item__label--caption text-caption">An open-source cloud-native of large multimodal models serving framework</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://github.com/jina-ai/jerboa"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/jerboa.af6b308b.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Jerboa</div><div class="q-item__label q-item__label--caption text-caption">An experimental finetuner for open-source LLMs</div></div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="a5e74cb7-ad89-4041-b60f-950d691edf3c" aria-label="Expand &quot;Company&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><span class="q-icon" aria-hidden="true" role="presentation"><img src="/J.svg"></span></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Company</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon fas fa-chevron-down q-expansion-item__toggle-icon" aria-hidden="true" role="presentation"> </i></div></div><div class="q-expansion-item__content relative-position" id="a5e74cb7-ad89-4041-b60f-950d691edf3c" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">About us</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/open-day"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Open day</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://jobs.lever.co/jina-ai"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Join us</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-avatar"><div class="q-avatar__content row flex-center overflow-hidden"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation">open_in_new</i></div></div></div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 82px; padding-bottom: 69px;"><main data-v-5519711e="" class="q-page" style="min-height: 100vh;"><div data-v-5519711e="" class="row justify-center q-gutter-lg"><div data-v-5519711e="" class="col q-my-xl q-py-xl"><div data-v-5519711e="" class="q-mx-md q-mx-md-xl q-px-md-md q-px-lg-lg q-px-xl-xl"><div data-v-5519711e="" class="row justify-between items-center q-mt-lg"><a data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch col-auto" tabindex="0" href="/news"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon on-left notranslate material-icons" aria-hidden="true" role="img">arrow_circle_left</i><span class="block">Back to Newsroom</span></span></a><div data-v-5519711e="" class="col-auto"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Events</div></div></div></div></div><div data-v-5519711e="" class="q-img q-img--menu q-mt-md" role="img"><div style="padding-bottom: 52.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--26-.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"><div data-v-5519711e="" class="text-white absolute-bottom q-pa-lg lock-blur text-h4" style="background: rgba(0, 0, 0, 0.5);">A Tale of Two Worlds: EMNLP 2023 at Sentosa</div></div></div><div data-v-5519711e="" class="row justify-center q-mt-lg q-pt-lg"><div data-v-5519711e="" class="box col-10 col-md-6"><i data-v-5519711e="" class="fas fa-quote-left fa2"></i><div data-v-5519711e="" class="text"><i data-v-5519711e="" class="fas fa-quote-right fa1"></i><div data-v-5519711e=""><p data-v-5519711e="">Just back from EMNLP2023 and my mind's still reeling! Witnessed NLP's seismic shift firsthand through daring papers and provocative posters that are challenging everything we thought we knew. Check out my take on the conference's boldest ideas.</p></div></div></div></div><div data-v-5519711e="" class="row justify-center items-center q-mt-lg"><div data-v-5519711e="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-363418f9="" data-v-5519711e="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-363418f9="" class="relative-position" style="height: 30px; width: 55px;"><div data-v-363418f9="" class="q-avatar bg-grey-9 overlapping" style="font-size: 30px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-363418f9="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div><div data-v-363418f9="" class="q-avatar bg-grey-9 overlapping" style="font-size: 30px; left: 25px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-363418f9="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div><div data-v-5519711e="" class="q-item__section column q-item__section--main justify-center text-grey-6"><div data-v-5519711e="" class="q-item__label">Han Xiao, Michael Günther</div><div data-v-5519711e="" class="q-item__label q-item__label--caption text-caption text-grey-6 q-mt-sm">December 16, 2023 • 17 minutes read</div></div></div></div><article data-v-5519711e="" class="article"><section data-v-5519711e="" class="gh-content"><p>The sun blazed down on the glistening sidewalks of Sentosa, a symphony of laughter and chatter filling the air. Tourists, decked in their holiday best, meandered through the vibrantmaze of Universal Studios, their faces alight with the joy of a day out in this fantasy land. The click of cameras capturing moments against the backdrop of thrilling rides and colorful parades was omnipresent. Nearby, the enticing aromas of international cuisines wafted through the air, luring guests to indulge in a culinary adventure.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg" class="kg-image" alt="" width="1440" height="1080" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2911702641285_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2911702641285_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2911702641285_.pic.jpg 1440w" sizes="(min-width: 720px) 720px"></figure><p>In stark contrast, just a stone's throw away, nestled in the heart of this revelry, was the Resorts World Convention Centre. Here, the atmosphere was charged with a different kind of excitement. The halls buzzed not with the sound of holiday-making, but with the fervor of intellectual discourse. Young researchers and seasoned academics, their conference badges swaying gently with each step, engaged in animated discussions about the latest in NLP.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/image-3.png" class="kg-image" alt="" width="2000" height="1500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/image-3.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/image-3.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/image-3.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/image-3.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>As I walked into this place, the difference was clear. On one side, there was the loud and happy noise of a holiday resort, full of life and excitement. On the other side, there was a serious and busy atmosphere where people talked about NLP , LLMs, ChatGPT, prompting, and Google's new Gemini model. It was like a movie scene – where learning and fun came together in a surprising way.</p><p>In this blog, I'll share some observations of EMNLP 2023 and some of the most interesting papers and posters we discovered at the conference.</p><h2 id="emnlp-2022-to-2023-shifts">EMNLP: 2022 to 2023 Shifts</h2><p>Attending EMNLP 2022 in Abu Dhabi and now looking back at EMNLP 2023, I've observed significant shifts in research focus and conference dynamics. These changes, driven by the swift advancements in AI, paint a vivid picture of our adaptive and forward-looking community. Below, I share a comparison of the two conferences, highlighting how our priorities and discussions have transformed in just a year.</p>

<table>
<thead>
<tr>
<th>EMNLP</th>
<th>2022</th>
<th>2023</th>
</tr>
</thead>
<tbody>
<tr>
<td>Main Research Focus</td>
<td>Diverse range of NLP methods, with emphasis on traditional approaches.</td>
<td>Strong focus on Large Language Models (LLMs) and prompting techniques.</td>
</tr>
<tr>
<td>Research Trends</td>
<td>Interest in a wide array of topics, but no standout groundbreaking papers.</td>
<td>Shift towards LLM interpretability, ethics, agents, and multimodal reasoning.</td>
</tr>
<tr>
<td>Conference Atmosphere</td>
<td>A bit peculiar and pessimistic due to the release of ChatGPT and its implications on traditional NLP methods.</td>
<td>More confidence and adaptability among researchers in embracing new trends.</td>
</tr>
<tr>
<td>Research Diversity</td>
<td>Still exploring traditional methods like topic modeling, n-grams smoothing, and Bayesian methods (as seen in COLING 2022).</td>
<td>Rapid adaptation to newer approaches, moving away from older methods.</td>
</tr>
<tr>
<td>Relevance of Presented Work</td>
<td>Consistent with contemporary research trends at the time.</td>
<td>Fast-paced AI development made some empirical methods and results feel outdated by the time of the conference.</td>
</tr>
<tr>
<td>Conference Engagement</td>
<td>Enjoyment derived more from personal conversations and interactions than from paper presentations.</td>
<td>Increased focus on personal communication, with more time spent at poster sessions than listening to oral presentations.</td>
</tr>
</tbody>
</table>

<h2 id="paper-highlights-from-emnlp-2023">Paper Highlights from EMNLP 2023</h2><p>At EMNLP 2023, several intriguing papers caught my attention, each addressing different aspects of NLP and pushing the boundaries of what's possible in this field. Let me share some of the highlights from these papers and my thoughts on them.</p><h3 id="hybrid-inverted-index-is-a-robust-accelerator-for-dense-retrieval">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.05521?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval</div><div class="kg-bookmark-description">Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI$^2$ to achieve lossless retrieval quality with competitive efficiency across various index settings. Our code and checkpoint are publicly available at https://github.com/namespace-Pt/Adon/tree/HI2.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt=""><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Peitian Zhang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt=""></div></a></figure><p>Text embeddings have become very popular for information retrieval tasks. However, performing an exact embedding vector search requires one to calculate similarities between the embedding representation of the query and the embedding of each document. This becomes very slow for large datasets and leads to latencies that are not acceptable for real-world search applications. Therefore, many applications use approximated nearest neighbor search techniques to speed up the search system, whereby many of these techniques rely on vector quantization algorithms that learn an index of clusters based on the data distribution.&nbsp;</p><p>In addition, <strong>hybrid search</strong> has become popular which combines embedding-based search with traditional BM25-based search techniques. Usually, BM25 and embedding search are performed completely independently in hybrid search settings and only the result sets are combined.&nbsp;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/hype-and-hybrids-multimodal-search-means-more-than-keywords-and-vectors-2/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hype and Hybrids: Search is more than Keywords and Vectors</div><div class="kg-bookmark-description">Twenty years ago, “hybrid” was a term used only by botanists and chemists. Today, hybrid is booming… even in search. Many search systems are rolling out hybrid search schemes with the latest AI. But is “hybrid search” really more than a buzzword?</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt=""><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/Jina-AI-Website-Banners-Templates--21-.png" alt=""></div></a></figure><p>This paper proposed a method to train a joined index of two parts: a cluster selector and a term selector. The cluster selector performs a vector quantization to assign texts into buckets of nearby clusters and the term selector determines the most representative terms of a document BM25 can be used to assign it into buckets associated with those terms, however, this is not trainable and can therefore not adjust to the training data. Alternatively, one can determine the most representative terms in a document with a BERT model with an MLP hat which is applied on each token to determine a score. In this way the term selector becomes trainable. Then the cluster centroids and the BERT model are trained together by using the KL divergence loss with the embedding model as a teacher to obtain a distribution of similarity values. The results in the paper show that this method can retrieve more relevant documents in the same amount of time as standard ANN techniques like HNSW and IVF-PQ implementations.</p><h3 id="is-chatgpt-good-at-search-investigating-large-language-models-as-re-ranking-agents">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2304.09542?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents</div><div class="kg-bookmark-description">Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model’s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt=""><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Weiwei Sun</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt=""></div></a></figure><p>This paper investigates techniques to utilize LLMs for re-ranking documents. Re-ranking is usually performed in a search system after a first retrieval step to re-order the retrieved documents, e.g., to select the most relevant ones among them. Commonly used models are finetuned transformer models which are called <strong>cross encoders</strong>. Those receive as input a pair composed of a query and a document candidate and return a relevance score. Besides, more traditional learning-to-rank models like LambdaMart are also popular, especially in cases where the ranking is not only done based on semantic relevance itself.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.sbert.net/examples/applications/cross-encoder/README.html?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Cross-Encoders — Sentence-Transformers documentation</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://www.sbert.net/_static/favicon.ico" alt=""></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://www.sbert.net/_static/logo.png" alt=""></div></a></figure><p>Observing the strong NLP capabilities of LLMs, the authors of this paper wanted to investigate whether Models like GPT4 can be used to rank documents better. However, the limitation of those closed API-based models is usually that probability outputs are not accessible. Accordingly, the paper investigates techniques that rely only on prompting and the generated output text for re-ranking. The technique they propose inserts the documents together with an ID in the prompt and instructs the LLM to output a sequence of IDs with respect to the relevancy of the documents. In cases where the whole set of documents is too long to fit into the prompt, they apply a sliding window approach, where re-ranking is first performed on the documents with the lowest retrieval score obtained from the first-stage retriever. Then the most relevant documents according to the output are presented together with the documents in the next window of retrieval candidates and so on.</p><p>Since GPT-4 is too expensive and too slow for using it in a real-world setting, the authors propose distilling its ranking capabilities into a transformer-based cross-encoder model. The results show that even a comparably small model (440M parameters) distilled with this technique can outperform much larger state-of-the-art re-ranking models.</p><h3 id="large-language-models-can-self-improve">Large Language Models Can Self-Improve</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2210.11610?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Large Language Models Can Self-Improve</div><div class="kg-bookmark-description">Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-confidence” rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and 63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt=""><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Jiaxin Huang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt=""></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-us.googleusercontent.com/qRfOSz3zHzg1gctMzgRjYRjeTyg-1pMyv7JIZZFJlTyKOQPoHJYfuB0u_eGC8wVnCKvN471-8D-avjk7-0XBGfAAFL7q0jI_-7eMGnXvCpEK8L8Pe3VSu5-iV5QHpfnY-5YSUQqvgT-gUs2sOUoQw0w" class="kg-image" alt="" width="624" height="236"></figure><p>For many tasks, LLMs achieve already good results in zero-shot settings. However, to improve their performance on specific tasks (beyond zero-shot) common techniques still utilize a lot of training data to fine-tune them. To reduce the amount of training data needed, this paper presents a technique that uses the LLM to “self-improve”. The main idea behind this approach is to augment existing training datasets by using data generated with the LLM itself.</p><p>This is achieved by using a dataset with only questions and without answers. First, a chain of thoughts (CoT) method is used to generate for each question a couple of different reasoning paths and answers by using a temperature greater than zero to make the generative text non-deterministic. By determining the answer with the highest frequency the probability of the answer being correct can be increased. The paper also shows that LLMs can be used to effectively estimate the confidence of an answer by investigating the consistency of the answer. If a high percentage of the answers is equal, this answer also has a high probability being correct. These most frequent answers and the corresponding reasoning paths can then be used to construct additional prompts for constructing additional training data. However, the authors propose not only to use those reasoning paths and answers but also to add examples in different formats, e.g., presenting the question without any reasoning path and using the question together with some generic instruction like “Let’s think step by step”. Finally, this enhanced training dataset can be used to fine-tune the LLM on the specific task. In their evaluation the authors show this technique is effective to fine-tune LLMs with minimal training data but also that it generalizes well, as it improves the result on out-of-domain datasets.</p><h3 id="adapting-language-models-to-compress-contexts">Adapting Language Models to Compress Contexts</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2305.14788?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Adapting Language Models to Compress Contexts</div><div class="kg-bookmark-description">Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt=""><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Alexis Chevalier</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt=""></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-us.googleusercontent.com/RDaQqjnEa45QEArhdPfuNKXsZReEQfT7Hg_zgr-DlkmlMeeXJAYP0kiKfpQ3Vj8iL_I2mK5JZ0evaoHZAdBDHSaep9Z49Qx1dt5JdCcMrfH8UF-nRy_ZO6GT7wLwR654p0_9CF4jD7soGg-zBXbSFNg" class="kg-image" alt="" width="624" height="496"></figure><p>Language models are usually constrained by limited context length. While there are various techniques like <strong>AliBi</strong> to construct language models that can handle larger context lengths, those techniques do not help in cases where you want to use an existing model that has only a very limited context length. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2108.12409?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</div><div class="kg-bookmark-description">Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi’s inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt=""><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Ofir Press</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt=""></div></a></figure><p>Instead, this paper addresses this problem by introducing a new method that allows one to fine-tune an exciting language model to apply it to longer context lengths. To achieve this, the model's vocabulary is extended by a fixed number of summary tokens, which are supposed to tell the model to summarize the content of the other tokens in the respective embeddings. During inference, the text input is segmented into shorter text sequences which are extended with the summary tokens and prepended with the output summary vectors of the previously processed segments. To fine-tune the model to handle long sequences, the model is trained on a next-token prediction task. Thereby, the model should make use of the information encoded in the summary embeddings of the previous segments. Notably, the segment length is varied during the training and the backpropagation is done for one document as a whole. In other words, the model is first applied to all sequences in the afore-described scheme before starting the backpropagation. The technique is evaluated by the authors on OPT models of various sizes and the 7-B-Llama-2 model. Besides standard language modeling tasks, the authors also show that the models can be effectively used to solve in-context-learning classification tasks with longer prompts or used for re-ranking. Here the re-ranking follows a language modeling approach in which the passages are re-ranked based on the language model’s likelihood to generate the question from the given passage.&nbsp; </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.emnlp-main.249/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Improving Passage Retrieval with Zero-Shot Question Generation</div><div class="kg-bookmark-description">Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, Luke Zettlemoyer. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt=""><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.emnlp-main.249.jpg" alt=""></div></a></figure><h2 id="poster-highlights-from-emnlp-2023">Poster Highlights from EMNLP 2023</h2><p>At EMNLP 2023, alongside the compelling paper presentations, the poster sessions were a hub of vibrant discussion and exchange. Here's a rundown of some standout posters I came across, each offering a unique glimpse into the ongoing research and development within the field of NLP.</p><h3 id="can-retriever-augmented-language-models-reason"><strong>Can Retriever-Augmented Language Models Reason?</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2931702707518_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2931702707518_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2931702707518_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A poster from McGill University examined whether Retriever-Augmented Language Models (RALMs) can effectively reason by balancing the capabilities of both the retriever and the language model. The research highlighted the potential shortcomings of retrievers in sourcing all necessary statements for reasoning, and how language models might falter in reasoning even when provided with the required statements. It was a deep dive into improving the interactive components of language models.</p><h3 id="contrastive-learning-based-sentence-encoders"><strong>Contrastive Learning-based Sentence Encoders</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2941702707520_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2941702707520_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2941702707520_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Researchers from Tohoku University presented findings on how Contrastive Learning (CL) can induce sentence encoders to implicitly weight informative words, enhancing the model's understanding and processing of language. This approach could refine the way sentence encoders prioritize and process key elements in text, making them more efficient and effective.</p><h3 id="investigating-semantic-subspaces-of-transformer-sentence-embeddings"><strong>Investigating Semantic Subspaces of Transformer Sentence Embeddings</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2951702707522_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2951702707522_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2951702707522_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>A team from the University of Stuttgart showcased their work on probing the semantic subspaces of transformer sentence embeddings. By employing linear structural probing, they aimed to understand how different layers of a transformer contribute to semantic content processing, offering insights into the inner workings of sentence embeddings.</p><h3 id="can-pre-trained-vision-and-language-model-answer-visual-information-seeking-questions"><strong>Can Pre-trained Vision and Language Model Answer Visual Information-Seeking Questions?</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2961702707523_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2961702707523_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2961702707523_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>An intriguing poster by researchers from the Georgia Institute of Technology, Google Research, and Google DeepMind introduced a benchmark for testing the world knowledge in multimodal Large Language Models (LLMs) through Visual Information-Seeking Questions. The research focused on the capabilities of retrieval-augmented models and GPT-4 in answering questions that require visual understanding, pushing the envelope on multimodal AI.</p><h3 id="to-split-or-not-to-split-composing-compounds-in-contextual-vector-spaces"><strong>To Split or Not to Split: Composing Compounds in Contextual Vector Spaces</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2971702707525_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2971702707525_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2971702707525_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>From the University of Stuttgart, a study delved into whether splitting compounds in contextual vector spaces is beneficial for the model's performance. The research explored the impact of compounds on semantic representation and processing, contributing to our understanding of compositional semantics in language models.</p><h3 id="subspace-chronicles-how-linguistic-information-emerges-shifts-and-interacts-during-language-model-training"><strong>Subspace Chronicles: How Linguistic Information Emerges, Shifts, and Interacts during Language Model Training</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2981702707526_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2981702707526_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2981702707526_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>This poster detailed an exploration into the dynamics of linguistic information as it emerges and evolves during the training of language models. It's a fascinating look at the underpinnings of language model training and the critical learning phases that define their capabilities.</p><h3 id="theory-of-mind-for-multi-agent-collaboration-via-large-language-models"><strong>Theory of Mind for Multi-Agent Collaboration via Large Language Models</strong></h3><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg" class="kg-image" alt="" width="1280" height="1707" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/2991702707528_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/2991702707528_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/2991702707528_.pic.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><p>Lastly, a poster outlined research on the Theory of Mind in Large Language Models and their application in multi-agent collaboration tasks. It's an exciting foray into the cognitive capabilities of LMs and their potential in collaborative environments.</p><h2 id="embeddings-roundtable-a-birds-of-a-feather-at-emnlp-2023">Embeddings Roundtable: A Birds of a Feather at EMNLP 2023</h2><p>During EMNLP 2023, we hosted a Birds of a Feather (BoF) session on embeddings that turned into a rich tapestry of insights and discussions. With a crowd of over 80 attendees, the session was an electrifying blend of sharp minds and cutting-edge topics.</p><figure class="kg-card kg-video-card kg-width-regular" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg" data-kg-custom-thumbnail="">
            <div class="kg-video-container">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691.mp4" poster="https://img.spacergif.org/v1/1280x720/0a/spacer.png" width="1280" height="720" playsinline="" preload="metadata" style="background: transparent url('https://jina-ai-gmbh.ghost.io/content/media/2023/12/307_1702708691_thumb.jpg') 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"></path>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"></path>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"></rect>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"></rect>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">0:09</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1×</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"></path>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"></path>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            
        </figure><h3 id="lightning-talks-and-panel-discussion">Lightning Talks and Panel Discussion</h3><p>The BoF session featured lightning talks by renowned researchers like Huiqiang, Hassan, Hwiyeol, Mattia, and Yang Chen. Each speaker brought a unique perspective to the table, sharing their latest findings in embedding research within NLP. The talks sparked an energizing dialogue that transitioned into a thought-provoking panel discussion.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 1.33386 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg" width="1702" height="1276" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3041702708673_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3041702708673_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3041702708673_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3041702708673_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image" style="flex: 1.33386 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg" width="1702" height="1276" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3031702708669_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3031702708669_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3031702708669_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3031702708669_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image" style="flex: 1.33386 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg" width="1702" height="1276" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3061702708681_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3061702708681_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3061702708681_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3061702708681_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 1.33386 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg" width="1702" height="1276" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3051702708676_.pic-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3051702708676_.pic-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3051702708676_.pic-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3051702708676_.pic-1.jpg 1702w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image" style="flex: 1.33386 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg" width="1702" height="1276" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3081702708695_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3081702708695_.pic.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3081702708695_.pic.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3081702708695_.pic.jpg 1702w" sizes="(min-width: 720px) 720px"></div></div></div></figure><p>The panel, graced by Sebastian Ruder, Nicola Cancedda, Chia Ying Lee, Michael Günther, and Han Xiao, delved deep into the intricacies of embedding technologies. They covered a breadth of topics, from the evolution of embeddings to their future in a world increasingly dominated by Generative AI and Large Language Models (LLMs).</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg" class="kg-image" alt="" width="720" height="541" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3011702708364_.pic.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/2023/12/3011702708364_.pic.jpg 720w" sizes="(min-width: 720px) 720px"></figure><h3 id="key-takeaways-from-the-panel">Key Takeaways from the Panel</h3><ol><li><strong>Diverse Perspectives on Embeddings:</strong><br>The panelists introduced themselves and their work with various embeddings, discussing the common threads and divergences they've observed. They emphasized the nuanced differences in how embeddings behave depending on their design and application contexts.</li><li><strong>The Relevance of Embeddings Amidst Generative AI:</strong><br>With 2023's spotlight on LLMs, the panelists reflected on the enduring importance of embeddings. They highlighted that despite the LLM trend, embeddings retain a crucial role in understanding and processing language at a more granular level.</li><li><strong>Context Length in Embeddings vs. LLMs:</strong><br>A curious observation was the disparity in context length expansion between LLMs and embedding models. The panelists shed light on the technical and practical constraints that currently limit the context window in embedding models.</li><li><strong>Search and Generation:</strong><br>Addressing the assertion that 'search is an overfitted generation, and generation is an underfitted search,' the panelists shared mixed views, sparking a lively debate on the interplay between search functions and generative capabilities.</li><li><strong>Future of RAG and Agent Models:</strong><br>Looking towards EMNLP 2024, the conversation turned to the prospective challenges and developments in Retrieval Augmented Generation (RAG) and agent models. The panelists hinted at their vision for the future integration of embeddings within these applications, recognizing the pivotal role they will continue to play.</li></ol><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 0.750751 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3101702709471_.pic_hd-1.jpg" width="2000" height="2664" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3101702709471_.pic_hd-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3101702709471_.pic_hd-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3101702709471_.pic_hd-1.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3101702709471_.pic_hd-1.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image" style="flex: 0.750751 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/3091702709471_.pic_hd.jpg" width="2000" height="2664" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/12/3091702709471_.pic_hd.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/12/3091702709471_.pic_hd.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/12/3091702709471_.pic_hd.jpg 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2023/12/3091702709471_.pic_hd.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div></div></figure><h2 id="summary">Summary</h2><p>Wrapping up EMNLP 2023, I'm buzzing with ideas and energized by the community's shared passion for pushing the boundaries of NLP. Our Embeddings BoF session was a hit – the engagement and insights made it a highlight for me.</p><p>Looking to get hands-on with the future of embeddings? We are hiring! We're all about diving deep into long-context, multilingual, and multimodal embeddings. So, if you're up for the challenge, check out the open roles here and maybe I'll see you at our Berlin, Shenzhen, or Beijing office.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/internship/?ref=jina-ai-gmbh.ghost.io"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Intern Program</div><div class="kg-bookmark-description">Worldwide call for students: Intern in research, engineering, marketing, sales and more to pioneer multimodal AI together.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt=""></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-internship.png" alt=""></div></a></figure><p>Can't wait to see what we'll cook up by EMNLP 2024 in Miami. Until then, keep innovating, keep questioning, and let's keep the conversations going!</p></section></article><hr data-v-5519711e="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-5519711e="" class="row justify-between items-center q-py-md"><div data-v-5519711e=""><span data-v-5519711e="" class="text-weight-bold">Categories:</span><span data-v-5519711e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Events</div></div></div></span></div><div data-v-5519711e=""><div data-v-5519711e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-5519711e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fnews%2Fa-tale-of-two-worlds-emnlp-2023-at-sentosa%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-5519711e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fnews%2Fa-tale-of-two-worlds-emnlp-2023-at-sentosa%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-5519711e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fnews%2Fa-tale-of-two-worlds-emnlp-2023-at-sentosa%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-5519711e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fnews%2Fa-tale-of-two-worlds-emnlp-2023-at-sentosa%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-5519711e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fnews%2Fa-tale-of-two-worlds-emnlp-2023-at-sentosa%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><button data-v-5519711e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-icons" aria-hidden="true" role="img">rss_feed</i></span></button></div></div></div><hr data-v-5519711e="" class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><div data-v-5519711e="" class="text-h6 text-grey-2 text-weight-bold q-mt-lg">Learn more</div><a data-v-5519711e="" class="q-card q-card--dark q-dark q-card--flat no-shadow block cursor-pointer q-hoverable non-selectable q-my-md" href="/news/discover-the-latest-in-embeddings-at-emnlp-2023-in-person-bof-session" style="overflow: hidden; text-decoration: none !important;"><span class="q-focus-helper"></span><div class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div class="q-card__section q-card__section--vert column justify-between"><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label text-h5 text-weight-light q-mb-lg" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Discover the Latest in Embeddings at EMNLP 2023 In-Person BoF Session</div><div class="q-item__label q-item__label--caption text-caption" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 3;">Mark your calendars for an immersive BoF session on Embeddings at EMNLP 2023. Join us on December 9th from 11:00 AM to 12:30 PM, Singapore time, in the 'Aquarius 1' room for a deep dive into the latest embeddings.</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-363418f9="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-363418f9="" class="relative-position" style="height: 30px; width: 30px;"><div data-v-363418f9="" class="q-avatar bg-grey-9 overlapping" style="font-size: 30px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-363418f9="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Jina AI</div><div class="q-item__label q-item__label--caption text-caption">December 01, 2023 • 2 minutes read</div></div></div></div><div class="q-img q-img--menu col-4" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/12/Explore-image-storytelling-beyond-pixels--25--1.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></a><a data-v-5519711e="" class="q-card q-card--dark q-dark q-card--flat no-shadow block cursor-pointer q-hoverable non-selectable q-my-md" href="/news/give-and-take-in-fdps-ai-geopolitics-panel" style="overflow: hidden; text-decoration: none !important;"><span class="q-focus-helper"></span><div class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div class="q-card__section q-card__section--vert column justify-between"><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label text-h5 text-weight-light q-mb-lg" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Give and Take in FDP's AI Geopolitics Panel</div><div class="q-item__label q-item__label--caption text-caption" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 3;">This blog post captures my personal reflections and contributions to the panel, highlighting key differences in AI strategies among global powers like the US, China, and the EU.</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-363418f9="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-363418f9="" class="relative-position" style="height: 30px; width: 30px;"><div data-v-363418f9="" class="q-avatar bg-grey-9 overlapping" style="font-size: 30px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-363418f9="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Han Xiao</div><div class="q-item__label q-item__label--caption text-caption">November 29, 2023 • 7 minutes read</div></div></div></div><div class="q-img q-img--menu col-4" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/11/Explore-image-storytelling-beyond-pixels--23-.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></a><a data-v-5519711e="" class="q-card q-card--dark q-dark q-card--flat no-shadow block cursor-pointer q-hoverable non-selectable q-my-md" href="/news/jina-ap-virtual-ticket-giveaway-for-pycon-2023" style="overflow: hidden; text-decoration: none !important;"><span class="q-focus-helper"></span><div class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div class="q-card__section q-card__section--vert column justify-between"><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label text-h5 text-weight-light q-mb-lg" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Jina AI Virtual Ticket Giveaway for PyCon 2023 Online</div><div class="q-item__label q-item__label--caption text-caption" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 3;">Jina AI's giving away PyCon US Online passes. Deadline April 10.</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-363418f9="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-363418f9="" class="relative-position" style="height: 30px; width: 30px;"><div data-v-363418f9="" class="q-avatar bg-grey-9 overlapping" style="font-size: 30px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-363418f9="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/03/avatar.jpg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Jocelyn Matthews</div><div class="q-item__label q-item__label--caption text-caption">April 03, 2023 • 1 minutes read</div></div></div></div><div class="q-img q-img--menu col-4" role="img"><div style="padding-bottom: 56.25%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--waiting" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/04/virtual-ticket-promo.png" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></a></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow overflow-hidden print-hide"><video autoplay="" loop="" playsinline="" class="non-selectable" poster="/assets/portal-2-poster.44997122.webp" style="width: 100%; height: 420px; object-fit: cover;"><source src="/assets/portal-2.0da13c26.mp4" type="video/mp4"></video><div class="q-card q-card--dark q-dark q-card--flat no-shadow row absolute-left items-center bg-transparent non-selectable fit"><div class="text-white bg-transparent lock-blur fit q-pa-sm"><div class="row justify-between q-gutter-lg"><div class="col-12 col-md-3"><div class="q-list q-list--dark"><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><strong>Offices</strong></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation" style="font-size: 32px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlin, Germany (HQ)</div><div class="q-item__label q-item__label--caption text-caption">Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany</div><div class="q-item__label q-item__label--caption text-caption">Geschäftsanschrift: Leipziger str. 96, 10117 Berlin, Germany</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation" style="font-size: 32px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Beijing, China</div><div class="q-item__label q-item__label--caption text-caption">Level 5, Building 6, No.48 Haidian West St. Beijing Haidian, China</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-icons" aria-hidden="true" role="presentation" style="font-size: 32px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, China</div><div class="q-item__label q-item__label--caption text-caption">402, Floor 4, Fu'an Technology Building, Shenzhen Nanshan, China</div></div></div></div></div></div><div class="row text-caption justify-center q-mt-xl"> © Jina AI GmbH 2020-2024. All rights reserved.</div></div></div></div><footer class="q-footer q-layout__section--marginal fixed-bottom lock-blur bg-transparent print-hide"><div class="row"><div class="q-btn-group row no-wrap q-btn-group--flat inline col-12 col-md-6 justify-center"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-discord" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></span></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://twitter.com/jinaAI_/" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" type="button" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://www.youtube.com/c/JinaAI" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-youtube" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="https://www.meetup.com/jina-community-meetup/" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fa-brands fa-meetup" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px; padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-icons" aria-hidden="true" role="img">email</i></span></a></div><div class="row col-12 col-md-6 items-center text-caption justify-center"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--stretch inline" no-caps=""><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--no-uppercase no-border-radius self-stretch text-caption" tabindex="0" href="/legal/#privacy-policy" style="padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="block">Privacy Policy</span></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--no-uppercase no-border-radius self-stretch text-caption" tabindex="0" href="/legal/#terms-and-conditions" style="padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="block">Terms and Conditions</span></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--no-uppercase no-border-radius self-stretch text-caption" tabindex="0" href="javascript:UC_UI.showSecondLayer();" style="padding: 8px; min-width: 0px; min-height: 0px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="block">Privacy Settings</span></span></a></div></div></div></footer></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
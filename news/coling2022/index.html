<!DOCTYPE html><html translate="no" dir="ltr" lang="en-US"><head><title>COLING2022 Summary on Multimodal AI</title><meta charset="utf-8"><meta name="title" content="COLING2022 Summary on Multimodal AI"><meta name="description" content="Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. In this article, I will review the multimodal AI-related work in COLING 2022."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/coling2022"><meta property="og:title" content="COLING2022 Summary on Multimodal AI"><meta property="og:description" content="Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. In this article, I will review the multimodal AI-related work in COLING 2022."><meta property="og:image" content="https://jina.ai/blog-banner/coling2022.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/coling2022"><meta property="twitter:title" content="COLING2022 Summary on Multimodal AI"><meta property="twitter:description" content="Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. In this article, I will review the multimodal AI-related work in COLING 2022."><meta property="twitter:image" content="https://jina.ai/blog-banner/coling2022.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-Bobc0DvW.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-4QmF1J8i.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-CAWQxWHK.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-DecUubaf.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-Df8riurI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-D7u7kkHN.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-B3EnG0a0.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BBTILXe9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-Dw_XJG0P.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-FgVWuitm.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-saY5HJnN.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-xURacPmp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-i3tqqfgJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-qcAmDw9Z.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-BTIu-Lt9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-CvV4iax9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-C7wbelex.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-BEekXcB-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-DoRdumDx.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-0tLQOvu4.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-BMvnn5-t.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-qGHhBDc2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-Df612yUv.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DgKyog8e.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-BBWAlFro.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-D7LixXMA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-Bdcs1rFT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-CbXT0h-e.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-BglxL5Ck.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-DLQZRArS.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-CL5zlZxa.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-CSo8Iu1S.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-DpyQcgUF.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-DTwn6_CN.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-Byvdl9vC.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-C1vDmaBY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Han Xiao"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Han Xiao"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="12 mins read"><meta property="article:published_time" content="2022-10-21T11:58:17.000+02:00"><meta property="article:modified_time" content="2024-01-24T19:32:06.000+01:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "COLING2022 Summary on Multimodal AI",
  "description": "Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. In this article, I will review the multimodal AI-related work in COLING 2022.",
  "image": [
    "https://jina.ai/blog-banner/coling2022.webp"
  ],
  "datePublished": "2022-10-21T11:58:17.000+02:00",
  "dateModified": "2024-01-24T19:32:06.000+01:00",
  "author": [
    {
      "@type": "Person",
      "name": "Han Xiao",
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">News</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Models</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_a69756ed-317f-4ffa-b2b3-7b2df78aa1c5" aria-label="Expand &quot;Products&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Products</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_a69756ed-317f-4ffa-b2b3-7b2df78aa1c5" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Products"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DeepSearch</div><div class="q-item__label q-item__label--caption text-caption">Search, read and reason until best answer found.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reader</div><div class="q-item__label q-item__label--caption text-caption">Convert any URL to Markdown for better grounding LLMs.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Embeddings</div><div class="q-item__label q-item__label--caption text-caption">World-class multimodal multilingual embeddings.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reranker</div><div class="q-item__label q-item__label--caption text-caption">World-class reranker for maximizing search relevancy.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_db2c6eda-8d4c-4d09-9e3e-a0aa6a470029" aria-label="Expand &quot;More&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">More</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_db2c6eda-8d4c-4d09-9e3e-a0aa6a470029" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classifier</div><div class="q-item__label q-item__label--caption text-caption">Zero-shot and few-shot classification for image and text.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmenter</div><div class="q-item__label q-item__label--caption text-caption">Cut long text into chunks and do tokenization.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">API Docs</div><div class="q-item__label q-item__label--caption text-caption">Auto codegen for your copilot IDE or LLM</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_0f8a1276-abf5-48cd-b824-f9602c4fbb9f" aria-label="Expand &quot;Company&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Company</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_0f8a1276-abf5-48cd-b824-f9602c4fbb9f" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">About us</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Join us</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Download logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Log in"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Log in</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_352da6e0-a8d7-4972-801c-bc8acb0c2888"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_352da6e0-a8d7-4972-801c-bc8acb0c2888" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_352da6e0-a8d7-4972-801c-bc8acb0c2888_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Are Visual-Linguistic Models Commonsense Knowledge Bases?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">In-the-Wild Video Question Answering</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Multi-Task Learning of T5 Model with I3D Features</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Other Interesting Multimodal Research</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Summary</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Event</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">October 21, 2022</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">COLING2022 Summary on Multimodal AI</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. In this article, I will review the multimodal AI-related work in COLING 2022.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img" aria-label="Group of students posing together at the COLING 2022 event, projecting a sense of camaraderie"><div style="padding-bottom: 50%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Group of students posing together at the COLING 2022 event, projecting a sense of camaraderie" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Jina-AI-Website-Banners-Templates--7-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Han Xiao • 12 minutes read</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>Last week I was in Gyeongju, Korea for the COLING2022 conference. COLING is the premier conference in computational linguistics held every two years. </p><p>The conference was very well organized, and the talks were very interesting. I learned a lot about the latest research in computational linguistics and NLP, such as automated writing evaluation and multi-hop question answering. I also had a chance to meet many old friends and make new ones at the Jina AI dinner event. </p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 1.33333 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9203.jpg" width="1600" height="1200" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9203.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9203.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9203.jpg 1600w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div><div class="kg-gallery-image" style="flex: 1.33333 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9214-1.jpg" width="1600" height="1200" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9214-1.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9214-1.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9214-1.jpg 1600w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div><div class="kg-gallery-image" style="flex: 0.750117 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9222.jpg" width="1600" height="2133" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9222.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9222.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9222.jpg 1600w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 0.75 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9291.jpg" width="1200" height="1600" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9291.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9291.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9291.jpg 1200w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div><div class="kg-gallery-image" style="flex: 1.33333 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9295.jpg" width="1600" height="1200" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9295.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9295.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9295.jpg 1600w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div><div class="kg-gallery-image" style="flex: 1.33333 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9173.jpg" width="1600" height="1200" alt="" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/IMG_9173.jpg 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/IMG_9173.jpg 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/IMG_9173.jpg 1600w" sizes="(min-width: 720px) 720px" style="cursor: help;"></div></div></div><figcaption><p><span style="white-space: pre-wrap;">Dinner event hosted by Jina AI at COLING2022</span></p></figcaption></figure><p>In this article, I will review the multimodal AI-related work presented at COLING 2022. Despite being primarily an NLP conference, there were 26 presentations focused on multimodal AI, covering text-image, text-video, and text-speech domains. I would like to highlight three presentations that I found particularly interesting.</p><h2 id="are-visual-linguistic-models-commonsense-knowledge-bases" style="position: relative;"><a href="#are-visual-linguistic-models-commonsense-knowledge-bases" title="Are Visual-Linguistic Models Commonsense Knowledge Bases?" id="anchor-are-visual-linguistic-models-commonsense-knowledge-bases"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Are Visual-Linguistic Models Commonsense Knowledge Bases?</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.491/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Are Visual-Linguistic Models Commonsense Knowledge Bases?</div><div class="kg-bookmark-description">Hsiu-Yu Yang, Carina Silberer. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.491.jpg" alt="" style="cursor: help;"></div></a></figure><p>We have all seen the huge potential for pretrained language models (PTLMs) over the last few years. Models such as Transformer and GPT have been widely used in many downstream tasks due to their potential as a commonsense knowledge base. However, the text corpora used to train PTLMs can make them very biased, and this can make them inconsistent and not very robust. In this paper, Yang and Silberer suggest that text corpora alone may be insufficient for knowledge acquisition and raise important questions for visual-linguistic models like UNITER, VILBERT, and CLIP:</p><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">🤔</div><div class="kg-callout-text">Can a language model with a visual component represent our physical world better?</div></div><p>They address this problem with probing questions like this one:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-8.png" class="kg-image" alt="Educational slide with a quiz asking where a mouse is likely to be found, depicting an attic bedroom, a bridge over the ocean, and a forest" width="704" height="350" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-8.png 704w" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Prompt-based zero-shot QA with visual input. QA pairs can be categorized into multiple commonsense dimensions to identify what sort of relational knowledge the model has acquired: part-whole, taxonomic, distinctness, similarity, quality, utility, creation, temporal, spatial, and desire. During experiments, the author selectively masks out image input to study the what commonsense knowledge the models have captured.</span></figcaption></figure><h3 id="which-dimensions-of-commonsense-do-visual-linguistic-models-possess-compared-to-text-only-ptlms" style="position: relative;"><a href="#which-dimensions-of-commonsense-do-visual-linguistic-models-possess-compared-to-text-only-ptlms" title="Which dimensions of commonsense do visual-linguistic models possess compared to text-only PTLMs?" id="anchor-which-dimensions-of-commonsense-do-visual-linguistic-models-possess-compared-to-text-only-ptlms"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Which dimensions of commonsense do v<strong>isual-linguistic</strong> models possess compared to text-only PTLMs?</h3><p>Hsui-Yu shows that visual-linguistic (VL) models such as UNITER and VILBERT are better at <em>part-whole</em>, <em>spatial</em>, and <em>desire</em> dimensions but struggle with <em>taxonomic</em>, <em>distinctness,</em> and <em>temporal</em> dimension. In terms of <em>spatial </em>dimension, VL models consistently outperform RoBERTa by 8%. On non-visual related dimensions such as <em>taxonomic </em>and <em>temporal, </em>VL models underperform by 8%, which is understandable as taxonomy and temporality are rarely represented in images. When using real-world images for training VL, <em>distinctness </em>is also hard to learn because opposite concepts e.g. <em>flood </em>and <em>drought</em>, rarely come together in one picture.</p><h3 id="during-pretraining-does-explicit-visual-information-ie-images-benefit-commonsense-knowledge-encoding" style="position: relative;"><a href="#during-pretraining-does-explicit-visual-information-ie-images-benefit-commonsense-knowledge-encoding" title="During pretraining, does explicit visual information (i.e., images) benefit commonsense knowledge encoding?" id="anchor-during-pretraining-does-explicit-visual-information-ie-images-benefit-commonsense-knowledge-encoding"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>During pretraining, does explicit visual information (i.e., images) benefit commonsense knowledge encoding? </h3><p>When adding a BERT pretrained with image captions, the advantage of VL models becomes less significant, even on the <em>spatial </em>dimension. This shows that image captions can already provide a good proxy for visual information. The only dimension that VL models outperform is <em>part-whole, </em>with a tiny margin though<em>.</em></p><h3 id="during-inference-is-explicit-visual-observation-ie-images-necessary-for-recalling-commonsense-knowledge" style="position: relative;"><a href="#during-inference-is-explicit-visual-observation-ie-images-necessary-for-recalling-commonsense-knowledge" title="During inference, is explicit visual observation (i.e., images) necessary for recalling commonsense knowledge?" id="anchor-during-inference-is-explicit-visual-observation-ie-images-necessary-for-recalling-commonsense-knowledge"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>During inference, is explicit visual observation (i.e., images) necessary for recalling commonsense knowledge? </h3><p>The above experiments have not used the visual input during the zero-shot QA task, so a natural question is, will adding images help? The answer is no. Visual information just makes everything worse. In fact, experiments show that all dual-stream (textual + visual) models underperform compared to their text-only counterparts on <em>part-whole</em>, <em>spatial, taxonomic</em>, and <em>distinctness </em>dimensions. This suggests that the texts are the driving force for successful inference on purely linguistic tasks. </p><p>What I found interesting about this work is the demystifying of the multimodal model, showing that although state-of-the-art VL models do encode complementary knowledge types to pure language models, the way that they combine visual and textual signals is still very primitive and not human-like at all.</p><h2 id="dual-capsule-attention-mask-network-with-mutual-learning-for-visual-question-answering" style="position: relative;"><a href="#dual-capsule-attention-mask-network-with-mutual-learning-for-visual-question-answering" title="Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering" id="anchor-dual-capsule-attention-mask-network-with-mutual-learning-for-visual-question-answering"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering</h2><p>It is worth mentioning that the probing task from the last work is <strong>not </strong>a visual question-answering (VQA) task, though it contains QA pairs and images. A formal VQA task looks like the following:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-9.png" class="kg-image" alt="Concept map with &quot;VQA Model&quot; central node, &quot;Sleeping&quot; sub-node, and image of dog lying on pavement with bicycle" width="1124" height="578" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-9.png 1124w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">The goal of VQA is to answer questions related to the content of images correctly. It has a wide range of practical applications, such as helping people with visual impairments and human-computer Q&amp;A</span></figcaption></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.500/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Dual Capsule Attention Mask Network with Mutual Learning for Visual Question Answering</div><div class="kg-bookmark-description">Weidong Tian, Haodong Li, Zhong-Qiu Zhao. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.500.jpg" alt="" style="cursor: help;"></div></a></figure><p>The challenge of the VQA task is how to leverage fine-grained features with critical information to ensure that feature extraction emphasizes the objects related to the questions. Let's see two examples below:</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-10.png" class="kg-image" alt="Split image with a girl on a horse wearing a helmet on the left, and a multi-purpose bedroom with a bed, desk, and electronics on the right" width="1486" height="706" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-10.png 1486w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>In the left example, the fine-grained features with attention have the critical information required for the answer inference, which helps the model generate the correct answer by eliminating irrelevant factors that could interfere. In the right example, unattended coarse-grained features contain the richer semantic information needed for a correct answer.</p><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-emoji">🤔</div><div class="kg-callout-text">Can we combine global coarse-grained information and local fine-grained information to provide better information for the VQA task?</div></div><p>Weidong et al. address that in their paper. They propose a dual capsule attention mask network (DCAMN) with mutual learning for VQA.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-11.png" class="kg-image" alt="Diagram of a deep learning network with coarse-grained and fine-grained branches, decoders, and fusion blocks labeled" width="2000" height="1115" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2022/10/image-11.png 2400w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>DCAMN can process features at different granularities, taking global information into account and focusing on critical information. Combining different perspectives and granularities can improve the generalization capability of the model and make more accurate predictions. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-12.png" class="kg-image" alt="Table displaying text classification performance by method, with columns for Test-dev, Test-std, and All, including numerical scores" width="1450" height="764" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-12.png 1450w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">DCAMN performance on three types of VQA: yes/no, number, and others</span></figcaption></figure><p>In addition, the proposed DCAMN can effectively fuse multimodal features and locate evidence, improving the interpretability of the network, as the picture below illustrates:</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image" style="flex: 0.947955 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-1.png" width="510" height="538" alt="" style="cursor: help;"></div><div class="kg-gallery-image" style="flex: 0.840741 1 0%;"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2-1.png" width="454" height="540" alt="" style="cursor: help;"></div></div></div></figure><p>The attention mechanism and the model's interpretability are of particular interest for explainable multimodal AI. </p><h2 id="in-the-wild-video-question-answering" style="position: relative;"><a href="#in-the-wild-video-question-answering" title="In-the-Wild Video Question Answering" id="anchor-in-the-wild-video-question-answering"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>In-the-Wild Video Question Answering</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.496/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">In-the-Wild Video Question Answering</div><div class="kg-bookmark-description">Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai Burzo, Rada Mihalcea. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.496.jpg" alt="" style="cursor: help;"></div></a></figure><p>As a temporal extension of Visual QA, Video QA plays an important role in the development of intelligent AI systems, as it enables the effective processing of modality and temporal information. In the example below, given a long-video and a question, a video QA system first aims to generate open-ended answers. Second, it retrieves visual support for a given question and answer, which is represented by a span.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-13.png" class="kg-image" alt="Man in forest using bow saw and handaxe on tree branch, with timestamps 00:00, 00:37, and 00:51, indicating action progression" width="2000" height="660" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-13.png 2338w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Finding the relevant frames in a video for a given question-answer pair can help a system in its reasoning process, and is in line with ongoing efforts to build interpretable models</span></figcaption></figure><p>Much of the existing work on Video QA is based on the MovieQA (Tapaswi et al., 2016), and TVQA (Lei et al., 2018) datasets, which focus on common human activities in a multiple-choice setting. These datasets consist primarily of video from cooking videos or movies, making them very limited in domain.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-15.png" class="kg-image" alt="Comparative table of video QA datasets, indicating metrics like video counts, questions, average duration, and annotations" width="2000" height="785" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2022/10/image-15.png 2400w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Comparison between WILDQA and other existing datasets. VE?: Whether the dataset provides “Video Evidences”?; MC: “Multiple Choice” question answering; OE: “Open Ended” question answering; ES: “Evidence Selection”.</span></figcaption></figure><p>Santiago et al. propose the WildQA dataset and tasks that focus on scenes recorded in the outside world in an open-ended setting, like the example below:</p><figure class="kg-card kg-video-card kg-width-regular kg-card-hascaption" data-kg-thumbnail="https://jina-ai-gmbh.ghost.io/content/images/2022/10/media-thumbnail-ember964.jpg" data-kg-custom-thumbnail="">
            <div class="kg-video-container" style="padding-bottom: 56.25%;">
                <video src="https://jina-ai-gmbh.ghost.io/content/media/2022/10/Norwegian-Explorer_11-clip-54.mp4" poster="https://img.spacergif.org/v1/640x360/0a/spacer.png" width="640" height="360" playsinline="" preload="metadata" style="background: transparent url('https://jina-ai-gmbh.ghost.io/content/images/2022/10/media-thumbnail-ember964.jpg') 50% 50% / cover no-repeat;"></video>
                <div class="kg-video-overlay">
                    <button class="kg-video-large-play-icon" aria-label="Play video">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                            <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"></path>
                        </svg>
                    </button>
                </div>
                <div class="kg-video-player-container">
                    <div class="kg-video-player">
                        <button class="kg-video-play-icon" aria-label="Play video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z"></path>
                            </svg>
                        </button>
                        <button class="kg-video-pause-icon kg-video-hide" aria-label="Pause video">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <rect x="3" y="1" width="7" height="22" rx="1.5" ry="1.5"></rect>
                                <rect x="14" y="1" width="7" height="22" rx="1.5" ry="1.5"></rect>
                            </svg>
                        </button>
                        <span class="kg-video-current-time">0:00</span>
                        <div class="kg-video-time">
                            /<span class="kg-video-duration">1:16</span>
                        </div>
                        <input type="range" class="kg-video-seek-slider" max="100" value="0">
                        <button class="kg-video-playback-rate" aria-label="Adjust playback speed">1×</button>
                        <button class="kg-video-unmute-icon" aria-label="Unmute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z"></path>
                            </svg>
                        </button>
                        <button class="kg-video-mute-icon kg-video-hide" aria-label="Mute">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                                <path d="M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z"></path>
                            </svg>
                        </button>
                        <input type="range" class="kg-video-volume-slider" max="100" value="100">
                    </div>
                </div>
            </div>
            <figcaption><p><span style="white-space: pre-wrap;">Q1: What kinds of bodies of water are there? A1: There are rivers and streams. Evidence: 0:07 - 0:14</span></p></figcaption>
        </figure><h2 id="multi-task-learning-of-t5-model-with-i3d-features" style="position: relative;"><a href="#multi-task-learning-of-t5-model-with-i3d-features" title="Multi-Task Learning of T5 Model with I3D Features" id="anchor-multi-task-learning-of-t5-model-with-i3d-features"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Multi-Task Learning of T5 Model with I3D Features</h2><p>To solve WildQA, Santiago et al. concatenate the text features with the visual features and input the concatenated features to the T5 model. They extract <a href="https://iashin.ai/video_features/models/i3d/">I3D video features</a> and take one feature per second.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-16.png" class="kg-image" alt="Diagram of a deep reinforcement learning model for video game question-answering featuring extractors, encoders, and decoders" width="2000" height="1107" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-16.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-16.png 2270w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>T5 encoder outputs a sequence of the encoded states. Santiago et al. treat the subsequence corresponding to the visual features as the encoded hidden sequence for the video frames. They then multiply the sequence with two vectors to get the maximum likelihood prediction of the start and the end of the evidence, respectively.</p><p>The tasks of visual evidence selection and video question-answering can be jointly trained together by simply combining the two losses in a weighted-manner. </p><p>In the experiment, Santiago et al. propose multiple baselines including: randomly choosing answers from the dev set (i.e. Random); always predicting the most common answer in the dev set (i.e. Common); and retrieving the answers for the dev set question whose embedding has the highest cosine similarity to the test question (i.e. Closest).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-17.png" class="kg-image" alt="Table comparing performance metrics like ROUGE scores and IOU-F1 for models including Random, Closest, T5, and Human" width="2000" height="701" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-17.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-17.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-17.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-17.png 2020w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Performance on video question-answering, measured by ROUGE score (left); and on video evidence selection (right) measured by Intersection-Over-Union. IOU is defined as the length of their intersection divided by the length of their union. The prediction is counted as a match if it overlaps with any of the ground truth spans by more than the threshold. They then use these partial matches to calculate an F1 score (i.e. IOU-F1 scores).</span></figcaption></figure><p>I noticed that introducing video information (T5 T+V) does not improve performance on video question-answering tasks, which confirms what Yang &amp; Silberer said in their work. This may also due to the types of questions in WildQA, which do not match commonsense knowledge. In the video evidence selection task, the T5 model is not even better than the random baseline. Adding multi-task learning does not help. In general, we can conclude WildQA is a pretty challenging task with a significant gap between the state-of-the-art multimodal model and human. </p><h2 id="other-interesting-multimodal-research" style="position: relative;"><a href="#other-interesting-multimodal-research" title="Other Interesting Multimodal Research" id="anchor-other-interesting-multimodal-research"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Other Interesting Multimodal Research</h2><h3 id="multimodal-social-media" style="position: relative;"><a href="#multimodal-social-media" title="Multimodal Social Media" id="anchor-multimodal-social-media"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Multimodal Social Media</h3><p>There are multiple research projects focusing on multimodality in the social media. For example, the upswing of text and image sharing on social media platforms during mass emergency situations has led to numerous opportunities to gain timely access to valuable information that can help disaster relief authorities act more quickly and more efficiently.</p><p>Iustin et al. extend the <a href="https://arxiv.org/abs/2001.07685">FixMatch algorithm</a> to a multimodal scenario and offer two extensions to the original approach relevant for text and multimodal datasets. They show that multimodal FixMatch can leverage inexpensive unlabeled data to improve performance on diaster classification tasks.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.239/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Multimodal Semi-supervised Learning for Disaster Tweet Classification</div><div class="kg-bookmark-description">Iustin Sirbu, Tiberiu Sosea, Cornelia Caragea, Doina Caragea, Traian Rebedea. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.239.jpg" alt="" style="cursor: help;"></div></a></figure><p>Extracting spatial information from social media and tweets has also received substantial attention recently. Zhaomin et al. propose BERT+VGG16 multimodal model to determine whether people are located in the places they mention in their tweets.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.226/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Are People Located in the Places They Mention in Their Tweets? A Multimodal Approach</div><div class="kg-bookmark-description">Zhaomin Xiao, Eduardo Blanco. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.226.jpg" alt="" style="cursor: help;"></div></a></figure><p>To better understand when the text and image are most beneficial, Zhaomin et al. perform a qualitative analysis of the errors made by the single-stream model, and how these errors are fixed by the dual-stream model. </p><p>Social media is a good place to conduct sentiment analysis. As a popular way to express emotion on social media, stickers &amp; memes in posts can supplement missing sentiments and help identify sentiments precisely. Stickers and memes, despite being represented as images or simple gifs, have very different semantics from real-world photos. Feng et al. point out three challenges when working with stickers/memes:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.591/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Towards Exploiting Sticker for Multimodal Sentiment Analysis in Social Media: A New Dataset and Baseline</div><div class="kg-bookmark-description">Feng Ge, Weizhao Li, Haopeng Ren, Yi Cai. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.591.jpg" alt="" style="cursor: help;"></div></a></figure><ul><li>First, stickers may be inherently multimodal because they are embedded with texts. Same sticker with different sticker texts may vary significantly in sentiment.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/image-18.png" class="kg-image" alt="Collage of Philosoraptor memes depicting philosophical and humorous questions related to the meme genre" width="2000" height="1038" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2022/10/image-18.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2022/10/image-18.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2022/10/image-18.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/size/w2400/2022/10/image-18.png 2400w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Millennial Humor on "Philosoraptor"</span></figcaption></figure><ul><li>Second, stickers are highly varied in style, preventing the models from learning robust representations.</li><li>Third, the way sentiment is fused together from text and stickers is complicated and sometimes inconsistent.</li></ul><p>This work actually reminds me a popular Youtube video explaining Gen Z humor and their meme behaviors:</p><div class="kg-card kg-callout-card kg-callout-card-grey"><div class="kg-callout-text">Popular = Not Funny<br>Was popular = Funny<br>Ironic = Funny<br>Make no sense = Funny<br>Unfunny = Funny</div></div><p>There's nothing that Gen-Zers like more than changing and hating things. So perhaps our models on sentiment analysis have become obsolete? </p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/oVlspd9hxFA?start=61&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" title="Why is Gen Z Humor So Weird?"></iframe></figure><h3 id="text-gestures-multimodal" style="position: relative;"><a href="#text-gestures-multimodal" title="Text-Gestures Multimodal" id="anchor-text-gestures-multimodal"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Text-Gestures Multimodal</h3><p>Communication is a multimodal process. Information from verbal and non-verbal modalities are mixed into one channel. It has been revealed from a long history of empirical studies that speakers’ expression in the visual modality, including gestures, body poses, eye contacts and other types of non-verbal behaviors, play critical roles in face-to-face communication, because they add subtle information that is hard to convey in verbal language.</p><p>Yang etc. consider gestures as non-verbal communication and prove that non-verbal communication also conforms to the principle of entropy rate constancy (ERC). Under this assumption, communication in any form (written or spoken) should optimize the rate of information transmission rate by keeping the overall entropy rate constant.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.12/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Gestures Are Used Rationally: Information Theoretic Evidence from Neural Sequential Models</div><div class="kg-bookmark-description">Yang Xu, Yang Cheng, Riya Bhatia. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.12.jpg" alt="" style="cursor: help;"></div></a></figure><p>This means that the information encoded in hand gestures, albeit subtle, is actually organized in a rational way that enhances the decoding/understanding of information from a receiver’s perspective.</p><p>Artem etc. consider human gestures together with their corresponding utterances. They explore a multimodal approach to learning gesture embeddings through contrastive learning, and attempt to predict psycholinguistic categories and the language of the speaker from their gesture embeddings.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.488/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Towards Understanding the Relation between Gestures and Language</div><div class="kg-bookmark-description">Artem Abzaliev, Andrew Owens, Rada Mihalcea. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.488.jpg" alt="" style="cursor: help;"></div></a></figure><h3 id="text-image-multimodal-but-via-topic-modelling" style="position: relative;"><a href="#text-image-multimodal-but-via-topic-modelling" title="Text-Image Multimodal, but via Topic Modelling" id="anchor-text-image-multimodal-but-via-topic-modelling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Text-Image Multimodal, but via Topic Modelling</h3><p>Perhaps the most surprising work to me is from Elaine et al., who present a novel neural multilingual and multimodal topic model that takes advantage of pretrained document and image embeddings to abstract the complexities between languages and modalities. Their work is based on the contextualized topic model, a family of topic models that uses contextualized document embeddings as input.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aclanthology.org/2022.coling-1.355/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Multilingual and Multimodal Topic Modelling with Pretrained Embeddings</div><div class="kg-bookmark-description">Elaine Zosa, Lidia Pivovarova. Proceedings of the 29th International Conference on Computational Linguistics. 2022.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://aclanthology.org/aclicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">ACL Anthology</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://aclanthology.org/thumb/2022.coling-1.355.jpg" alt="" style="cursor: help;"></div></a></figure><p>It surprises me that after more than 12 years, Latent Dirichlet Allocation (LDA) models still show up in top conferences like COLING. After reading through the paper, I found a lot of adoption has been made over the last few years to make the original LDA more "deep". In fact, it is now called neural topic models (NTMs), which refers to a class of topic models that use neural networks to estimate the parameters of the topic-word and document-topic distributions.</p><h2 id="summary" style="position: relative;"><a href="#summary" title="Summary" id="anchor-summary"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Summary</h2><p>There has been some great work on multimodal AI presented at COLING this year. It is clear that this is a subject that is only going to continue to grow in importance, as I pointed out in the blog post below.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/news/paradigm-shift-towards-multimodal-ai/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">The Paradigm Shift Towards Multimodal AI</div><div class="kg-bookmark-description">We are on the cusp of a new era in AI, one in which multimodal AI will be the norm. At Jina AI, our MLOps platform helps businesses and developers win while they’re right at the starting line of this paradigm shift, and build the applications of the future today.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/android-icon-192x192.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">Jina AI</span><span class="kg-bookmark-publisher">Jina AI</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/7.png" alt="" style="cursor: help;"></div></a></figure><p>Multimodal AI allows machines to better understand the world around them by processing data from multiple modalities (e.g. text, images, audio). This is an important step towards true artificial general intelligence, as it allows machines to more closely approximate the way humans process information.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categories:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Event</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fcoling2022%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fcoling2022%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fcoling2022%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fcoling2022%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fcoling2022%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-c36e4d4e="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-c36e4d4e="" class="text-h5 q-my-xl">Read more</div><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/what-we-learned-at-iclr2025"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">May 25, 2025 • 21 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">What We Learned at ICLR2025</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Jina AI"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Jina AI" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Three people smiling on a stage at a conference with an ICLR banner visible, suggesting a warm and lively event atmosphere."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Three people smiling on a stage at a conference with an ICLR banner visible, suggesting a warm and lively event atmosphere." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/call-for-participants-emnlp-2024-bof-on-embeddings-reranker-small-lms-for-better-search"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">November 05, 2024 • 2 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Call for Participants: EMNLP 2024 BoF on Embeddings, Reranker &amp; Small LMs for Better Search</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Jina AI"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Jina AI" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Event poster for &quot;Embedding Reranker, Small LM &amp; Better Search&quot; on Nov 14, 2024, from 10:30 to 12:00 at Miami Lecture Hall. F"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Event poster for &quot;Embedding Reranker, Small LM &amp; Better Search&quot; on Nov 14, 2024, from 10:30 to 12:00 at Miami Lecture Hall. F" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/bof-banner.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/what-we-learned-at-icml2024-ft-plag-xrm-tinybenchmark-magiclens-prompt-sketching-etc"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">August 07, 2024 • 10 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">What We Learned at ICML2024 ft. PLaG, XRM, tinyBenchmark, MagicLens, Prompt Sketching etc.</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 89px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Florian Hönicke"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Florian Hönicke" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/06/florian-small.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Michael Günther"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Michael Günther" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 36px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Georgios Mastrapas"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Georgios Mastrapas" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/profile.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 54px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Two logos on gray background: upper &quot;ICML International Conference on Machine Learning,&quot; lower abstract &quot;vibo&quot; logo."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Two logos on gray background: upper &quot;ICML International Conference on Machine Learning,&quot; lower abstract &quot;vibo&quot; logo." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/icml-banner.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Offices</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Sunnyvale, CA</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Berlin, Germany (HQ)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Germany</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Beijing, China</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Level 5, Building 6, No.48 Haidian West St. Beijing, China</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Shenzhen, China</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">402 Floor 4, Fu'an Technology Building, Shenzhen, China</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Search Foundation</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">DeepSearch</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Reader</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Embeddings</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Reranker</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Classifier</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Segmenter</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">API Documentation</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Get Jina API key</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Rate Limit</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">API Status</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Company</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">About us</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Newsroom</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Join us</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Download logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Terms</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Security</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Privacy</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Manage Cookies</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
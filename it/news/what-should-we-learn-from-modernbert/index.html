<!DOCTYPE html><html translate="no" dir="ltr" lang="it"><head><title>Cosa Dovremmo Imparare da ModernBERT?</title><meta charset="utf-8"><meta name="title" content="Cosa Dovremmo Imparare da ModernBERT?"><meta name="description" content="Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="og:title" content="Cosa Dovremmo Imparare da ModernBERT?"><meta property="og:description" content="Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT."><meta property="og:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="twitter:title" content="Cosa Dovremmo Imparare da ModernBERT?"><meta property="twitter:description" content="Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-3ulg346R.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-CmGpSnYT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-CAWQxWHK.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-Dahgv1jk.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-wDpKb7Hu.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-Cvow9xzV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-Bbwc1D0B.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/it-cPZrmCL2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-ZSq9PT49.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-Ddqd-kot.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-CyXTgVUC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-D5sfNPW-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-BsPrOXLJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-DHleNtye.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-C2H-jTiZ.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-BzuKfM_n.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-qdBTbkQR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-_n0iH75v.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-JArV4eP8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-DkUFzLsh.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-DSEEBqpa.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-DYr0d6to.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-BOCjaVU3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-DyM9Zv1p.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-Byr0PJDU.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-hh2b4Npl.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-ByVKoLGE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-BKCtQJPn.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-CTp3xGnI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-C_dCzjdb.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-CCouKbSa.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-CmTIMnTI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-uAqjkC7s.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-AeRWtxYb.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-D8cXTjZl.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-DaYqJ6ge.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-C1vDmaBY.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Nan Wang, Alex C-G"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Nan Wang, Alex C-G"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="10 mins read"><meta property="article:published_time" content="2025-01-22T08:31:26.000+01:00"><meta property="article:modified_time" content="2025-01-22T08:31:26.000+01:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Cosa Dovremmo Imparare da ModernBERT?",
  "description": "Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT.",
  "image": [
    "https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"
  ],
  "datePublished": "2025-01-22T08:31:26.000+01:00",
  "dateModified": "2025-01-22T08:31:26.000+01:00",
  "author": [
    {
      "@type": "Person",
      "name": "Nan Wang",
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "@type": "Person",
      "name": "Alex C-G",
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Notizia</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Modelli</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_6f2888aa-41c2-4227-87cf-90d1c1ea41bb" aria-label="Espandi &quot;Prodotti&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Prodotti</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_6f2888aa-41c2-4227-87cf-90d1c1ea41bb" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Prodotti"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lettore</div><div class="q-item__label q-item__label--caption text-caption">Leggi gli URL e cerca sul web per ottenere LLM più approfonditi.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incorporamenti</div><div class="q-item__label q-item__label--caption text-caption">Incorporamenti multilingue multimodali di livello mondiale.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Riclassificazione</div><div class="q-item__label q-item__label--caption text-caption">Recupero neurale di livello mondiale per massimizzare la pertinenza della ricerca.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Ricerca profonda</div><div class="q-item__label q-item__label--caption text-caption">Cerca, leggi e ragiona finché non trovi la risposta migliore.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_7a73ba75-af26-4974-a82d-c0efe95aaa6e" aria-label="Espandi &quot;Di più&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Di più</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_7a73ba75-af26-4974-a82d-c0efe95aaa6e" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classificatore</div><div class="q-item__label q-item__label--caption text-caption">Classificazione zero-shot e few-shot per immagini e testo.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentatore</div><div class="q-item__label q-item__label--caption text-caption">Tagliare il testo lungo in blocchi ed effettuare la tokenizzazione.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Documentazione API</div><div class="q-item__label q-item__label--caption text-caption">Generazione automatica di codice per il tuo IDE o LLM di Copilot</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_9e4b5279-23fe-4673-be3f-e0124e8eeb22" aria-label="Espandi &quot;Azienda&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Azienda</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_9e4b5279-23fe-4673-be3f-e0124e8eeb22" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Chi siamo</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contatta le vendite</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programma di stagista</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Unisciti a noi</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Scarica il logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Termini &amp; Condizioni</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Login"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Login</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_5a62c33f-ea4c-4282-abff-d44212d96428"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_5a62c33f-ea4c-4282-abff-d44212d96428" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_5a62c33f-ea4c-4282-abff-d44212d96428_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Efficienza dei Parametri di ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Modellazione del Codice di ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Gestione del contesto lungo di ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">La lezione amara?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Conclusione</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog tecnico</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">gennaio 22, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">Cosa Dovremmo Imparare da ModernBERT?</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Dati di addestramento più ampi, dimensionamento efficiente dei parametri e un'architettura profonda ma snella: ModernBERT traccia una direzione per i futuri modelli di tipo BERT.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png" style="object-fit: contain; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Nan Wang"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Nan Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Nan Wang, Alex C-G • 10 minuti letti</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>Nel 2018, Google ha rilasciato BERT che ha rivoluzionato il campo del NLP, molto prima dell'attuale ondata di LLM. Ancora oggi, molti Small Language Models sono costruiti su BERT. Nel dicembre 2024, <a href="https://huggingface.co/blog/modernbert" rel="noreferrer">ModernBERT</a> applica ciò che abbiamo imparato dai recenti sviluppi degli LLM a questi modelli più piccoli. I punti chiave? Migliore efficienza dei parametri, comprensione del codice e gestione di contesti lunghi.</p><p>In questo post, analizzeremo come ModernBERT si confronta con due modelli che conosciamo a fondo: <code>jina-XLM-RoBERTa</code> (la base multilingua dietro <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>) e <code>RobERTa-large</code>. Esaminiamo ogni modello:</p><ul><li><strong>ModernBERT</strong> (dic. 2024) è un SLM di recente rilascio, sviluppato in collaborazione da Answer.AI, LightOn e HuggingFace. Sfrutta ottimizzazioni moderne come RoPE per una finestra di contesto di 8.192 token e <a href="https://arxiv.org/abs/2002.05202">layer GeGLU</a>, migliorando le prestazioni mantenendo l'efficienza.</li><li><a href="https://huggingface.co/jinaai/xlm-roberta-flash-implementation"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong></strong> (sett. 2024) è un modello di embedding testuale multilingue basato su <a href="https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta"><code>XLM-RoBERTa</code></a> di Meta. Mentre l'originale <code>XLM-RoBERTa</code> migliora <code>RoBERTa</code> usando il grande dataset multilingue XLM, <code>jina-XLM-RoBERTa</code> va oltre con training su contesti estesi, implementazione <a href="https://arxiv.org/abs/2104.09864">RoPE</a> e supporto <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a>. Questo modello funge da base per <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>.</li><li><a href="https://huggingface.co/FacebookAI/roberta-large"><strong><code>RoBERTa-large</code></strong></a> (luglio 2019) sviluppato da Meta, è una versione migliorata di BERT con 355 milioni di parametri. Attraverso training esteso, dataset più grandi e innovazioni come il masking dinamico, ha raggiunto risultati impressionanti su benchmark chiave tra cui <a href="https://gluebenchmark.com/">GLUE</a>, <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a> e <a href="https://arxiv.org/abs/1704.04683">RACE</a>. Questo lo rende adatto a vari compiti NLP dalla classificazione del testo alle risposte a domande.</li></ul><p>Confrontando questi modelli su tre aspetti fondamentali, puntiamo a evidenziare le efficaci scelte progettuali di ModernBERT per altri sviluppatori di modelli e identificare intuizioni chiave di sviluppo per futuri modelli simili a BERT. Condivideremo anche i nostri apprendimenti dallo sviluppo di <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a> e discuteremo i miglioramenti pianificati per <code>jina-embeddings-v4</code> e <code>jina-reranker-v3</code>.</p><h2 id="modernberts-parameter-efficiency" style="position: relative;"><a href="#modernberts-parameter-efficiency" title="Efficienza dei Parametri di ModernBERT" id="anchor-modernberts-parameter-efficiency"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Efficienza dei Parametri di ModernBERT</h2><p>Esaminiamo prima l'approccio di ModernBERT all'efficienza dei parametri - sta introducendo diverse intuizioni chiave dai recenti sviluppi LLM. ModernBERT sfrutta tre strategie principali: un'architettura più profonda ma più sottile, dimensione controllata del vocabolario e upscaling progressivo del modello partendo da modelli più piccoli.</p><h3 id="deep-and-thin-architecture" style="position: relative;"><a href="#deep-and-thin-architecture" title="Architettura Deep-And-Thin" id="anchor-deep-and-thin-architecture"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Architettura Deep-And-Thin</h3><p>ModernBERT-large va più in profondità con 28 layer, mentre <code>jina-XLM-RoBERTa</code> e <code>RoBERTa-large</code> ne hanno 24. Ma ecco la parte interessante - pareggia <code>RoBERTa-large</code> nel conteggio dei parametri nonostante quei layer extra. <code>jina-XLM-RoBERTa</code> necessita di più parametri poiché gestisce 89 lingue, mentre gli altri due si concentrano solo sull'inglese.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg" class="kg-image" alt="" width="1389" height="547" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">La profondità (numero di layer) è più importante della larghezza (numero di unità nascoste) per i piccoli LLM. Una struttura profonda e sottile eccelle nel catturare concetti astratti, risultando in prestazioni finali superiori.</span></figcaption></figure><p>La maggior parte dei parametri di un transformer proviene dai layer di attention e fully-connected. ModernBERT rimane competitivo nelle dimensioni diventando "più sottile" - utilizza 2.624 unità nascoste su 28 layer, rispetto alle 4.096 unità di RoBERTa-large su 24 layer. Questo setup "più profondo" ma più sottile permette loro di raggiungere i loro obiettivi di prestazioni senza gonfiare il modello.</p>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameters</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Hidden states</td>
<td>1,024</td>
<td>1,024</td>
<td>1,024</td>
</tr>
<tr>
<td>Intermediate dims</td>
<td>2,624</td>
<td>4,096</td>
<td>4,096</td>
</tr>
<tr>
<td>Attention heads</td>
<td>16</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>Layers</td>
<td>28</td>
<td>24</td>
<td>24</td>
</tr>
<tr>
<td>Vocabulary size</td>
<td>50,368</td>
<td>250,002</td>
<td>50,265</td>
</tr>
</tbody>
</table>

<p>Questo approccio si allinea con la ricerca <a href="https://openreview.net/pdf?id=EIGbXbxcUQ">MobileLLM</a> di Meta, che ha scoperto che per i modelli più piccoli, la profondità è più importante della larghezza quando si tratta di catturare pattern complessi e guidare le prestazioni. Essenzialmente, la capacità di elaborare informazioni attraverso più layer transformer si rivela più preziosa dell'avere layer più larghi per l'elaborazione parallela.</p><p>Diamo un'occhiata ai dati su come si comporta questa architettura deep-and-thin.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg" class="kg-image" alt="" width="872" height="371" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Confrontandolo con modelli comparabili che usano l'architettura tradizionale shallow-fat, ModernBERT fornisce risultati migliori su task chiave come retrieval e STS - tutto mentre mantiene simile il conteggio dei parametri.</span></figcaption></figure>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>STS12</td>
<td>72.6</td>
<td><strong>72.7</strong></td>
<td>68.9</td>
</tr>
<tr>
<td>STS13</td>
<td><strong>84.9</strong></td>
<td>83.9</td>
<td>81.0</td>
</tr>
<tr>
<td>STS14</td>
<td>77.5</td>
<td><strong>77.7</strong></td>
<td>74.8</td>
</tr>
<tr>
<td>STS15</td>
<td>84.8</td>
<td><strong>85.8</strong></td>
<td>84.1</td>
</tr>
<tr>
<td>STS16</td>
<td>79.4</td>
<td><strong>79.6</strong></td>
<td>78.6</td>
</tr>
<tr>
<td>STS17</td>
<td><strong>87.5</strong></td>
<td>87.2</td>
<td>87.2</td>
</tr>
<tr>
<td>TRECCOVID</td>
<td><strong>61.1</strong></td>
<td>59.6</td>
<td>49.3</td>
</tr>
<tr>
<td>FiQA</td>
<td><strong>44.4</strong></td>
<td>40.0</td>
<td>40.7</td>
</tr>
<tr>
<td>NFCorpus</td>
<td><strong>32.6</strong></td>
<td>30.6</td>
<td>27.9</td>
</tr>
<tr>
<td>SciFact</td>
<td><strong>68.6</strong></td>
<td>65.5</td>
<td>63.1</td>
</tr>
<tr>
<td>Average</td>
<td><strong>69.3</strong></td>
<td>68.2</td>
<td>65.6</td>
</tr>
</tbody>
</table>

<p>Prendiamo <code>jina-XLM-RoBERTa</code> - si basa sull'architettura shallow-fat di <code>RoBERTa-large</code> ma aumenta il vocabolario da 50K a 250K token e si addestra su più dati. Eppure ModernBERT lo supera comunque, suggerendo che il cambiamento architetturale sta facendo una reale differenza in termini di efficienza.</p><h3 id="vocabulary-size-matters" style="position: relative;"><a href="#vocabulary-size-matters" title="La Dimensione del Vocabolario è Importante" id="anchor-vocabulary-size-matters"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>La Dimensione del Vocabolario è Importante</h3><p>Prima, vediamo come vengono contati i parametri del vocabolario nei transformer. Per qualsiasi transformer, <code>parametri vocabolario = numero di token distinti × dimensione nascosta</code>. Prendiamo <code>jina-XLM-RoBERTa</code>: con 250K token e 1.024 dimensioni, necessita di 256M parametri solo per la codifica del vocabolario - prima di gestire qualsiasi task linguistico!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg" class="kg-image" alt="" width="3757" height="715" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Nei transformer, il primo strato mappa i token in stati nascosti usando una matrice di pesi, ovvero i pesi del vocabolario. Considerando tutti i code point UTF-8 (1.112.064) con 1.024 dimensioni nascoste, sarebbero necessari ben </span><code spellcheck="false" style="white-space: pre-wrap;"><span>1,112,064 × 1,024 = 1 B</span></code><span style="white-space: pre-wrap;"> parametri solo per la conversione dei token. Mentre i LLM più grandi (oltre 100B di parametri) possono gestire questo overhead, è un serio vincolo per i modelli più piccoli. È esattamente per questo che usiamo tokenizer come BPE, che uniscono efficientemente code point UTF-8 comuni in token singoli.</span></figcaption></figure><p>Ma ecco il punto: <strong>i pesi del vocabolario non contribuiscono ai meccanismi di attenzione - sono solo tabelle di lookup.</strong> Per gli SLM che lavorano con budget di parametri fissi, un vocabolario più ampio significa meno parametri disponibili per i layer di attenzione, che eseguono l'effettiva elaborazione del linguaggio. Questo spiega perché ModernBERT-large solo inglese supera le prestazioni del multilingue <code>jina-XLM-RoBERTa</code> nonostante sia più piccolo - <code>jina-XLM-RoBERTa</code> alloca più parametri (47%!) per supportare più lingue. Il vocabolario focalizzato di ModernBERT non solo migliora le prestazioni ma velocizza anche l'inferenza, rendendolo particolarmente efficace per applicazioni con risorse limitate.</p><p>Quindi se ora guardiamo <em>solo</em> i parametri del modello core (escludendo i pesi del vocabolario), ModernBERT ha in realtà più potenza computazionale dei suoi pari: ModernBERT dedica il 19% in più di parametri alla <em>vera</em> modellazione del linguaggio rispetto a <code>jina-XLM-RoBERTa</code> e il 15% in più rispetto a <code>RoBERTa-large</code>!</p>

<table>
<thead>
<tr>
<th>Specifiche Modello</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Supporto Lingue</td>
<td>Solo Inglese</td>
<td>89 Lingue</td>
<td>Solo Inglese</td>
</tr>
<tr>
<td>Dimensione Vocabolario</td>
<td>50,4K</td>
<td>250K</td>
<td>50,3K</td>
</tr>
<tr>
<td>Parametri Totali</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Parametri Vocabolario</td>
<td>51M</td>
<td>256M</td>
<td>51M</td>
</tr>
<tr>
<td>Rapporto Parametri Vocabolario</td>
<td>13%</td>
<td>47%</td>
<td>14%</td>
</tr>
<tr>
<td>Parametri Modello Core</td>
<td><b>349M</b></td>
<td>294M</td>
<td>304M</td>
</tr>
</tbody>
</table>

<h3 id="model-upscaling-by-weight-tiling" style="position: relative;"><a href="#model-upscaling-by-weight-tiling" title="Upscaling del Modello tramite &quot;Weight Tiling&quot;" id="anchor-model-upscaling-by-weight-tiling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Upscaling del Modello tramite "Weight Tiling"</h3><p>Nella costruzione del backbone di <a href="https://huggingface.co/jinaai/jina-bert-implementation"><code>jina-BERT-v2</code></a>, abbiamo scoperto che addestrare SLM da zero era dispendioso in termini di risorse e complesso. ModernBERT affronta questo problema con un approccio intelligente di inizializzazione chiamato <strong>weight tiling</strong> - essenzialmente avviando ModernBERT-large dai pesi della sua versione base più piccola.</p><p>Questa tecnica non è del tutto nuova - si basa sul lavoro di DeepMind con <a href="https://gpt3demo.com/apps/deepmind-gopher">Gopher</a> e appare anche nei modelli <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2</a> di Microsoft. Ma la sua applicazione qui è particolarmente efficace per affrontare il collo di bottiglia dell'addestramento SLM.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg" class="kg-image" alt="" width="1877" height="1308" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT scala da 22 a 28 layer usando la strategia di inizializzazione della profondità del team Gopher. Per quei layer extra (23-28), inizializzano ciascuno usando i pesi dai 22 layer originali di ModernBERT-base. Per le matrici di pesi di ogni layer, usano l'approccio di tiling centrale di Phi-2. Funziona così: prendono i pesi di ModernBERT-base e li posizionano proprio nel centro delle matrici di ModernBERT-large. Per i bordi ancora vuoti? Avvolgono ciclicamente i pesi originali per riempirli.</span></figcaption></figure><p>Questa strategia di inizializzazione dà a ModernBERT-large un vantaggio significativo - invece di partire da zero, sfrutta i pattern pre-appresi dalla sua controparte più piccola. Si è dimostrata particolarmente <a href="https://arxiv.org/pdf/2112.11446">efficace per scalare i modelli di linguaggio in questo intervallo di dimensioni</a>.</p><blockquote>Troviamo che un modello avviato a caldo si riprende rapidamente da una perdita iniziale elevata (dovuta ai parametri aggiunti) fino a una perdita molto vicina a quella del modello base. Siamo in grado di espandere 417M parametri di oltre 3 volte in dimensione e mantenere prestazioni superiori a un equivalente modello nuovo addestrato da zero fino alla convergenza, implicando che i guadagni non erano limitati all'inizio dell'addestramento. Tuttavia, a dimensioni maggiori, i guadagni relativi raggiunti alla convergenza diminuiscono, specialmente con espansioni in larghezza.</blockquote><p>L'avvolgimento ciclico dei pesi non è solo una comodità - si allinea bene con il modo in cui le matrici di attenzione mostrano naturalmente pattern periodici. La ricerca di Gopher mostra che questo approccio brilla davvero per gli SLM (meno di 9B parametri), anche se i benefici iniziano a diminuire quando si passa a modelli più grandi.</p><h2 id="modernberts-code-modeling" style="position: relative;"><a href="#modernberts-code-modeling" title="Modellazione del Codice di ModernBERT" id="anchor-modernberts-code-modeling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Modellazione del Codice di ModernBERT</h2><p>ModernBERT porta un approccio specializzato alla comprensione del codice con il suo tokenizer ottimizzato per il codice e i dati di addestramento. Questa messa a punto per l'elaborazione del codice si ripaga sia nei compiti di comprensione che di recupero.</p><p>Abbiamo eseguito un benchmark usando il corpus <code>jina-embeddings-v2-code</code>, confrontando tre modelli come backbone: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, e <code>RoBERTa-large</code>. Il test? <a href="https://github.com/github/CodeSearchNet">CodeSearchNet</a> - abbinare descrizioni testuali a frammenti di codice. ModernBERT ha superato entrambe le alternative su tutta la linea.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg" class="kg-image" alt="" width="787" height="489" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Il divario ha senso - né </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> né </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;"> hanno visto linguaggi di programmazione durante l'addestramento. Nel frattempo, ModernBERT-large si è addestrato su due trilioni di token, inclusa una quantità sostanziale di codice. Questa esposizione alla sintassi e ai pattern di programmazione gli dà un chiaro vantaggio nei compiti relativi al codice. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> supera di poco </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;">, probabilmente grazie ai suoi dati di addestramento multilingue più ampi - stessa architettura, maggiore esposizione. Tuttavia, entrambi sono significativamente indietro rispetto a ModernBERT-large.</span></figcaption></figure>

<table>
<thead>
<tr>
<th>Compito</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>AdvRetrieval</td>
<td>0.342</td>
<td><strong>0.363</strong></td>
<td>0.331</td>
</tr>
<tr>
<td>QueryRetrieval.python</td>
<td>0.521</td>
<td><strong>0.530</strong></td>
<td>0.525</td>
</tr>
<tr>
<td>QueryRetrieval java</td>
<td><strong>0.679</strong></td>
<td>0.633</td>
<td>0.644</td>
</tr>
<tr>
<td>QueryRetrieval.javascript</td>
<td>0.755</td>
<td><strong>0.768</strong></td>
<td>0.732</td>
</tr>
<tr>
<td>QueryRetrieval.php</td>
<td><strong>0.815</strong></td>
<td>0.781</td>
<td>0.755</td>
</tr>
<tr>
<td>QueryRetrieval.ruby</td>
<td>0.729</td>
<td><strong>0.744</strong></td>
<td>0.722</td>
</tr>
<tr>
<td>QueryRetrieval.go</td>
<td><strong>0.833</strong></td>
<td>0.809</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.go</td>
<td><strong>0.778</strong></td>
<td>0.750</td>
<td>0.759</td>
</tr>
<tr>
<td>Retrieval.java</td>
<td><strong>0.840</strong></td>
<td>0.792</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.javascript</td>
<td><strong>0.817</strong></td>
<td>0.792</td>
<td>0.757</td>
</tr>
<tr>
<td>Retrieval.php</td>
<td><strong>0.852</strong></td>
<td>0.805</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.python</td>
<td><strong>0.849</strong></td>
<td>0.816</td>
<td>0.787</td>
</tr>
<tr>
<td>Retrieval.ruby</td>
<td><strong>0.849</strong></td>
<td>0.796</td>
<td>0.803</td>
</tr>
<tr>
<td>Media</td>
<td><strong>0.743</strong></td>
<td>0.721</td>
<td>0.708</td>
</tr>
</tbody>
</table>

<h3 id="the-tokenizer-edge" style="position: relative;"><a href="#the-tokenizer-edge" title="Il vantaggio del Tokenizer" id="anchor-the-tokenizer-edge"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Il vantaggio del Tokenizer</h3><p>Analizziamo perché ModernBERT gestisce così bene il codice - utilizza il <a href="https://huggingface.co/docs/transformers/en/model_doc/olmo" rel="noreferrer">tokenizer OLMo</a>, specificamente addestrato sul codice, invece dei tokenizer standard BERT/RoBERTa.</p><p>Un tokenizer divide il testo UTF-8 in token che vengono mappati in vettori - questi sono ciò che il modello elabora effettivamente. Durante l'addestramento, impara a combinare sequenze di caratteri che si verificano frequentemente in singoli token. La differenza? Un tokenizer standard potrebbe dividere <code>init</code> in <code>in</code> + <code>it</code>, perdendo il contesto di programmazione. Ma il tokenizer di ModernBERT orientato al codice lo comprende senza spezzarlo.</p><p>Ecco dove diventa interessante la gestione degli spazi: ModernBERT preserva gli spazi iniziali di Python come token singoli e differenzia tra 4 e 8 spazi - cruciale per la struttura del codice. Nel frattempo, <strong><code>jina-XLM-RoBERTa</code> comprime tutti gli spazi continui in un singolo <code>_</code>, e RoBERTa-large tratta ogni spazio come un token separato.</strong> Questo significa che l'encoder di ModernBERT riceve input più puliti e significativi quando elabora il codice, mentre gli altri lavorano con token frammentati e meno coerenti.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg" class="kg-image" alt="" width="3156" height="1247" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT preserva gli spazi iniziali di Python come token singoli e differenzia tra 4 e 8 spazi - cruciale per la struttura del codice; mentre gli altri lavorano con token frammentati e meno coerenti.</span></figcaption></figure><h2 id="modernberts-long-context-handling" style="position: relative;"><a href="#modernberts-long-context-handling" title="Gestione del contesto lungo di ModernBERT" id="anchor-modernberts-long-context-handling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Gestione del contesto lungo di ModernBERT</h2><p>ModernBERT ha fatto progressi significativi nell'elaborazione di testi lunghi, grazie al suo ampio corpus di addestramento (300B token con campioni da 8.192 token) e tecniche avanzate come l'attenzione combinata globale e locale.</p><p>Per valutare le capacità di gestione dei documenti lunghi, abbiamo utilizzato il <a href="https://huggingface.co/datasets/Shitao/MLDR">dataset MLDR</a> - un benchmark completo per testi lunghi che copre 13 lingue. Poiché ModernBERT attualmente supporta solo l'inglese, ci siamo concentrati sul sottoinsieme inglese di MLDR per confrontare ModernBERT con <code>jina-XLM-RoBERTa</code>. Mentre entrambi questi modelli possono gestire input di 8K token, <code>RoBERTa-large</code> è stato escluso da questo benchmark a causa del suo limite di 512 token, insufficiente per l'analisi di testi lunghi.</p>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>MLDR-en</td>
<td><strong>0.351</strong></td>
<td>0.290</td>
</tr>
</tbody>
</table>

<p>Le prestazioni superiori di ModernBERT non sono dovute solo al suo esteso addestramento su testi lunghi - sono in gran parte dovute alla sua innovativa combinazione di meccanismi di attenzione globale e locale. A differenza di <code>jina-XLM-RoBERTa</code>, che applica un'attenzione globale computazionalmente costosa a ogni livello, ModernBERT adotta un approccio più efficiente. Alterna tra attenzione globale (usata ogni terzo livello con un <code>theta</code> di 160.000) e attenzione locale (usando una finestra scorrevole di 128 token con un <code>theta</code> di 100.000). Questa strategia ibrida mantiene alte prestazioni riducendo drasticamente i tempi di addestramento.</p><blockquote>In ModernBERT, ogni terzo livello impiega attenzione globale con un theta RoPE di 160.000 e i livelli rimanenti utilizzano una finestra scorrevole locale di 128 token con un theta RoPE di 10.000. —— <a href="https://arxiv.org/pdf/2412.13663">ModernBERT</a></blockquote><h2 id="the-bitter-lesson" style="position: relative;"><a href="#the-bitter-lesson" title="La lezione amara?" id="anchor-the-bitter-lesson"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>La lezione amara?</h2><p>La legge di scaling e <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">la lezione amara</a> suggeriscono che i principali miglioramenti delle prestazioni derivano principalmente dall'aumento del numero di parametri e dei dati di addestramento. Questo principio ha guidato il nostro approccio di espandere il corpus e utilizzare LoRA per adattamenti specifici ai task.</p><p>Tuttavia, il successo di ModernBERT ha rivelato che abbiamo sottovalutato il potere dell'ottimizzazione architettonica. Dimostra che gli SLM possono raggiungere risultati eccezionali attraverso una migliore efficienza dati-modello, senza necessariamente aumentare i parametri. Un recente <a href="https://arxiv.org/pdf/2408.11868">report tecnico di Stella Embeddings</a> rafforza questa scoperta, indicando che gli attuali metodi di addestramento dei modelli di embedding possono essere migliorati senza aumentare le dimensioni del corpus o del modello.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg" class="kg-image" alt="Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu" width="949" height="949" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Legge di scaling dei modelli di embedding. La performance media MTEB sui task in inglese è tracciata rispetto al numero di parametri del modello. Ogni punto rappresenta un modello di embedding. La linea di tendenza, che rappresenta tutti i modelli, è evidenziata, con i modelli multilingue enfatizzati in ciano. Si può vedere che </span><a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a><span style="white-space: pre-wrap;"> dimostra prestazioni superiori rispetto ai modelli di dimensioni simili, mostrando anche un miglioramento superlineare rispetto al suo predecessore, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2</span></code><span style="white-space: pre-wrap;">. Questo grafico è stato creato selezionando i primi 100 modelli di embedding dalla classifica MTEB, escludendo quelli senza informazioni sulle dimensioni, tipicamente modelli closed-source o proprietari. Sono state filtrate anche le submission identificate come trolling evidente.</span></figcaption></figure><p>Andando avanti, prevediamo costi computazionali inferiori e dimensioni dei modelli più piccole man mano che acquisiamo una comprensione più profonda dell'utilizzo dei dati e implementiamo le tecniche di ModernBERT. Nel breve termine, possiamo implementare i miglioramenti diretti delineati nel paper di ModernBERT - in particolare integrando più dati relativi al codice e adottando un tokenizer ottimizzato per il codice. Cambiamenti più complessi, come il passaggio a un'architettura deep-and-thin o il bootstrapping di modelli grandi da quelli più piccoli, richiederanno la costruzione di modelli backbone da zero - un'iniziativa a medio termine.</p><p>Mentre l'efficienza di ModernBERT è notevole, la sua limitazione al solo testo indica sfide future. Con la crescente popolarità dei modelli di embedding multimodali, la nostra prossima sfida è sviluppare modelli di ricerca fondamentali più intelligenti, veloci e capaci che possano gestire input per applicazioni multimodali. Queste applicazioni richiedono finestre di contesto ancora più lunghe - una sfida di efficienza che resta da risolvere.</p><h2 id="conclusion" style="position: relative;"><a href="#conclusion" title="Conclusione" id="anchor-conclusion"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Conclusione</h2><p>In questo post, abbiamo esplorato come ModernBERT fa avanzare i modelli della famiglia BERT attraverso tre innovazioni chiave: la sua architettura deep-and-thin, il tokenizer ottimizzato e lo scaling efficiente utilizzando il weight tiling. Questi miglioramenti permettono a ModernBERT di offrire prestazioni eccezionali in dimensioni relativamente compatte, superando sia <code>RoBERTa-large</code> che <code>jina-XLM-RoBERTa</code> in vari task. ModernBERT dimostra che i miglioramenti architettonici possono contare più delle dimensioni dei parametri, aprendo le porte a modelli più efficienti. Il suo uso efficace del weight tiling mostra come lo scaling progressivo possa ridurre i costi di addestramento mantenendo o addirittura migliorando le prestazioni. Inoltre, il suo vocabolario compatto e le ottimizzazioni mirate suggeriscono crescenti opportunità per SLM specializzati in ambienti con risorse limitate.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categorie:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog tecnico</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Uffici</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Sunnyvale, California</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, Stati Uniti</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Berlino, Germania (sede centrale)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlino, Germania</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Pechino, Cina</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Livello 5, Edificio 6, No.48 Haidian West St. Pechino, Cina</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Shenzen, Cina</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">402 Piano 4, Fu'an Technology Building, Shenzhen, Cina</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fondazione di ricerca</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Lettore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Incorporamenti</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Riclassificazione</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Ricerca profonda</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Classificatore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Segmentatore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Documentazione API</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Ottieni la chiave API Jina</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Limite di velocità</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Stato dell'API</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Azienda</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Chi siamo</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contatta le vendite</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sala stampa</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programma di stagista</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Unisciti a noi</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Scarica il logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Termini</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sicurezza</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Termini &amp; Condizioni</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Privacy</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Gestisci i cookie</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
<!DOCTYPE html><html translate="no" dir="ltr" lang="it"><head><title>Un'analisi approfondita della tokenizzazione</title><meta charset="utf-8"><meta name="title" content="Un'analisi approfondita della tokenizzazione"><meta name="description" content="La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti più piccole per l'elaborazione. Quindi perché gli embedding vengono fatturati per token?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="og:title" content="Un'analisi approfondita della tokenizzazione"><meta property="og:description" content="La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti più piccole per l'elaborazione. Quindi perché gli embedding vengono fatturati per token?"><meta property="og:image" content="https://jina.ai/blog-banner/a-deep-dive-into-tokenization.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="twitter:title" content="Un'analisi approfondita della tokenizzazione"><meta property="twitter:description" content="La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti più piccole per l'elaborazione. Quindi perché gli embedding vengono fatturati per token?"><meta property="twitter:image" content="https://jina.ai/blog-banner/a-deep-dive-into-tokenization.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-0duIC24z.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-UyArmqdO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-Bo4niwzY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-DIwLu4T3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-aQYI3ruT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-cQkGmENj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-BYwPH3r9.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/assets/it-cPZrmCL2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-DE8MWCR8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-DJse3iPo.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-B9wsmYeb.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-BhNasYxs.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-pAdQXWNc.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-DrsLy1gD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-CzwoVK9b.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-DVj7y3JJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-B0HabzMk.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-DR5yJeGW.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-9wmqZonN.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-C6thIQs0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-hk4Z-hgl.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-BAP2ZZSj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-CT-e6toc.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-9YuXTbXO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DqqEgxOW.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-Bu64LrYb.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-DQcgrNe_.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-BDGHzqVG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-CJ98DRUM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-B7KU-Quq.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-CwOHw0a8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-6RbYrhKX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-DVd6Z5G8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-MwczcO1C.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-qBckrgSn.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-B0SesCkc.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-C1vDmaBY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Scott Martens"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-01-31T16:10:14.000+01:00"><meta property="article:modified_time" content="2024-08-14T11:38:01.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Un'analisi approfondita della tokenizzazione",
  "description": "La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti più piccole per l'elaborazione. Quindi perché gli embedding vengono fatturati per token?",
  "image": [
    "https://jina.ai/blog-banner/a-deep-dive-into-tokenization.webp"
  ],
  "datePublished": "2024-01-31T16:10:14.000+01:00",
  "dateModified": "2024-08-14T11:38:01.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Notizia</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Modelli</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_9c75fb53-eb99-4065-ac86-c0245d797d01" aria-label="Espandi &quot;Prodotti&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Prodotti</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_9c75fb53-eb99-4065-ac86-c0245d797d01" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Prodotti"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Ricerca profonda</div><div class="q-item__label q-item__label--caption text-caption">Cerca, leggi e ragiona finché non trovi la risposta migliore.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lettore</div><div class="q-item__label q-item__label--caption text-caption">Leggi gli URL e cerca sul web per ottenere LLM più approfonditi.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incorporamenti</div><div class="q-item__label q-item__label--caption text-caption">Incorporamenti multilingue multimodali di livello mondiale.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Riclassificazione</div><div class="q-item__label q-item__label--caption text-caption">Recupero neurale di livello mondiale per massimizzare la pertinenza della ricerca.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classificatore</div><div class="q-item__label q-item__label--caption text-caption">Classificazione zero-shot e few-shot per immagini e testo.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentatore</div><div class="q-item__label q-item__label--caption text-caption">Tagliare il testo lungo in blocchi ed effettuare la tokenizzazione.</div></div></a><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Documentazione API</div><div class="q-item__label q-item__label--caption text-caption">Generazione automatica di codice per il tuo IDE o LLM di Copilot</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_434282c4-2e5b-481a-bf03-688e11cd2cad" aria-label="Espandi &quot;Azienda&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Azienda</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_434282c4-2e5b-481a-bf03-688e11cd2cad" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Chi siamo</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contatta le vendite</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programma di stagista</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Unisciti a noi</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Scarica il logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Termini &amp; Condizioni</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Login"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Login</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_4f55f8f9-a7c9-4983-893e-4bb0315e1e20"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_4f55f8f9-a7c9-4983-893e-4bb0315e1e20" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_4f55f8f9-a7c9-4983-893e-4bb0315e1e20_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">tl;dr</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Parole, Token, Numeri</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Mappare il Linguaggio in Numeri</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Perché Tokenizziamo? E Perché in Questo Modo?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Stime Empiriche delle Dimensioni di Output dei Token</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Prendere sul serio i Token</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog tecnico</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">gennaio 31, 2024</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">Un'analisi approfondita della tokenizzazione</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">La tokenizzazione, nei modelli LLM, significa suddividere i testi di input in parti più piccole per l'elaborazione. Quindi perché gli embedding vengono fatturati per token?</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img" aria-label="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Scott Martens • 16 minuti letti</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>Ci sono molte barriere alla comprensione dei modelli AI, alcune piuttosto significative, che possono ostacolare l'implementazione dei processi AI. Ma la prima che molte persone incontrano è capire cosa intendiamo quando parliamo di <strong>token</strong>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/tokenizer"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Tokenizer API</div><div class="kg-bookmark-description">Free API to tokenize texts, count and get first/last-N tokens.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-tokenize-api.png" alt="" style="cursor: help;"></div></a></figure><p>Uno dei parametri pratici più importanti nella scelta di un modello linguistico AI è la dimensione della sua finestra di contesto — la dimensione massima del testo in input — che viene espressa in token, non in parole o caratteri o altre unità automaticamente riconoscibili.</p><p>Inoltre, i servizi di embedding sono tipicamente calcolati "per token", il che significa che i token sono importanti per comprendere la fattura.</p><p>Questo può essere molto confuso se non si ha chiaro cosa sia un token.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="Listino prezzi attuale di Jina Embeddings (febbraio 2024)." width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Listino prezzi attuale di Jina Embeddings (febbraio 2024). Nota che i prezzi sono indicati per "1M token".</span></figcaption></figure><p>Ma tra tutti gli aspetti confusi dell'AI moderna, i token sono probabilmente i meno complicati. Questo articolo cercherà di chiarire cosa sia la tokenizzazione, cosa fa e perché la facciamo in questo modo.</p><h2 id="tldr" style="position: relative;"><a href="#tldr" title="tl;dr" id="anchor-tldr"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>tl;dr</h2><p>Per coloro che desiderano o necessitano di una risposta rapida per calcolare quanti token acquistare da Jina Embeddings o una stima di quanti prevedono di dover acquistare, ecco le statistiche che state cercando.</p><h3 id="tokens-per-english-word" style="position: relative;"><a href="#tokens-per-english-word" title="Token per Parola Inglese" id="anchor-tokens-per-english-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Token per Parola Inglese</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Una chiamata all'API di Jina Embeddings v2 per i modelli inglesi utilizzerà <b><strong style="white-space: pre-wrap;">approssimativamente</strong></b> <b><strong style="white-space: pre-wrap;">il 10% in più</strong></b> di token rispetto al numero di parole nel testo, <b><strong style="white-space: pre-wrap;">più due token per embedding</strong></b>.</div></div><p>Durante i test empirici, descritti più avanti in questo articolo, una varietà di testi in inglese è stata convertita in token con un rapporto di circa il 10% in più di token rispetto alle parole, utilizzando i modelli solo inglese di Jina Embeddings. Questo risultato è stato piuttosto robusto.</p><p>I modelli Jina Embeddings v2 hanno una finestra di contesto di 8192 token. Questo significa che se passi a un modello Jina un testo in inglese più lungo di 7.400 parole, c'è una buona probabilità che venga troncato.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">La dimensione massima per l'input di <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 per l'inglese</strong></b> è approssimativamente <b><strong style="white-space: pre-wrap;">7.400 parole</strong></b>.</div></div><h3 id="tokens-per-chinese-character" style="position: relative;"><a href="#tokens-per-chinese-character" title="Token per Carattere Cinese" id="anchor-tokens-per-chinese-character"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Token per Carattere Cinese</h3><p>Per il cinese, i risultati sono più variabili. A seconda del tipo di testo, i rapporti variano da 0,6 a 0,75 token per carattere cinese (汉字). I testi in inglese dati a Jina Embeddings v2 per il cinese producono approssimativamente lo stesso numero di token di Jina Embeddings v2 per l'inglese: circa il 10% in più rispetto al numero di parole.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">La dimensione massima per l'input in cinese di <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 per cinese e inglese</strong></b> è approssimativamente <b><strong style="white-space: pre-wrap;">10.500 caratteri</strong></b> (<b><strong style="white-space: pre-wrap;">字数</strong></b>), o <b><strong style="white-space: pre-wrap;">da 0,6 a 0,75 token per carattere cinese, più due per embedding.</strong></b></div></div><h3 id="tokens-per-german-word" style="position: relative;"><a href="#tokens-per-german-word" title="Token per Parola Tedesca" id="anchor-tokens-per-german-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Token per Parola Tedesca</h3><p>I rapporti parola-token in tedesco sono più variabili rispetto all'inglese ma meno del cinese. A seconda del genere del testo, ho ottenuto in media dal 20% al 30% in più di token rispetto alle parole. Dare testi in inglese a Jina Embeddings v2 per tedesco e inglese usa alcuni token in più rispetto ai modelli solo inglese e cinese/inglese: dal 12% al 15% in più di token rispetto alle parole.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Jina Embeddings v2 per tedesco e inglese conterà <b><strong style="white-space: pre-wrap;">dal 20% al 30% in più di token rispetto alle parole, più due per embedding</strong></b>. La dimensione massima del contesto di input è approssimativamente <b><strong style="white-space: pre-wrap;">6.300 parole tedesche</strong></b>.</div></div><h3 id="caution" style="position: relative;"><a href="#caution" title="Attenzione!" id="anchor-caution"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Attenzione!</h3><p>Questi sono calcoli semplici, ma dovrebbero essere approssimativamente corretti per la maggior parte dei testi in linguaggio naturale e per la maggior parte degli utenti. In definitiva, possiamo solo promettere che il numero di token sarà sempre non superiore al numero di caratteri nel tuo testo, più due. In pratica sarà sempre molto meno di quello, ma non possiamo promettere in anticipo alcun conteggio specifico.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">I Risultati Possono Variare! </strong></b><br><br>Queste sono stime basate su calcoli statisticamente ingenui. Non garantiamo quanti token richiederà una particolare richiesta.</div></div><p>Se ti serve solo un consiglio su quanti token acquistare per Jina Embeddings, puoi fermarti qui. Altri modelli di embedding, di aziende diverse da Jina AI, potrebbero non avere gli stessi rapporti token-parola e token-carattere-cinese dei modelli Jina, ma in generale non saranno molto diversi.</p><p>Se vuoi capire il perché, il resto di questo articolo è un'analisi più approfondita della tokenizzazione per i modelli linguistici.</p><h2 id="words-tokens-numbers" style="position: relative;"><a href="#words-tokens-numbers" title="Parole, Token, Numeri" id="anchor-words-tokens-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Parole, Token, Numeri</h2><p>La tokenizzazione fa parte dell'elaborazione del linguaggio naturale da prima che esistessero i moderni modelli AI.</p><p>È un po' un cliché dire che tutto in un computer è solo un numero, ma è anche per lo più vero. Il linguaggio, tuttavia, non è naturalmente solo un insieme di numeri. Potrebbe essere parlato, composto da onde sonore, o scritto, fatto di segni su carta, o anche un'immagine di un testo stampato o un video di qualcuno che usa il linguaggio dei segni. Ma la maggior parte delle volte, quando parliamo di usare i computer per elaborare il linguaggio naturale, intendiamo testi composti da sequenze di caratteri: lettere (a, b, c, ecc.), numeri (0, 1, 2…), punteggiatura e spazi, in diverse lingue e codifiche testuali.</p><p>Gli ingegneri informatici le chiamano "stringhe".</p><p>I modelli linguistici AI prendono sequenze di numeri come input. Quindi, potresti scrivere la frase:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Ma, dopo la tokenizzazione, il modello AI riceve come input:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>La tokenizzazione è il processo di conversione di una stringa di input in una specifica sequenza di numeri che il tuo modello AI può comprendere.</p><p>Quando usi un modello AI tramite un'API web che addebita agli utenti per token, ogni richiesta viene convertita in una sequenza di numeri come quella sopra. Il numero di token nella richiesta è la lunghezza di quella sequenza di numeri. Quindi, chiedere a Jina Embeddings v2 for English di darti un embedding per "<em>What is today's weather in Berlin?</em>" ti costerà 11 token perché ha convertito quella frase in una sequenza di 11 numeri prima di passarla al modello AI.</p><p>I modelli AI basati sull'<a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">architettura Transformer</a> hanno una <strong>finestra di contesto</strong> di dimensione fissa misurata in token. A volte viene chiamata "finestra di input", "dimensione del contesto" o "lunghezza della sequenza" (specialmente sulla <a href="https://huggingface.co/spaces/mteb/leaderboard">classifica MTEB di Hugging Face</a>). Significa la dimensione massima del testo che il modello può vedere in una volta.</p><p>Quindi, se vuoi usare un modello di embedding, questa è la dimensione massima di input consentita.</p><p>I modelli Jina Embeddings v2 hanno tutti una finestra di contesto di 8.192 token. Altri modelli avranno finestre di contesto diverse (tipicamente più piccole). Questo significa che qualunque sia la quantità di testo che inserisci, il tokenizer associato a quel modello Jina Embeddings deve convertirlo in non più di 8.192 token.</p><h2 id="mapping-language-to-numbers" style="position: relative;"><a href="#mapping-language-to-numbers" title="Mappare il Linguaggio in Numeri" id="anchor-mapping-language-to-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mappare il Linguaggio in Numeri</h2><p>Il modo più semplice per spiegare la logica dei token è questo:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Un token è un numero che rappresenta una parte di una stringa.</div></div><p>Per i modelli di linguaggio naturale, la parte di stringa che un token rappresenta è una parola, una parte di una parola o un pezzo di punteggiatura. Gli spazi generalmente non ricevono alcuna rappresentazione esplicita nell'output del tokenizer.</p><p>La tokenizzazione fa parte di un gruppo di tecniche nell'elaborazione del linguaggio naturale chiamate <a href="https://en.wikipedia.org/wiki/Text_segmentation"><em>segmentazione del testo</em></a>, e il modulo che esegue la tokenizzazione è chiamato, molto logicamente, <strong>tokenizer</strong>.</p><p>Per mostrare come funziona la tokenizzazione, tokenizzeremo alcune frasi usando il più piccolo modello Jina Embeddings v2 per l'inglese: <code>jina-embeddings-v2-small-en</code>. L'altro modello solo inglese di Jina Embeddings — <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> — usa lo stesso tokenizer, quindi non ha senso scaricare megabyte extra di modello AI che non useremo in questo articolo.</p><p>Prima, installa il modulo <code>transformers</code> nel tuo ambiente Python o notebook. Usa ilIl flag <code>-U</code> assicura l'aggiornamento all'ultima versione poiché questo modello non funzionerà con alcune versioni precedenti:</p><pre class="hljs-copy-wrapper"><code class="language-bash hljs">pip install -U transformers
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Quindi, scarica <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> usando <code>AutoModel.from_pretrained</code>:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-small-en'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Per tokenizzare una stringa, usa il metodo <code>encode</code> dell'oggetto <code>tokenizer</code> del modello:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">model.tokenizer.encode(<span class="hljs-string">"What is today's weather in Berlin?"</span>)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato è una lista di numeri:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Per convertire questi numeri in forma di stringa, usa il metodo <code>convert_ids_to_tokens</code> dell'oggetto <code>tokenizer</code>:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">model.tokenizer.convert_ids_to_tokens([<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>])
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato è una lista di stringhe:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'what'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">"'"</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'in'</span>,
 <span class="hljs-string">'berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Nota che il tokenizer del modello ha:</p><ol><li>Aggiunto <code>[CLS]</code> all'inizio e <code>[SEP]</code> alla fine. Questo è necessario per ragioni tecniche e significa che <strong>ogni richiesta di embedding costerà due token extra</strong>, oltre a qualsiasi numero di token richieda il testo.</li><li>Separato la punteggiatura dalle parole, trasformando "<em>Berlin?</em>" in: <code>berlin</code> e <code>?</code>, e "<em>today's</em>" in <code>today</code>, <code>'</code>, e <code>s</code>.</li><li>Messo tutto in minuscolo. Non tutti i modelli lo fanno, ma questo può aiutare durante l'addestramento quando si usa l'inglese. Potrebbe essere meno utile in lingue dove la capitalizzazione ha un significato diverso.</li></ol><p>Diversi algoritmi di conteggio parole in programmi diversi potrebbero contare le parole in questa frase in modo differente. OpenOffice la conta come sei parole. L'algoritmo di segmentazione del testo Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) conta sette parole. Altri software potrebbero arrivare a numeri diversi, a seconda di come gestiscono la punteggiatura e i clitici come "'s".</p><p>Il tokenizer per questo modello produce nove token per quelle sei o sette parole, più i due token extra necessari per ogni richiesta.</p><p>Ora, proviamo con un nome di luogo meno comune di Berlino:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"I live in Kinshasa."</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'kin'</span>, <span class="hljs-string">'##sha'</span>, <span class="hljs-string">'##sa'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il nome "Kinshasa" è diviso in tre token: <code>kin</code>, <code>##sha</code>, e <code>##sa</code>. Il <code>##</code> indica che questo token non è l'inizio di una parola.</p><p>Se diamo al tokenizer qualcosa di completamente estraneo, il numero di token rispetto al numero di parole aumenta ancora di più:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"Klaatu barada nikto"</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)

[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'##la'</span>, <span class="hljs-string">'##at'</span>, <span class="hljs-string">'##u'</span>, <span class="hljs-string">'bar'</span>, <span class="hljs-string">'##ada'</span>, <span class="hljs-string">'nik'</span>, <span class="hljs-string">'##to'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Tre parole diventano otto token, più i token <code>[CLS]</code> e <code>[SEP]</code>.</p><p>La tokenizzazione in tedesco è simile. Con il modello <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/" rel="noreferrer">Jina Embeddings v2 per il tedesco</a>, possiamo tokenizzare una traduzione di "What is today's weather in Berlin?" allo stesso modo del modello inglese.</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">german_model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-base-de'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
token_ids = german_model.tokenizer.encode(<span class="hljs-string">"Wie wird das Wetter heute in Berlin?"</span>)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Wie'</span>, <span class="hljs-string">'wird'</span>, <span class="hljs-string">'das'</span>, <span class="hljs-string">'Wetter'</span>, <span class="hljs-string">'heute'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'Berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Questo tokenizer è leggermente diverso da quello inglese in quanto <code>&lt;s&gt;</code> e <code>&lt;/s&gt;</code> sostituiscono <code>[CLS]</code> e <code>[SEP]</code> ma svolgono la stessa funzione. Inoltre, il testo non viene normalizzato in maiuscole/minuscole - maiuscole e minuscole rimangono come scritte - perché in tedesco la capitalizzazione ha un significato diverso rispetto all'inglese.</p><p>(Per semplificare questa presentazione, ho rimosso un carattere speciale che indica l'inizio di una parola.)</p><p>Ora, proviamo con una frase più complessa <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html">da un testo giornalistico</a>:</p><blockquote>Ein Großteil der milliardenschweren Bauern-Subventionen bleibt liegen – zu genervt sind die Landwirte von bürokratischen Gängelungen und Regelwahn.</blockquote><pre class="hljs-copy-wrapper"><code class="hljs language-python">sentence = <span class="hljs-string">"""
Ein Großteil der milliardenschweren Bauern-Subventionen
bleibt liegen – zu genervt sind die Landwirte von 
bürokratischen Gängelungen und Regelwahn.
"""</span>
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato tokenizzato:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Ein'</span>, <span class="hljs-string">'Großteil'</span>, <span class="hljs-string">'der'</span>, <span class="hljs-string">'mill'</span>, <span class="hljs-string">'iarden'</span>, <span class="hljs-string">'schwer'</span>, 
 <span class="hljs-string">'en'</span>, <span class="hljs-string">'Bauern'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'Sub'</span>, <span class="hljs-string">'ventionen'</span>, <span class="hljs-string">'bleibt'</span>, <span class="hljs-string">'liegen'</span>, 
 <span class="hljs-string">'–'</span>, <span class="hljs-string">'zu'</span>, <span class="hljs-string">'gen'</span>, <span class="hljs-string">'ervt'</span>, <span class="hljs-string">'sind'</span>, <span class="hljs-string">'die'</span>, <span class="hljs-string">'Landwirte'</span>, <span class="hljs-string">'von'</span>, 
 <span class="hljs-string">'büro'</span>, <span class="hljs-string">'krat'</span>, <span class="hljs-string">'ischen'</span>, <span class="hljs-string">'Gän'</span>, <span class="hljs-string">'gel'</span>, <span class="hljs-string">'ungen'</span>, <span class="hljs-string">'und'</span>, <span class="hljs-string">'Regel'</span>, 
 <span class="hljs-string">'wahn'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Qui si vede che molte parole tedesche sono state suddivise in pezzi più piccoli e non necessariamente seguendo le regole grammaticali tedesche. Il risultato è che una parola tedesca lunga, che verrebbe contata come una sola parola da un contatore di parole, potrebbe corrispondere a un numero qualsiasi di token per il modello AI di Jina.</p><p>Facciamo lo stesso in cinese, traducendo "What is today's weather in Berlin?" come:</p><blockquote>柏林今天的天气怎么样？</blockquote><pre class="hljs-copy-wrapper"><code class="hljs language-makefile">chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(<span class="hljs-string">"柏林今天的天气怎么样？"</span>)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato tokenizzato:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'柏林'</span>, <span class="hljs-string">'今天的'</span>, <span class="hljs-string">'天气'</span>, <span class="hljs-string">'怎么样'</span>, <span class="hljs-string">'？'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>In cinese, solitamente non ci sono spazi tra le parole nel testo scritto, ma il tokenizer di Jina Embeddings frequentemente unisce più caratteri cinesi insieme:</p>

<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Significato</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏林</td>
<td>Bólín</td>
<td>Berlino</td>
</tr>
<tr>
<td>今天的</td>
<td>jīntiān de</td>
<td>di oggi</td>
</tr>
<tr>
<td>天气</td>
<td>tiānqì</td>
<td>tempo</td>
</tr>
<tr>
<td>怎么样</td>
<td>zěnmeyàng</td>
<td>come</td>
</tr>
</tbody>
</table>

<p>Usiamo una frase più complessa <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7">da un giornale di Hong Kong</a>:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">sentence = <span class="hljs-string">"""
新規定執行首日，記者在下班高峰前的下午5時來到廣州地鐵3號線，
從繁忙的珠江新城站啟程，向機場北方向出發。
"""</span>
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>(Traduzione: <em>"Il primo giorno dell'entrata in vigore delle nuove norme, questo giornalista è arrivato alla Linea 3 della Metropolitana di Guangzhou alle 17:00, durante l'ora di punta, partendo dalla Stazione Zhujiang New Town in direzione nord verso l'aeroporto."</em>)</p><p>Il risultato:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'新'</span>, <span class="hljs-string">'規定'</span>, <span class="hljs-string">'執行'</span>, <span class="hljs-string">'首'</span>, <span class="hljs-string">'日'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'記者'</span>, <span class="hljs-string">'在下'</span>, <span class="hljs-string">'班'</span>, 
 <span class="hljs-string">'高峰'</span>, <span class="hljs-string">'前的'</span>, <span class="hljs-string">'下午'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'時'</span>, <span class="hljs-string">'來到'</span>, <span class="hljs-string">'廣州'</span>, <span class="hljs-string">'地'</span>, <span class="hljs-string">'鐵'</span>, <span class="hljs-string">'3'</span>, 
 <span class="hljs-string">'號'</span>, <span class="hljs-string">'線'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'從'</span>, <span class="hljs-string">'繁忙'</span>, <span class="hljs-string">'的'</span>, <span class="hljs-string">'珠江'</span>, <span class="hljs-string">'新城'</span>, <span class="hljs-string">'站'</span>, <span class="hljs-string">'啟'</span>, 
 <span class="hljs-string">'程'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'向'</span>, <span class="hljs-string">'機場'</span>, <span class="hljs-string">'北'</span>, <span class="hljs-string">'方向'</span>, <span class="hljs-string">'出發'</span>, <span class="hljs-string">'。'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Questi token non corrispondono a nessun dizionario specifico di parole cinesi (词典). Per esempio, "啟程" - <em>qǐchéng</em> (partire, mettersi in viaggio) sarebbe tipicamente categorizzato come una singola parola ma qui è diviso nei suoi due caratteri costituenti. Similmente, "在下班" sarebbe normalmente riconosciuto come due parole, con la divisione tra "在" - <em>zài</em> (in, durante) e "下班" - <em>xiàbān</em> (fine della giornata lavorativa, ora di punta), non tra "在下" e "班" come ha fatto il tokenizer in questo caso.</p><p>In tutte e tre le lingue, i punti in cui il tokenizer divide il testo non sono direttamente correlati ai punti logici in cui un lettore umano li dividerebbe.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">L'algoritmo di tokenizzazione non utilizza un dizionario convenzionale consapevole della lingua, quindi il suo comportamento non corrisponde a come gli umani contano le parole.</div></div><p>Questa non è una caratteristica specifica dei modelli Jina Embeddings. Questo approccio alla tokenizzazione è quasi universale nello sviluppo di modelli AI. Sebbene due diversi modelli AI possano non avere tokenizer identici, nello stato attuale dello sviluppo, praticamente tutti useranno tokenizer con questo tipo di comportamento.</p><p>La prossima sezione discuterà l'algoritmo specifico utilizzato nella tokenizzazione e la logica alla sua base.</p><h2 id="why-do-we-tokenize-and-why-this-way" style="position: relative;"><a href="#why-do-we-tokenize-and-why-this-way" title="Perché Tokenizziamo? E Perché in Questo Modo?" id="anchor-why-do-we-tokenize-and-why-this-way"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Perché Tokenizziamo? E Perché in Questo Modo?</h2><p>I modelli linguistici AI prendono in input sequenze di numeri che rappresentano sequenze di testo, ma succede qualcosa in più prima di eseguire la rete neurale sottostante e creare un embedding. Quando viene presentata una lista di numeri che rappresentano piccole sequenze di testo, il modello cerca ogni numero in un dizionario interno che memorizza un vettore unico per ogni numero. Poi li combina, e questo diventa l'input per la rete neurale.</p><p>Questo significa che il tokenizer <strong>deve</strong> essere in grado di convertire <strong><em>qualsiasi</em></strong> testo di input che gli diamo in token che appaiono nel dizionario dei vettori di token del modello. Se prendessimo i nostri token da un dizionario convenzionale, la prima volta che incontrassimo un errore di ortografia o un nome proprio raro o una parola straniera, l'intero modello si fermerebbe. Non potrebbe processare quell'input.</p><p>Nell'elaborazione del linguaggio naturale, questo è chiamato il problema del fuori-vocabolario (OOV), ed è pervasivo in tutti i tipi di testo e in tutte le lingue. Ci sono alcune strategie per affrontare il problema OOV:</p><ol><li>Ignorarlo. Sostituire tutto ciò che non è nel dizionario con un token "sconosciuto".</li><li>Aggirarlo. Invece di usare un dizionario che mappa sequenze di testo a vettori, usarne uno che mappa <em>singoli caratteri</em> a vettori. L'inglese usa solo 26 lettere la maggior parte delle volte, quindi questo deve essere più piccolo e più robusto contro i problemi OOV rispetto a qualsiasi dizionario.</li><li>Trovare sottosequenze frequenti nel testo, metterle nel dizionario e usare caratteri (token a singola lettera) per tutto il resto.</li></ol><p>La prima strategia significa che molte informazioni importanti vengono perse. Il modello non può nemmeno imparare dai dati che ha visto se prendono la forma di qualcosa che non è nel dizionario. Molte cose nel testo ordinario non sono presenti nemmeno nei dizionari più grandi.</p><p>La seconda strategia è possibile, e i ricercatori l'hanno investigata. Tuttavia, significa che il modello deve accettare molti più input e deve imparare molto di più. Questo significa un modello molto più grande e molti più dati di addestramento per un risultato che non si è mai dimostrato migliore della terza strategia.</p><p>I modelli linguistici AI implementano praticamente tutti la terza strategia in qualche forma. La maggior parte usa qualche variante dell'<a href="https://huggingface.co/learn/nlp-course/chapter6/6">algoritmo Wordpiece</a> <a href="https://ieeexplore.ieee.org/document/6289079">[Schuster e Nakajima 2012]</a> o una tecnica simile chiamata <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/">Senrich et al. 2016</a>] Questi algoritmi sono <em>language-agnostic</em>. Ciò significa che funzionano allo stesso modo per tutte le lingue scritte senza alcuna conoscenza oltre a un elenco completo dei possibili caratteri. Sono stati progettati per modelli multilingue come BERT di Google che prendono qualsiasi input dal web scraping - centinaia di lingue e testi diversi dal linguaggio umano come programmi per computer - in modo da poter essere addestrati senza fare linguistica complicata.</p><p>Alcune ricerche mostrano miglioramenti significativi utilizzando tokenizer più specifici e consapevoli della lingua. [<a href="https://aclanthology.org/2021.acl-long.243/">Rust et al. 2021</a>] Ma costruire tokenizer in questo modo richiede tempo, denaro e competenza. Implementare una strategia universale come BPE o Wordpiece è molto più economico e facile.</p><p>Tuttavia, di conseguenza, non c'è modo di sapere quanti token rappresenta un testo specifico se non facendolo passare attraverso un tokenizer e poi contando il numero di token che ne escono. Poiché la sottoseqeunza più piccola possibile di un testo è una lettera, si può essere sicuri che il numero di token non sarà maggiore del numero di caratteri (meno gli spazi) più due.</p><p>Per ottenere una buona stima, dobbiamo sottoporre molto testo al nostro tokenizer e calcolare empiricamente quanti token otteniamo in media, rispetto a quante parole o caratteri inseriamo. Nella prossima sezione, faremo alcune misurazioni empiriche non molto sistematiche per tutti i modelli Jina Embeddings v2 attualmente disponibili.</p><h2 id="empirical-estimates-of-token-output-sizes" style="position: relative;"><a href="#empirical-estimates-of-token-output-sizes" title="Stime Empiriche delle Dimensioni di Output dei Token" id="anchor-empirical-estimates-of-token-output-sizes"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Stime Empiriche delle Dimensioni di Output dei Token</h2><p>Per l'inglese e il tedesco, ho utilizzato l'algoritmo di segmentazione del testo Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) per ottenere il conteggio delle parole per i testi. Questo algoritmo è ampiamente utilizzato per selezionare frammenti di testo quando si fa doppio clic su qualcosa. È la cosa più vicina disponibile a un contatore di parole universale oggettivo.</p><p>Ho installato la <a href="https://pypi.org/project/polyglot/">libreria polyglot</a> in Python, che implementa questo segmentatore di testo:</p><pre class="hljs-copy-wrapper"><code class="language-bash hljs">pip install -U polyglot
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Per ottenere il conteggio delle parole di un testo, puoi utilizzare un codice come questo snippet:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> polyglot.text <span class="hljs-keyword">import</span> Text

txt = <span class="hljs-string">"What is today's weather in Berlin?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(Text(txt).words))
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Il risultato dovrebbe essere <code>7</code>.</p><p>Per ottenere un conteggio dei token, segmenti del testo sono stati passati ai tokenizer di vari modelli Jina Embeddings, come descritto di seguito, e ogni volta, ho sottratto due dal numero di token restituiti.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text">I conteggi dei token elencati qui <b><strong style="white-space: pre-wrap;">non includono</strong></b> i due token extra all'inizio e alla fine di ogni testo tokenizzato.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" style="position: relative;"><a href="#english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" title="Inglese
(jina-embeddings-v2-small-en e jina-embeddings-v2-base-en)" id="anchor-english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Inglese<br>(<code>jina-embeddings-v2-small-en</code> e <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>)</h3><p>Per calcolare le medie, ho scaricato due corpora di testo inglese da <a href="https://wortschatz.uni-leipzig.de/en" rel="noreferrer">Wortschatz Leipzig</a>, una collezione di corpora liberamente scaricabili in diverse lingue e configurazioni ospitata dall'Università di Leipzig:</p><ul><li>Un corpus di un milione di frasi di notizie in inglese del 2020 (<code>eng_news_2020_1M</code>)</li><li>Un corpus di un milione di frasi da <a href="https://en.wikipedia.org/">Wikipedia inglese</a> del 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Entrambi si possono trovare sulla loro <a href="https://wortschatz.uni-leipzig.de/en/download/English">pagina di download inglese</a>.</p><p>Per diversità, ho anche scaricato la <a href="https://www.gutenberg.org/ebooks/135">traduzione di Hapgood de <em>I Miserabili</em> di Victor Hugo</a> da Project Gutenberg, e una copia della Bibbia di Re Giacomo, tradotta in inglese nel 1611.</p><p>Per tutti e quattro i testi, ho contato le parole usando il segmentatore Unicode implementato in <code>polyglot</code>, poi ho contato i token creati da <code>jina-embeddings-v2-small-en</code>, sottraendo due token per ogni richiesta di tokenizzazione. I risultati sono i seguenti:</p>

<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Testo</th><th id="|<X;" class="simple-table-header-color simple-table-header">Conteggio parole<br>(Segmentatore Unicode)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Conteggio token<br>(Jina Embeddings v2 <br>per inglese)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class=""><code>eng_news_2020_1M</code></td><td id="|<X;" class="">22.825.712</td><td id="GHal" class="">25.270.581</td><td id="h]mu" class="">1,107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class=""><code>eng_wikipedia_2016_1M</code></td><td id="|<X;" class="">24.243.607</td><td id="GHal" class="">26.813.877</td><td id="h]mu" class="">1,106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class=""><code>les_miserables_en</code></td><td id="|<X;" class="">688.911</td><td id="GHal" class="">764.121</td><td id="h]mu" class="">1,109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class=""><code>kjv_bible</code></td><td id="|<X;" class="">1.007.651</td><td id="GHal" class="">1.099.335</td><td id="h]mu" class="">1,091</td></tr></tbody></table>

<p>L'uso di numeri precisi non significa che questo sia un risultato preciso. Il fatto che documenti di generi così diversi abbiano tutti tra il 9% e l'11% di token in più rispetto alle parole indica che probabilmente ci si può aspettare circa il 10% di token in più rispetto alle parole, come misurato dal segmentatore Unicode. I word processor spesso non contano la punteggiatura, mentre il segmentatore Unicode lo fa, quindi non ci si può aspettare che il conteggio delle parole del software da ufficio corrisponda necessariamente a questo.</p><h3 id="german-jina-embeddings-v2-base-de" style="position: relative;"><a href="#german-jina-embeddings-v2-base-de" title="Tedesco
(jina-embeddings-v2-base-de)" id="anchor-german-jina-embeddings-v2-base-de"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tedesco<br>(<a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-de" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-de</span></a>)</h3><p>Per il tedesco, ho scaricato tre corpora dalla <a href="https://wortschatz.uni-leipzig.de/en/download/German">pagina tedesca di Wortschatz Leipzig</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> — Un milione di frasi da una miscela bilanciata di testi di diversi generi, risalenti al 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> — Un milione di frasi di testo giornalistico del 2019.</li><li><code>deu_wikipedia_2021_1M</code> — Un milione di frasi estratte dalla Wikipedia tedesca nel 2021.</li></ul><p>E per diversità, ho anche scaricato tutti e <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata">tre i volumi del <em>Kapital</em> di Karl Marx</a> dal <a href="https://www.deutschestextarchiv.de/" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>Ho quindi seguito la stessa procedura utilizzata per l'inglese:</p>

<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Testo</th><th id="|qln" class="simple-table-header-color simple-table-header">Conteggio parole<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Conteggio token<br>(Jina Embeddings v2 <br>per tedesco e inglese)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class="">7.924.024</td><td id="YXZX" class="">9.772.652</td><td id="oEoQ" class="">1,234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class="">17.949.120</td><td id="YXZX" class="">21.711.555</td><td id="oEoQ" class="">1,210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class="">17.999.482</td><td id="YXZX" class="">22.654.901</td><td id="oEoQ" class="">1,259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class="" style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class="">784.336</td><td id="YXZX" class="">1.011.377</td><td id="oEoQ" class="">1,289</td></tr></tbody></table>

<p>Questi risultati hanno una dispersione maggiore rispetto al modello solo inglese ma suggeriscono comunque che il testo tedesco produrrà, in media, dal 20% al 30% di token in più rispetto alle parole.</p><p>I testi inglesi producono più token con il tokenizzatore tedesco-inglese rispetto a quello solo inglese:</p>

<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc<l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Testo</th><th id="R@A^" class="simple-table-header-color simple-table-header">Conteggio parole<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Conteggio token<br>(Jina Embeddings v2 <br>per tedesco e inglese)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Rapporto token/parole<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class="">24.243.607</td><td id="UUfl" class="">27.758.535</td><td id="iTZS" class="">1,145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class="">22.825.712</td><td id="UUfl" class="">25.566.921</td><td id="iTZS" class="">1,120</td></tr></tbody></table>

<p>Ci si dovrebbe aspettare di aver bisogno del 12-15% di token in più rispetto alle parole per incorporare testi inglesi con il modello bilingue tedesco/inglese rispetto a quello solo inglese.</p><h3 id="chinese-jina-embeddings-v2-base-zh" style="position: relative;"><a href="#chinese-jina-embeddings-v2-base-zh" title="Cinese
(jina-embeddings-v2-base-zh)" id="anchor-chinese-jina-embeddings-v2-base-zh"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Cinese<br>(<a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-zh" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-zh</span></a>)</h3><p>Il cinese è tipicamente scritto senza spazi e non aveva una nozione tradizionale di "parole" prima del XX secolo. Di conseguenza, la dimensione di un testo cinese viene tipicamente misurata in caratteri (<strong>字数</strong>). Quindi, invece di utilizzare il segmentatore Unicode, ho misurato la lunghezza dei testi cinesi rimuovendo tutti gli spazi e ottenendo semplicemente la lunghezza dei caratteri.</p><p>Ho scaricato tre corpora dalla <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese">pagina del corpus cinese di Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> — Un milione di frasi dalla Wikipedia in lingua cinese, estratte nel 2018.</li><li><code>zho_news_2007-2009_1M</code> — Un milione di frasi da fonti di notizie cinesi, raccolte dal 2007 al 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> — Un milione di frasi da fonti di notizie che utilizzano esclusivamente caratteri cinesi tradizionali (繁體字).</li></ul><p>Inoltre, per avere una maggiore diversità, ho utilizzato anche <em>La vera storia di Ah Q</em> (阿Q正傳), una novella di Lu Xun (魯迅) scritta all'inizio degli anni '20. Ho scaricato la <a href="https://www.gutenberg.org/ebooks/25332">versione in caratteri tradizionali da Project Gutenberg</a>.</p>

<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Testo</th><th id="CaUc" class="simple-table-header-color simple-table-header">Conteggio caratteri<br>(字数)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Conteggio token<br>(Jina Embeddings v2 <br>per cinese e inglese)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Rapporto token/caratteri<br>(a 3 decimali)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class="">45.116.182</td><td id="CQ{d" class="">29.193.028</td><td id="_};C" class="">0,647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class="">44.295.314</td><td id="CQ{d" class="">28.108.090</td><td id="_};C" class="">0,635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class="" style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class="">54.585.819</td><td id="CQ{d" class="">40.290.982</td><td id="_};C" class="">0,738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class="" style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class="">41.268</td><td id="CQ{d" class="">25.346</td><td id="_};C" class="">0,614</td></tr></tbody></table>

<p>Questa variazione nei rapporti token-caratteri è inaspettata, e in particolare l'anomalia nel corpus di caratteri tradizionali merita ulteriori indagini. Tuttavia, possiamo concludere che per il cinese, ci si deve aspettare di aver bisogno di <em>meno</em> token rispetto al numero di caratteri nel testo. A seconda del contenuto, ci si può aspettare di aver bisogno del 25-40% in meno.</p><p>I testi in inglese in Jina Embeddings v2 per cinese e inglese hanno prodotto approssimativamente lo stesso numero di token come nel modello solo inglese:</p>

<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="<rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class="" style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class="" style="width:165.3125px">24.243.607</td><td id="=]?F" class="">26.890.176</td><td id="<rlw" class="">1,109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class="" style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class="" style="width:165.3125px">22.825.712</td><td id="=]?F" class="">25.060.352</td><td id="<rlw" class="">1,097</td></tr></tbody></table>

<h2 id="taking-tokens-seriously" style="position: relative;"><a href="#taking-tokens-seriously" title="Prendere sul serio i Token" id="anchor-taking-tokens-seriously"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Prendere sul serio i Token</h2><p>I token sono un'importante impalcatura per i modelli di linguaggio AI, e la ricerca in questo settore è in corso.</p><p>Uno dei campi in cui i modelli AI si sono dimostrati rivoluzionari è la scoperta che sono molto robusti contro i dati rumorosi. Anche se un particolare modello non utilizza la strategia di tokenizzazione ottimale, se la rete è abbastanza grande, con dati sufficienti e adeguatamente addestrata, può imparare a fare la cosa giusta anche da input imperfetti.</p><p>Di conseguenza, viene speso molto meno sforzo nel migliorare la tokenizzazione rispetto ad altre aree, ma questo potrebbe cambiare.</p><p>Come utente di embeddings, che li acquista tramite una <a href="https://jina.ai/embeddings/">API come Jina Embeddings</a>, non puoi sapere esattamente quanti token ti serviranno per un compito specifico e potresti dover fare alcuni test per ottenere numeri precisi. Ma le stime fornite qui — circa il 110% del conteggio delle parole per l'inglese, circa il 125% del conteggio delle parole per il tedesco e circa il 70% del conteggio dei caratteri per il cinese — dovrebbero essere sufficienti per un budget di base.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categorie:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog tecnico</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fit%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Uffici</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Sunnyvale, California</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, Stati Uniti</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Berlino, Germania (sede centrale)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlino, Germania</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Pechino, Cina</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Livello 5, Edificio 6, No.48 Haidian West St. Pechino, Cina</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Shenzen, Cina</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">402 Piano 4, Fu'an Technology Building, Shenzhen, Cina</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fondazione di ricerca</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Ricerca profonda</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Lettore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Incorporamenti</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Riclassificazione</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Classificatore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Segmentatore</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Documentazione API</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Ottieni la chiave API Jina</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Limite di velocità</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Stato dell'API</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Azienda</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Chi siamo</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contatta le vendite</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sala stampa</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programma di stagista</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Unisciti a noi</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Scarica il logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Termini</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sicurezza</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Termini &amp; Condizioni</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Privacy</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Gestisci i cookie</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
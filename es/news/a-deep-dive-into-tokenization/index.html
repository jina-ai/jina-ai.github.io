<!DOCTYPE html><html translate="no" dir="ltr" lang="es"><head><title>Un análisis profundo de la tokenización</title><meta charset="utf-8"><meta name="title" content="Un análisis profundo de la tokenización"><meta name="description" content="La tokenización, en los LLMs, significa dividir los textos de entrada en partes más pequeñas para su procesamiento. Entonces, ¿por qué los embeddings se cobran por token?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="og:title" content="Un análisis profundo de la tokenización"><meta property="og:description" content="La tokenización, en los LLMs, significa dividir los textos de entrada en partes más pequeñas para su procesamiento. Entonces, ¿por qué los embeddings se cobran por token?"><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="twitter:title" content="Un análisis profundo de la tokenización"><meta property="twitter:description" content="La tokenización, en los LLMs, significa dividir los textos de entrada en partes más pequeñas para su procesamiento. Entonces, ¿por qué los embeddings se cobran por token?"><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-Dk2hbcOb.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-DkC9LM0F.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-BfM6GO4d.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-D8xifkDj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-mFss399z.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-Cfm7YbE-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-BTKVnlIr.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-Bigt9Xdz.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/es-DJJjFCSD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-Ce8K-1K1.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpace-BLZKeq-K.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-DZlXEru1.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SeFoComponent-B1eDN4cG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTabs-DYJwmdmj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-Dk0qPrZJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-5Dpf8GQy.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-C_1LvoUS.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/format-dqFa3MV6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnGroup-BIETBGBJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/orderBy-khqHYZlR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/finetune-CYsla9xN.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-Ys4TvDvE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-meta-bIpZ9ps-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/embedding-CPgoxB8r.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTable-C4209MHj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-fullscreen-Brz150L-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-fjYJGvvT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QForm-B5ZpajNz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-BQixWPhj.js"><link rel="stylesheet" crossorigin="" href="/assets/QForm-M_nMOs0J.css"><link rel="stylesheet" crossorigin="" href="/assets/SeFoComponent-F-ymhHEr.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-CHOtLOms.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-BMR_OewU.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-8v5w6aPt.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-DhdtR0B6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-CA4-1Pi1.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-BJh2mfx5.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-BbyyOnWb.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-BYg3bc8w.css"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-DYFCjuWX.css"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-01-31T16:10:14.000+01:00"><meta property="article:modified_time" content="2024-08-14T11:38:01.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Un análisis profundo de la tokenización",
  "description": "La tokenización, en los LLMs, significa dividir los textos de entrada en partes más pequeñas para su procesamiento. Entonces, ¿por qué los embeddings se cobran por token?",
  "image": [
    "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"
  ],
  "datePublished": "2024-01-31T16:10:14.000+01:00",
  "dateModified": "2024-08-14T11:38:01.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><link prerender-ignore rel=preconnect href=//api.usercentrics.eu><link prerender-ignore rel=preconnect href=//privacy-proxy.usercentrics.eu><link prerender-ignore rel=preload href=//app.usercentrics.eu/browser-ui/latest/loader.js as=script><link prerender-ignore rel=preload href=//privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js as=script><script prerender-ignore id=usercentrics-cmp data-settings-id=w5v6v2pJsC3wdR src=https://app.usercentrics.eu/browser-ui/latest/loader.js async></script><script prerender-ignore src=https://privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div class="q-space"></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">notifications</i></div><div class="q-item__section column q-item__section--main justify-center">Noticias</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_d275dd59-e1c8-4b90-9522-2166d36a9fc4" aria-label="Expandir &quot;Productos&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">box</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Productos</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_d275dd59-e1c8-4b90-9522-2166d36a9fc4" style="display: none;"><div class="q-list q-list--dark" role="list" label="Productos"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Para Empresas</span><div><div class="q-chip row inline no-wrap items-center q-chip--dense q-chip--outline q-chip--square q-chip--dark q-dark cursor-pointer" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">⇧1</div></div></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--no-uppercase q-btn--dense" tabindex="0" type="button" style="font-size: 8px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="block">Maximizar</span></span></button></div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incrustaciones</div><div class="q-item__label q-item__label--caption text-caption">Integraciones multilingües y multimodales de clase mundial.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">reclasificador</div><div class="q-item__label q-item__label--caption text-caption">Recuperador neuronal de clase mundial para maximizar la relevancia de la búsqueda.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lector</div><div class="q-item__label q-item__label--caption text-caption">Lea las URL y busque en la web para obtener una base más sólida para su LLM.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Clasificador</div><div class="q-item__label q-item__label--caption text-caption">Clasificación de cero disparos y pocos disparos para imágenes y texto.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentador</div><div class="q-item__label q-item__label--caption text-caption">Corta el texto largo en fragmentos y haz tokenización.</div></div></a><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Para usuarios avanzados</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://promptperfect.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://promptperfect.jina.ai/PromptPerfect-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">PromptPerfect</div><div class="q-item__label q-item__label--caption text-caption">Herramienta principal para ingeniería rápida</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_1d2a9d4f-c7db-4b5a-be20-0b16b933df0b" aria-label="Expandir &quot;Más herramientas para usuarios avanzados&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Más herramientas para usuarios avanzados</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_1d2a9d4f-c7db-4b5a-be20-0b16b933df0b" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://scenex.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://scenex.jina.ai/SceneX - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">SceneXplain</div><div class="q-item__label q-item__label--caption text-caption">Solución líder de IA para subtítulos de imágenes y resúmenes de vídeos</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://bestbanner.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://bestbanner.jina.ai/bestbanner-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">BestBanner</div><div class="q-item__label q-item__label--caption text-caption">¡Blog a banner, sin las indicaciones!</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://chat.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://chat.jina.ai/JinaChat - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JinaChat</div><div class="q-item__label q-item__label--caption text-caption">Más modalidad, más memoria, menos costo</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://rationale.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://rationale.jina.ai/Rationale-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Rationale</div><div class="q-item__label q-item__label--caption text-caption">Las mejores herramientas de toma de decisiones de IA</div></div></a></div></div></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_b6e643e2-d394-4408-9e9c-b4f336417f2f" aria-label="Expandir &quot;Compañía&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon" aria-hidden="true" role="presentation"><img src="/J.svg"></i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Compañía</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_b6e643e2-d394-4408-9e9c-b4f336417f2f" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 56px;"><main data-v-692f7f2b="" class="q-page" style="min-height: 100vh;"><div data-v-692f7f2b="" class="row full-width relative-position justify-end"><div data-v-692f7f2b="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-692f7f2b="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">tl;dr</div></div></div><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">Palabras, Tokens, Números</div></div></div><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">Mapeando Lenguaje a Números</div></div></div><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">¿Por qué Tokenizamos? ¿Y Por qué de Esta Manera?</div></div></div><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">Estimaciones Empíricas de los Tamaños de Salida de Tokens</div></div></div><div data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-692f7f2b="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-692f7f2b="" class="q-item__label">Tomando los Tokens en Serio</div></div></div></div></div><div data-v-692f7f2b="" class="col-12 col-md-10 col-lg-12"><div data-v-692f7f2b="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></div><div data-v-692f7f2b="" class="row justify-center"><div data-v-692f7f2b="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-692f7f2b="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">enero 31, 2024</div><h1 data-v-692f7f2b="" class="text-weight-medium text-center q-px-md my-title">Un análisis profundo de la tokenización</h1><div data-v-692f7f2b="" class="col row justify-center"><div data-v-692f7f2b="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">La tokenización, en los LLMs, significa dividir los textos de entrada en partes más pequeñas para su procesamiento. Entonces, ¿por qué los embeddings se cobran por token?</div></div><div data-v-692f7f2b="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-692f7f2b="" class="q-img q-img--menu" role="img" aria-label="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-692f7f2b="" class="row justify-center"><div data-v-692f7f2b="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-692f7f2b="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-692f7f2b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-692f7f2b="" class="q-item__label">Scott Martens • 16 minutos de lectura</div></div></div></div><div data-v-692f7f2b="" class="row justify-center"><div data-v-692f7f2b="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-692f7f2b="" class="article"><section data-v-692f7f2b="" class="gh-content"><p>Hay muchas barreras para entender los modelos de IA, algunas de ellas bastante grandes, y pueden obstaculizar la implementación de procesos de IA. Pero la primera que muchas personas encuentran es entender a qué nos referimos cuando hablamos de <strong>tokens</strong>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/tokenizer"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Tokenizer API</div><div class="kg-bookmark-description">Free API to tokenize texts, count and get first/last-N tokens.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-tokenize-api.png" alt="" style="cursor: help;"></div></a></figure><p>Uno de los parámetros prácticos más importantes al elegir un modelo de lenguaje de IA es el tamaño de su ventana de contexto — el tamaño máximo del texto de entrada — que se da en tokens, no en palabras o caracteres ni ninguna otra unidad automáticamente reconocible.</p><p>Además, los servicios de embeddings típicamente se calculan "por token", lo que significa que los tokens son importantes para entender tu factura.</p><p>Esto puede ser muy confuso si no tienes claro qué es un token.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="Tabla de precios actual de Jina Embeddings (a febrero de 2024)." width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Tabla de precios actual de Jina Embeddings (a febrero de 2024). Nótese que los precios se indican por "1M tokens".</span></figcaption></figure><p>Pero de todos los aspectos confusos de la IA moderna, los tokens son probablemente los menos complicados. Este artículo intentará aclarar qué es la tokenización, qué hace y por qué lo hacemos de esta manera.</p><h2 id="tldr" style="position: relative;"><a href="#tldr" title="tl;dr" id="anchor-tldr"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>tl;dr</h2><p>Para aquellos que quieren o necesitan una respuesta rápida para calcular cuántos tokens comprar de Jina Embeddings o una estimación de cuántos necesitarán comprar, las siguientes estadísticas son lo que están buscando.</p><h3 id="tokens-per-english-word" style="position: relative;"><a href="#tokens-per-english-word" title="Tokens por Palabra en Inglés" id="anchor-tokens-per-english-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens por Palabra en Inglés</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Una llamada a la API de Jina Embeddings v2 para modelos en inglés usará <b><strong style="white-space: pre-wrap;">aproximadamente</strong></b> <b><strong style="white-space: pre-wrap;">10% más</strong></b> tokens que el número de palabras en tu texto, <b><strong style="white-space: pre-wrap;">más dos tokens por embedding</strong></b>.</div></div><p>Durante pruebas empíricas, descritas más adelante en este artículo, una variedad de textos en inglés se convirtieron en tokens a una tasa de aproximadamente 10% más tokens que palabras, usando los modelos solo en inglés de Jina Embeddings. Este resultado fue bastante robusto.</p><p>Los modelos Jina Embeddings v2 tienen una ventana de contexto de 8192 tokens. Esto significa que si pasas a un modelo Jina un texto en inglés más largo que 7,400 palabras, hay una buena probabilidad de que sea truncado.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">El tamaño máximo de entrada para <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 para inglés</strong></b> es aproximadamente <b><strong style="white-space: pre-wrap;">7,400 palabras</strong></b>.</div></div><h3 id="tokens-per-chinese-character" style="position: relative;"><a href="#tokens-per-chinese-character" title="Tokens por Carácter Chino" id="anchor-tokens-per-chinese-character"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens por Carácter Chino</h3><p>Para el chino, los resultados son más variables. Dependiendo del tipo de texto, las proporciones variaron de 0.6 a 0.75 tokens por carácter chino (汉字). Los textos en inglés dados a Jina Embeddings v2 para chino producen aproximadamente el mismo número de tokens que Jina Embeddings v2 para inglés: aproximadamente 10% más que el número de palabras.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">El tamaño máximo de entrada en chino para <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 para chino e inglés</strong></b> es aproximadamente <b><strong style="white-space: pre-wrap;">10,500 caracteres</strong></b> (<b><strong style="white-space: pre-wrap;">字数</strong></b>), o <b><strong style="white-space: pre-wrap;">0.6 a 0.75 tokens por carácter chino, más dos por embedding.</strong></b></div></div><h3 id="tokens-per-german-word" style="position: relative;"><a href="#tokens-per-german-word" title="Tokens por Palabra en Alemán" id="anchor-tokens-per-german-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens por Palabra en Alemán</h3><p>Las proporciones de palabra a token en alemán son más variables que en inglés pero menos que en chino. Dependiendo del género del texto, obtuve en promedio entre 20% y 30% más tokens que palabras. Dar textos en inglés a Jina Embeddings v2 para alemán e inglés usa algunos tokens más que los modelos solo en inglés y chino/inglés: 12% a 15% más tokens que palabras.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Jina Embeddings v2 para alemán e inglés contará <b><strong style="white-space: pre-wrap;">20% a 30% más tokens que palabras, más dos por embedding</strong></b>. El tamaño máximo del contexto de entrada es aproximadamente <b><strong style="white-space: pre-wrap;">6,300 palabras en alemán</strong></b>.</div></div><h3 id="caution" style="position: relative;"><a href="#caution" title="¡Precaución!" id="anchor-caution"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>¡Precaución!</h3><p>Estos son cálculos simples, pero deberían ser aproximadamente correctos para la mayoría de los textos en lenguaje natural y la mayoría de los usuarios. En última instancia, solo podemos prometer que el número de tokens siempre será no más que el número de caracteres en tu texto, más dos. Prácticamente siempre será mucho menos que eso, pero no podemos prometer ningún conteo específico por adelantado.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">¡Los Resultados Pueden Variar! </strong></b><br><br>Estas son estimaciones basadas en cálculos estadísticamente ingenuos. No garantizamos cuántos tokens requerirá una solicitud particular.</div></div><p>Si todo lo que necesitas es consejo sobre cuántos tokens comprar para Jina Embeddings, puedes detenerte aquí. Otros modelos de embeddings, de compañías diferentes a Jina AI, pueden no tener las mismas proporciones de token a palabra y token a carácter chino que tienen los modelos Jina, pero generalmente no serán muy diferentes en general.</p><p>Si quieres entender por qué, el resto de este artículo es una inmersión más profunda en la tokenización para modelos de lenguaje.</p><h2 id="words-tokens-numbers" style="position: relative;"><a href="#words-tokens-numbers" title="Palabras, Tokens, Números" id="anchor-words-tokens-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Palabras, Tokens, Números</h2><p>La tokenización ha sido parte del procesamiento del lenguaje natural durante más tiempo que la existencia de los modelos modernos de IA.</p><p>Es un poco cliché decir que todo en una computadora es solo un número, pero también es mayormente cierto. El lenguaje, sin embargo, no es naturalmente solo un montón de números. Puede ser habla, hecha de ondas sonoras, o escritura, hecha de marcas en papel, o incluso una imagen de un texto impreso o un video de alguien usando lenguaje de señas. Pero la mayoría de las veces, cuando hablamos de usar computadoras para procesar lenguaje natural, nos referimos a textos compuestos de secuencias de caracteres: letras (a, b, c, etc.), números (0, 1, 2…), puntuación y espacios, en diferentes idiomas y codificaciones textuales.</p><p>Los ingenieros de computación los llaman "strings".</p><p>Los modelos de lenguaje de IA toman secuencias de números como entrada. Así que, podrías escribir la oración:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Pero, después de la tokenización, el modelo de IA recibe como entrada:</p><pre><code class="language-python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>La tokenización es el proceso de convertir una cadena de entrada en una secuencia específica de números que tu modelo de IA puede entender.</p><p>Cuando usas un modelo de IA a través de una API web que cobra a los usuarios por token, cada solicitud se convierte en una secuencia de números como la anterior. El número de tokens en la solicitud es la longitud de esa secuencia de números. Así, pedir a Jina Embeddings v2 para inglés que te dé un embedding para "<em>What is today's weather in Berlin?</em>" te costará 11 tokens porque convirtió esa oración en una secuencia de 11 números antes de pasarla al modelo de IA.</p><p>Los modelos de IA basados en la <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">arquitectura Transformer</a> tienen una <strong>ventana de contexto</strong> de tamaño fijo cuyo tamaño se mide en tokens. A veces esto se llama "ventana de entrada", "tamaño de contexto" o "longitud de secuencia" (especialmente en el <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard MTEB de Hugging Face</a>). Significa el tamaño máximo de texto que el modelo puede ver a la vez.</p><p>Así que, si quieres usar un modelo de embeddings, este es el tamaño máximo de entrada permitido.</p><p>Los modelos Jina Embeddings v2 tienen todos una ventana de contexto de 8,192 tokens. Otros modelos tendrán diferentes (típicamente más pequeñas) ventanas de contexto. Esto significa que sin importar cuánto texto le introduzcas, el tokenizador asociado con ese modelo Jina Embeddings debe convertirlo en no más de 8,192 tokens.</p><h2 id="mapping-language-to-numbers" style="position: relative;"><a href="#mapping-language-to-numbers" title="Mapeando Lenguaje a Números" id="anchor-mapping-language-to-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mapeando Lenguaje a Números</h2><p>La forma más simple de explicar la lógica de los tokens es esta:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Un token es un número que representa una parte de una cadena.</div></div><p>Para modelos de lenguaje natural, la parte de una cadena que un token representa es una palabra, una parte de una palabra, o una pieza de puntuación. Los espacios generalmente no reciben ninguna representación explícita en la salida del tokenizador.</p><p>La tokenización es parte de un grupo de técnicas en procesamiento de lenguaje natural llamadas <a href="https://en.wikipedia.org/wiki/Text_segmentation"><em>segmentación de texto</em></a>, y el módulo que realiza la tokenización se llama, muy lógicamente, un <strong>tokenizador</strong>.</p><p>Para mostrar cómo funciona la tokenización, vamos a tokenizar algunas oraciones usando el modelo más pequeño de Jina Embeddings v2 para inglés: <code>jina-embeddings-v2-small-en</code>. El otro modelo solo en inglés de Jina Embeddings — <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> — usa el mismo tokenizador, así que no tiene sentido descargar megabytes extra de modelo de IA que no usaremos en este artículo.</p><p>Primero, instala el módulo <code>transformers</code> en tu entorno Python o notebook. Usa elLa bandera <code>-U</code> para asegurarse de actualizar a la última versión ya que este modelo no funcionará con algunas versiones anteriores:</p><pre><code class="language-bash hljs">pip install -U transformers
</code></pre><p>Luego, descarga <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> usando <code>AutoModel.from_pretrained</code>:</p><pre><code class="language-Python hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-small-en'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
</code></pre><p>Para tokenizar una cadena, usa el método <code>encode</code> del objeto miembro <code>tokenizer</code> del modelo:</p><pre><code class="language-Python hljs">model.tokenizer.encode(<span class="hljs-string">"What is today's weather in Berlin?"</span>)
</code></pre><p>El resultado es una lista de números:</p><pre><code class="language-Python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>Para convertir estos números de vuelta a forma de cadenas, usa el método <code>convert_ids_to_tokens</code> del objeto <code>tokenizer</code>:</p><pre><code class="language-Python hljs">model.tokenizer.convert_ids_to_tokens([<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>])
</code></pre><p>El resultado es una lista de cadenas:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'what'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">"'"</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'in'</span>,
 <span class="hljs-string">'berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Ten en cuenta que el tokenizador del modelo ha:</p><ol><li>Agregado <code>[CLS]</code> al inicio y <code>[SEP]</code> al final. Esto es necesario por razones técnicas y significa que <strong>cada solicitud de embedding costará dos tokens extra</strong>, además de los tokens que requiera el texto.</li><li>Separado la puntuación de las palabras, convirtiendo "<em>Berlin?</em>" en: <code>berlin</code> y <code>?</code>, y "<em>today's</em>" en <code>today</code>, <code>'</code>, y <code>s</code>.</li><li>Puesto todo en minúsculas. No todos los modelos hacen esto, pero puede ayudar con el entrenamiento cuando se usa inglés. Puede ser menos útil en idiomas donde la capitalización tiene un significado diferente.</li></ol><p>Diferentes algoritmos de conteo de palabras en diferentes programas pueden contar las palabras en esta oración de manera diferente. OpenOffice la cuenta como seis palabras. El algoritmo de segmentación de texto Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) cuenta siete palabras. Otro software puede llegar a otros números, dependiendo de cómo manejen la puntuación y los clíticos como "'s".</p><p>El tokenizador para este modelo produce nueve tokens para esas seis o siete palabras, más los dos tokens extra necesarios con cada solicitud.</p><p>Ahora, probemos con un nombre de lugar menos común que Berlín:</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"I live in Kinshasa."</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>El resultado:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'kin'</span>, <span class="hljs-string">'##sha'</span>, <span class="hljs-string">'##sa'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>El nombre "Kinshasa" se divide en tres tokens: <code>kin</code>, <code>##sha</code>, y <code>##sa</code>. El <code>##</code> indica que este token no es el comienzo de una palabra.</p><p>Si le damos al tokenizador algo completamente extraño, el número de tokens sobre el número de palabras aumenta aún más:</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"Klaatu barada nikto"</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)

[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'##la'</span>, <span class="hljs-string">'##at'</span>, <span class="hljs-string">'##u'</span>, <span class="hljs-string">'bar'</span>, <span class="hljs-string">'##ada'</span>, <span class="hljs-string">'nik'</span>, <span class="hljs-string">'##to'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Tres palabras se convierten en ocho tokens, más los tokens <code>[CLS]</code> y <code>[SEP]</code>.</p><p>La tokenización en alemán es similar. Con el modelo <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/" rel="noreferrer">Jina Embeddings v2 para alemán</a>, podemos tokenizar una traducción de "What is today's weather in Berlin?" de la misma manera que con el modelo en inglés.</p><pre><code class="language-Python hljs">german_model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-base-de'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
token_ids = german_model.tokenizer.encode(<span class="hljs-string">"Wie wird das Wetter heute in Berlin?"</span>)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>El resultado:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Wie'</span>, <span class="hljs-string">'wird'</span>, <span class="hljs-string">'das'</span>, <span class="hljs-string">'Wetter'</span>, <span class="hljs-string">'heute'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'Berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Este tokenizador es un poco diferente del inglés en que <code>&lt;s&gt;</code> y <code>&lt;/s&gt;</code> reemplazan a <code>[CLS]</code> y <code>[SEP]</code> pero cumplen la misma función. Además, el texto no está normalizado en cuanto a mayúsculas y minúsculas — las mayúsculas y minúsculas permanecen como están escritas — porque la capitalización es significativa en alemán de manera diferente al inglés.</p><p>(Para simplificar esta presentación, he eliminado un carácter especial que indica el comienzo de una palabra.)</p><p>Ahora, probemos con una oración más compleja <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html">de un texto periodístico</a>:</p><blockquote>Ein Großteil der milliardenschweren Bauern-Subventionen bleibt liegen – zu genervt sind die Landwirte von bürokratischen Gängelungen und Regelwahn.</blockquote><pre><code class="hljs language-python">sentence = <span class="hljs-string">"""
Ein Großteil der milliardenschweren Bauern-Subventionen
bleibt liegen – zu genervt sind die Landwirte von 
bürokratischen Gängelungen und Regelwahn.
"""</span>
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)</code></pre><p>El resultado tokenizado:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Ein'</span>, <span class="hljs-string">'Großteil'</span>, <span class="hljs-string">'der'</span>, <span class="hljs-string">'mill'</span>, <span class="hljs-string">'iarden'</span>, <span class="hljs-string">'schwer'</span>, 
 <span class="hljs-string">'en'</span>, <span class="hljs-string">'Bauern'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'Sub'</span>, <span class="hljs-string">'ventionen'</span>, <span class="hljs-string">'bleibt'</span>, <span class="hljs-string">'liegen'</span>, 
 <span class="hljs-string">'–'</span>, <span class="hljs-string">'zu'</span>, <span class="hljs-string">'gen'</span>, <span class="hljs-string">'ervt'</span>, <span class="hljs-string">'sind'</span>, <span class="hljs-string">'die'</span>, <span class="hljs-string">'Landwirte'</span>, <span class="hljs-string">'von'</span>, 
 <span class="hljs-string">'büro'</span>, <span class="hljs-string">'krat'</span>, <span class="hljs-string">'ischen'</span>, <span class="hljs-string">'Gän'</span>, <span class="hljs-string">'gel'</span>, <span class="hljs-string">'ungen'</span>, <span class="hljs-string">'und'</span>, <span class="hljs-string">'Regel'</span>, 
 <span class="hljs-string">'wahn'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Aquí, puedes ver que muchas palabras alemanas fueron divididas en piezas más pequeñas y no necesariamente siguiendo las reglas gramaticales alemanas. El resultado es que una palabra larga en alemán que contaría como una sola palabra para un contador de palabras podría ser cualquier número de tokens para el modelo de IA de Jina.</p><p>Hagamos lo mismo en chino, traduciendo "What is today's weather in Berlin?" como:</p><blockquote>柏林今天的天气怎么样？</blockquote><pre><code class="hljs language-makefile">chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(<span class="hljs-string">"柏林今天的天气怎么样？"</span>)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>El resultado tokenizado:</p><pre><code class="language-Python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'柏林'</span>, <span class="hljs-string">'今天的'</span>, <span class="hljs-string">'天气'</span>, <span class="hljs-string">'怎么样'</span>, <span class="hljs-string">'？'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>En chino, normalmente no hay separaciones de palabras en el texto escrito, pero el tokenizador de Jina Embeddings frecuentemente une múltiples caracteres chinos:</p>

<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏林</td>
<td>Bólín</td>
<td>Berlin</td>
</tr>
<tr>
<td>今天的</td>
<td>jīntiān de</td>
<td>today's</td>
</tr>
<tr>
<td>天气</td>
<td>tiānqì</td>
<td>weather</td>
</tr>
<tr>
<td>怎么样</td>
<td>zěnmeyàng</td>
<td>how</td>
</tr>
</tbody>
</table>

<p>Usemos una oración más compleja <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7">de un periódico de Hong Kong</a>:</p><pre><code class="language-Python hljs">sentence = <span class="hljs-string">"""
新規定執行首日，記者在下班高峰前的下午5時來到廣州地鐵3號線，
從繁忙的珠江新城站啟程，向機場北方向出發。
"""</span>
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>(Traducción: <em>"El primer día que las nuevas regulaciones entraron en vigor, este reportero llegó a la Línea 3 del Metro de Guangzhou a las 5 p.m., durante la hora pico, habiendo partido de la Estación Zhujiang New Town en dirección norte hacia el aeropuerto."</em>)</p><p>El resultado:</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'新'</span>, <span class="hljs-string">'規定'</span>, <span class="hljs-string">'執行'</span>, <span class="hljs-string">'首'</span>, <span class="hljs-string">'日'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'記者'</span>, <span class="hljs-string">'在下'</span>, <span class="hljs-string">'班'</span>, 
 <span class="hljs-string">'高峰'</span>, <span class="hljs-string">'前的'</span>, <span class="hljs-string">'下午'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'時'</span>, <span class="hljs-string">'來到'</span>, <span class="hljs-string">'廣州'</span>, <span class="hljs-string">'地'</span>, <span class="hljs-string">'鐵'</span>, <span class="hljs-string">'3'</span>, 
 <span class="hljs-string">'號'</span>, <span class="hljs-string">'線'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'從'</span>, <span class="hljs-string">'繁忙'</span>, <span class="hljs-string">'的'</span>, <span class="hljs-string">'珠江'</span>, <span class="hljs-string">'新城'</span>, <span class="hljs-string">'站'</span>, <span class="hljs-string">'啟'</span>, 
 <span class="hljs-string">'程'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'向'</span>, <span class="hljs-string">'機場'</span>, <span class="hljs-string">'北'</span>, <span class="hljs-string">'方向'</span>, <span class="hljs-string">'出發'</span>, <span class="hljs-string">'。'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Estos tokens no se corresponden con ningún diccionario específico de palabras chinas (词典). Por ejemplo, "啟程" - <em>qǐchéng</em> (partir, emprender) normalmente se categorizaría como una sola palabra pero aquí está dividida en sus dos caracteres constituyentes. De manera similar, "在下班" usualmente sería reconocido como dos palabras, con la división entre "在" - <em>zài</em> (en, durante) y "下班" - <em>xiàbān</em> (final de la jornada laboral, hora pico), no entre "在下" y "班" como lo ha hecho el tokenizador aquí.</p><p>En los tres idiomas, los lugares donde el tokenizador divide el texto no están directamente relacionados con los lugares lógicos donde un lector humano los dividiría.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">El algoritmo tokenizador no usa un diccionario convencional consciente del lenguaje, por lo que su comportamiento no coincide con la forma en que los humanos cuentan las palabras.</div></div><p>Esto no es una característica específica de los modelos Jina Embeddings. Este enfoque de tokenización es casi universal en el desarrollo de modelos de IA. Aunque dos modelos de IA diferentes pueden no tener tokenizadores idénticos, en el estado actual de desarrollo, prácticamente todos usarán tokenizadores con este tipo de comportamiento.</p><p>La siguiente sección discutirá el algoritmo específico usado en la tokenización y la lógica detrás de él.</p><h2 id="why-do-we-tokenize-and-why-this-way" style="position: relative;"><a href="#why-do-we-tokenize-and-why-this-way" title="¿Por qué Tokenizamos? ¿Y Por qué de Esta Manera?" id="anchor-why-do-we-tokenize-and-why-this-way"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>¿Por qué Tokenizamos? ¿Y Por qué de Esta Manera?</h2><p>Los modelos de lenguaje de IA toman como entrada secuencias de números que representan secuencias de texto, pero ocurren más cosas antes de ejecutar la red neuronal subyacente y crear un embedding. Cuando se presenta una lista de números que representan pequeñas secuencias de texto, el modelo busca cada número en un diccionario interno que almacena un vector único para cada número. Luego los combina, y eso se convierte en la entrada de la red neuronal.</p><p>Esto significa que el tokenizador <strong>debe</strong> poder convertir <strong><em>cualquier</em></strong> texto de entrada que le demos en tokens que aparezcan en el diccionario de vectores de tokens del modelo. Si tomáramos nuestros tokens de un diccionario convencional, la primera vez que encontráramos un error ortográfico o un nombre propio raro o una palabra extranjera, todo el modelo se detendría. No podría procesar esa entrada.</p><p>En el procesamiento del lenguaje natural, esto se llama el problema del vocabulario fuera de vocabulario (OOV), y está presente en todos los tipos de texto y todos los idiomas. Hay algunas estrategias para abordar el problema OOV:</p><ol><li>Ignorarlo. Reemplazar todo lo que no está en el diccionario con un token "desconocido".</li><li>Evitarlo. En lugar de usar un diccionario que mapee secuencias de texto a vectores, usar uno que mapee <em>caracteres individuales</em> a vectores. El inglés solo usa 26 letras la mayoría del tiempo, por lo que esto debe ser más pequeño y más robusto contra problemas OOV que cualquier diccionario.</li><li>Encontrar subsecuencias frecuentes en el texto, ponerlas en el diccionario y usar caracteres (tokens de una sola letra) para lo que quede.</li></ol><p>La primera estrategia significa que se pierde mucha información importante. El modelo ni siquiera puede aprender sobre los datos que ha visto si toman la forma de algo que no está en el diccionario. Muchas cosas en el texto ordinario simplemente no están presentes ni siquiera en los diccionarios más grandes.</p><p>La segunda estrategia es posible, y los investigadores la han estudiado. Sin embargo, significa que el modelo tiene que aceptar muchas más entradas y tiene que aprender mucho más. Esto significa un modelo mucho más grande y muchos más datos de entrenamiento para un resultado que nunca ha demostrado ser mejor que la tercera estrategia.</p><p>Los modelos de lenguaje de IA prácticamente todos implementan la tercera estrategia de alguna forma. La mayoría usa alguna variante del <a href="https://huggingface.co/learn/nlp-course/chapter6/6">algoritmo Wordpiece</a> <a href="https://ieeexplore.ieee.org/document/6289079">[Schuster y Nakajima 2012]</a> o una técnica similar llamada <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Codificación por Pares de Bytes</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/">Senrich et al. 2016</a>] Estos algoritmos son <em>agnósticos al lenguaje</em>. Eso significa que funcionan igual para todos los lenguajes escritos sin ningún conocimiento más allá de una lista exhaustiva de posibles caracteres. Fueron diseñados para modelos multilingües como BERT de Google que toman cualquier entrada del raspado de Internet — cientos de idiomas y textos que no son lenguaje humano como programas de computadora — para que pudieran ser entrenados sin hacer lingüística complicada.</p><p>Algunas investigaciones muestran mejoras significativas usando tokenizadores más específicos y conscientes del lenguaje. [<a href="https://aclanthology.org/2021.acl-long.243/">Rust et al. 2021</a>] Pero construir tokenizadores de esa manera requiere tiempo, dinero y experiencia. Implementar una estrategia universal como BPE o Wordpiece es mucho más barato y fácil.</p><p>Sin embargo, como consecuencia, no hay manera de saber cuántos tokens representa un texto específico más que ejecutarlo a través de un tokenizador y luego contar el número de tokens que salen de él. Debido a que la subsecuencia más pequeña posible de un texto es una letra, puedes estar seguro de que el número de tokens no será mayor que el número de caracteres (menos espacios) más dos.</p><p>Para obtener una buena estimación, necesitamos enviar mucho texto a nuestro tokenizador y calcular empíricamente cuántos tokens obtenemos en promedio, en comparación con cuántas palabras o caracteres ingresamos. En la siguiente sección, haremos algunas mediciones empíricas no muy sistemáticas para todos los modelos Jina Embeddings v2 actualmente disponibles.</p><h2 id="empirical-estimates-of-token-output-sizes" style="position: relative;"><a href="#empirical-estimates-of-token-output-sizes" title="Estimaciones Empíricas de los Tamaños de Salida de Tokens" id="anchor-empirical-estimates-of-token-output-sizes"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Estimaciones Empíricas de los Tamaños de Salida de Tokens</h2><p>Para inglés y alemán, usé el algoritmo de segmentación de texto Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) para obtener el conteo de palabras de los textos. Este algoritmo es ampliamente utilizado para seleccionar fragmentos de texto cuando haces doble clic en algo. Es lo más cercano disponible a un contador de palabras objetivo universal.</p><p>Instalé la <a href="https://pypi.org/project/polyglot/">biblioteca polyglot</a> en Python, que implementa este segmentador de texto:</p><pre><code class="language-bash hljs">pip install -U polyglot
</code></pre><p>Para obtener el conteo de palabras de un texto, puedes usar código como este fragmento:</p><pre><code class="language-python hljs"><span class="hljs-keyword">from</span> polyglot.text <span class="hljs-keyword">import</span> Text

txt = <span class="hljs-string">"What is today's weather in Berlin?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(Text(txt).words))
</code></pre><p>El resultado debería ser <code>7</code>.</p><p>Para obtener un conteo de tokens, se pasaron segmentos del texto a los tokenizadores de varios modelos Jina Embeddings, como se describe a continuación, y cada vez, resté dos del número de tokens devueltos.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text">Los conteos de tokens listados aquí <b><strong style="white-space: pre-wrap;">no incluyen</strong></b> los dos tokens extra al principio y al final de cada texto tokenizado.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" style="position: relative;"><a href="#english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" title="Inglés
(jina-embeddings-v2-small-en y jina-embeddings-v2-base-en)" id="anchor-english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Inglés<br>(<code>jina-embeddings-v2-small-en</code> y <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>)</h3><p>Para calcular promedios, descargué dos corpus de texto en inglés de <a href="https://wortschatz.uni-leipzig.de/en" rel="noreferrer">Wortschatz Leipzig</a>, una colección de corpus descargables gratuitamente en varios idiomas y configuraciones alojada por la Universidad de Leipzig:</p><ul><li>Un corpus de un millón de oraciones de datos de noticias en inglés de 2020 (<code>eng_news_2020_1M</code>)</li><li>Un corpus de un millón de oraciones de datos de <a href="https://en.wikipedia.org/">Wikipedia en inglés</a> de 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Ambos se pueden encontrar en <a href="https://wortschatz.uni-leipzig.de/en/download/English">su página de descargas en inglés</a>.</p><p>Para diversidad, también descargué la <a href="https://www.gutenberg.org/ebooks/135">traducción de Hapgood de <em>Los Miserables</em> de Victor Hugo</a> del Proyecto Gutenberg, y una copia de la Versión King James de la Biblia, traducida al inglés en 1611.</p><p>Para los cuatro textos, conté las palabras usando el segmentador Unicode implementado en <code>polyglot</code>, luego conté los tokens generados por <code>jina-embeddings-v2-small-en</code>, restando dos tokens por cada solicitud de tokenización. Los resultados son los siguientes:</p>

<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Texto</th><th id="|<X;" class="simple-table-header-color simple-table-header">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Conteo de tokens<br>(Jina Embeddings v2 <br>para inglés)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Proporción de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class=""><code>eng_news_2020_1M</code></td><td id="|<X;" class="">22.825.712</td><td id="GHal" class="">25.270.581</td><td id="h]mu" class="">1,107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class=""><code>eng_wikipedia_2016_1M</code></td><td id="|<X;" class="">24.243.607</td><td id="GHal" class="">26.813.877</td><td id="h]mu" class="">1,106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class=""><code>les_miserables_en</code></td><td id="|<X;" class="">688.911</td><td id="GHal" class="">764.121</td><td id="h]mu" class="">1,109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class=""><code>kjv_bible</code></td><td id="|<X;" class="">1.007.651</td><td id="GHal" class="">1.099.335</td><td id="h]mu" class="">1,091</td></tr></tbody></table>

<p>El uso de números precisos no significa que este sea un resultado exacto. El hecho de que documentos de géneros tan diferentes tengan entre 9% y 11% más tokens que palabras indica que probablemente puedes esperar alrededor de 10% más tokens que palabras, según medido por el segmentador Unicode. Los procesadores de texto a menudo no cuentan la puntuación, mientras que el segmentador Unicode sí lo hace, por lo que no puedes esperar que los conteos de palabras del software de oficina coincidan necesariamente con esto.</p><h3 id="german-jina-embeddings-v2-base-de" style="position: relative;"><a href="#german-jina-embeddings-v2-base-de" title="Alemán
(jina-embeddings-v2-base-de)" id="anchor-german-jina-embeddings-v2-base-de"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Alemán<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>Para el alemán, descargué tres corpus de la <a href="https://wortschatz.uni-leipzig.de/en/download/German">página alemana de Wortschatz Leipzig</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> — Un millón de oraciones de una mezcla equilibrada de textos en diferentes géneros, de 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> — Un millón de oraciones de texto periodístico de 2019.</li><li><code>deu_wikipedia_2021_1M</code> — Un millón de oraciones extraídas de la Wikipedia alemana en 2021.</li></ul><p>Y para diversidad, también descargué los <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata">tres volúmenes de <em>El Capital</em> de Karl Marx</a> del <a href="https://www.deutschestextarchiv.de/" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>Luego seguí el mismo procedimiento que para el inglés:</p>

<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Texto</th><th id="|qln" class="simple-table-header-color simple-table-header">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Conteo de tokens<br>(Jina Embeddings v2 <br>para alemán e inglés)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class="">7.924.024</td><td id="YXZX" class="">9.772.652</td><td id="oEoQ" class="">1,234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class="">17.949.120</td><td id="YXZX" class="">21.711.555</td><td id="oEoQ" class="">1,210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class="">17.999.482</td><td id="YXZX" class="">22.654.901</td><td id="oEoQ" class="">1,259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class="" style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class="">784.336</td><td id="YXZX" class="">1.011.377</td><td id="oEoQ" class="">1,289</td></tr></tbody></table>

<p>Estos resultados tienen una dispersión mayor que el modelo solo en inglés, pero aun así sugieren que el texto en alemán producirá, en promedio, entre 20% y 30% más tokens que palabras.</p><p>Los textos en inglés producen más tokens con el tokenizador alemán-inglés que con el de solo inglés:</p>

<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc<l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Texto</th><th id="R@A^" class="simple-table-header-color simple-table-header">Conteo de palabras<br>(Segmentador Unicode)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Conteo de tokens<br>(Jina Embeddings v2 <br>para alemán e inglés)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio de tokens a palabras<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class="">24243607</td><td id="UUfl" class="">27758535</td><td id="iTZS" class="">1,145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class="">22825712</td><td id="UUfl" class="">25566921</td><td id="iTZS" class="">1,120</td></tr></tbody></table>

<p>Deberías esperar necesitar entre 12% y 15% más tokens que palabras para embeber textos en inglés con el modelo bilingüe alemán/inglés que con el de solo inglés.</p><h3 id="chinese-jina-embeddings-v2-base-zh" style="position: relative;"><a href="#chinese-jina-embeddings-v2-base-zh" title="Chino
(jina-embeddings-v2-base-zh)" id="anchor-chinese-jina-embeddings-v2-base-zh"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Chino<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>El chino típicamente se escribe sin espacios y no tenía una noción tradicional de "palabras" antes del siglo XX. En consecuencia, el tamaño de un texto en chino se mide típicamente en caracteres (<strong>字数</strong>). Así que, en lugar de usar el segmentador Unicode, medí la longitud de los textos en chino eliminando todos los espacios y luego simplemente obteniendo la longitud de caracteres.</p><p>Descargué tres corpus de la <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese">página de corpus chino de Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> — Un millón de oraciones de la Wikipedia en chino, extraídas en 2018.</li><li><code>zho_news_2007-2009_1M</code> — Un millón de oraciones de fuentes periodísticas chinas, recopiladas entre 2007 y 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> — Un millón de oraciones de fuentes periodísticas que usan exclusivamente caracteres chinos tradicionales (繁體字).</li></ul><p>Además, para mayor diversidad, también usé <em>La verdadera historia de A Q</em> (阿Q正傳), una novela corta de Lu Xun (魯迅) escrita a principios de la década de 1920. Descargué la <a href="https://www.gutenberg.org/ebooks/25332">versión en caracteres tradicionales de Project Gutenberg</a>.</p>

<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Texto</th><th id="CaUc" class="simple-table-header-color simple-table-header">Conteo de caracteres<br>(字数)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Conteo de tokens<br>(Jina Embeddings v2 <br>para chino e inglés)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio de tokens a caracteres<br>(a 3 decimales)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class="">45.116.182</td><td id="CQ{d" class="">29.193.028</td><td id="_};C" class="">0,647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class="">44.295.314</td><td id="CQ{d" class="">28.108.090</td><td id="_};C" class="">0,635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class="" style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class="">54,585,819</td><td id="CQ{d" class="">40,290,982</td><td id="_};C" class="">0.738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class="" style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class="">41,268</td><td id="CQ{d" class="">25,346</td><td id="_};C" class="">0.614</td></tr></tbody></table>

<p>Esta dispersión en las proporciones de tokens a caracteres es inesperada, y especialmente el valor atípico para el corpus de caracteres tradicionales merece más investigación. No obstante, podemos concluir que para el chino, debes esperar necesitar <em>menos</em> tokens que caracteres hay en tu texto. Dependiendo de tu contenido, puedes esperar necesitar entre un 25% y un 40% menos.</p><p>Los textos en inglés en Jina Embeddings v2 para chino e inglés produjeron aproximadamente el mismo número de tokens que en el modelo solo para inglés:</p>

<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="<rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class="" style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class="" style="width:165.3125px">24,243,607</td><td id="=]?F" class="">26,890,176</td><td id="<rlw" class="">1.109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class="" style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class="" style="width:165.3125px">22,825,712</td><td id="=]?F" class="">25,060,352</td><td id="<rlw" class="">1.097</td></tr></tbody></table>

<h2 id="taking-tokens-seriously" style="position: relative;"><a href="#taking-tokens-seriously" title="Tomando los Tokens en Serio" id="anchor-taking-tokens-seriously"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tomando los Tokens en Serio</h2><p>Los tokens son un andamiaje importante para los modelos de lenguaje de IA, y la investigación en esta área continúa.</p><p>Uno de los aspectos donde los modelos de IA han demostrado ser revolucionarios es el descubrimiento de que son muy robustos frente a datos ruidosos. Incluso si un modelo particular no utiliza la estrategia de tokenización óptima, si la red es lo suficientemente grande, tiene suficientes datos y está adecuadamente entrenada, puede aprender a hacer lo correcto a partir de una entrada imperfecta.</p><p>En consecuencia, se dedica mucho menos esfuerzo a mejorar la tokenización que en otras áreas, pero esto podría cambiar.</p><p>Como usuario de embeddings, que los compras a través de una <a href="https://jina.ai/embeddings/">API como Jina Embeddings</a>, no puedes saber exactamente cuántos tokens necesitarás para una tarea específica y es posible que tengas que hacer algunas pruebas propias para obtener números sólidos. Pero las estimaciones proporcionadas aquí — aproximadamente 110% del recuento de palabras para inglés, aproximadamente 125% del recuento de palabras para alemán, y aproximadamente 70% del recuento de caracteres para chino — deberían ser suficientes para un presupuesto básico.</p></section></article><div data-v-692f7f2b="" class="row justify-between items-center q-py-md"><div data-v-692f7f2b=""><span data-v-692f7f2b="" class="text-weight-bold">Categorías:</span><span data-v-692f7f2b="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></span></div><div data-v-692f7f2b=""><div data-v-692f7f2b="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-692f7f2b="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-692f7f2b="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-692f7f2b="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-692f7f2b="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-692f7f2b="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-692f7f2b="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-692f7f2b="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-692f7f2b="" class="text-h5 q-my-xl">Leer más</div><a data-v-aa7e154f="" data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/meta-prompt-for-better-jina-api-integration-and-codegen"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">noviembre 19, 2024 • 9 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Meta-Prompt for Better Jina API Integration and CodeGen</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Is Meta-Prompt the new norm for API specs? Feed it to LLMs and generate integration code that reliably integrates Jina's APIs, saving you from the usual trial-and-error process.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--58-.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/beyond-clip-how-jina-clip-advances-multimodal-search"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">octubre 29, 2024 • 11 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Beyond CLIP: How Jina-CLIP Advances Multimodal Search</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Bo Wang"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Bo Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-692f7f2b="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/finding-optimal-breakpoints-in-long-documents-using-small-language-models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">octubre 25, 2024 • 19 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Finding Optimal Breakpoints in Long Documents Using Small Language Models</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">We trained three small language models to better segment long documents into chunks, and here are the key lessons we learned.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Andrei Ungureanu"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Andrei Ungureanu" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/07/Me.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="A pattern of yellow file icons on a blue background with one icon displaying a smiley face creating an emotive contrast." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/10/breakpoints.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div class="col-sm-12 col-md"><div class="q-list q-list--dark small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Oficinas</div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlín, Alemania (sede central)</div><div class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlín, Alemania</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Beijing, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">Nivel 5, Edificio 6, No.48 Haidian West St. Beijing Haidian, China</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">402, Piso 4, Edificio de Tecnología Fu'an, Shenzhen Nanshan, China</div></div></div></div></div><div class="col-sm-12 col-md row"><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fundación de búsqueda</div><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Incrustaciones</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">reclasificador</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Lector</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Clasificador</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Segmentador</div></a><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Obtenga la clave API de Jina AI</div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Límite de velocidad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-pa-none"><svg class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div class="q-item__section column q-item__section--main justify-center">Estado de la API</div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Compañía</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sala de prensa</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Términos</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/COMMERCIAL-LICENSE-TERMS.pdf" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Licencia comercial</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#security"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Seguridad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Privacidad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Administrar cookies</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div class="row items-center justify-between q-gutter-x-sm col-12 col-md"><label class="q-field row no-wrap items-start q-field--outlined q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dense q-field--dark text-caption" for="f_53e27acb-3923-4350-90bc-5b9040aadb68"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__prepend q-field__marginal row no-wrap items-center"><i class="q-icon text-white notranslate material-symbols material-symbols-sharp q-px-sm q-py-none" aria-hidden="true" role="presentation" style="font-size: 18px;">language</i></div><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_53e27acb-3923-4350-90bc-5b9040aadb68" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_53e27acb-3923-4350-90bc-5b9040aadb68_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">arrow_drop_down</i></div></div></div></label><div class="text-caption text-dim"> Jina AI GmbH © 2020-2024. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
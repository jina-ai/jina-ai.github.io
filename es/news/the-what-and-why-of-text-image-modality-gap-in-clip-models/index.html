<!DOCTYPE html><html translate="no" dir="ltr" lang="es"><head><title>El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP</title><meta charset="utf-8"><meta name="title" content="El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP"><meta name="description" content="No puedes simplemente usar un modelo CLIP para recuperar texto e imágenes y ordenar los resultados por puntuación. ¿Por qué? Por la brecha de modalidad. ¿Qué es y de dónde proviene?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models"><meta property="og:title" content="El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP"><meta property="og:description" content="No puedes simplemente usar un modelo CLIP para recuperar texto e imágenes y ordenar los resultados por puntuación. ¿Por qué? Por la brecha de modalidad. ¿Qué es y de dónde proviene?"><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models"><meta property="twitter:title" content="El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP"><meta property="twitter:description" content="No puedes simplemente usar un modelo CLIP para recuperar texto e imágenes y ordenar los resultados por puntuación. ¿Por qué? Por la brecha de modalidad. ¿Qué es y de dónde proviene?"><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-GCseaXgZ.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-CRvJtbiE.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-D_hJ0-gA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-BQgHXloZ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-8Pd5hKpC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-w6ctZ6fx.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-BLULP_uj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-HUcM_iPT.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/es-DJJjFCSD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BaAPYHko.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpace-CVTlsp1I.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-CyjV0oje.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QToolbar-BWPx1Nrt.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-DNp0WFzD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnGroup-D6rllugv.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-CKg9r6UJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-CaAVi1ec.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-CwaxqVj2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-CUHC-eu6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-CB2WH-fQ.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-BfYflfOA.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/PurchaseSuccessDialog-CKRmuIAK.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-CZ79QnDv.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-CbvNFWa6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/format-DyQxkAtJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/embedding--R7n9Hor.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-CXCbS_fR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs--Fz0-G31.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-CiYSpMfT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-Bm-IYLIa.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-DsB-SXl4.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-BMR_OewU.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-BnlPUdtD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-Eis6_QEl.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-DUy_U8Ht.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-BhXMWPs2.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-DgR6_B3P.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-BYg3bc8w.css"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-BxGzrPBq.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><meta name="author" content="Bo Wang, Scott Martens"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Bo Wang, Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="13 mins read"><meta property="article:published_time" content="2024-08-26T15:56:36.000+02:00"><meta property="article:modified_time" content="2024-08-27T20:10:53.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP",
  "description": "No puedes simplemente usar un modelo CLIP para recuperar texto e imágenes y ordenar los resultados por puntuación. ¿Por qué? Por la brecha de modalidad. ¿Qué es y de dónde proviene?",
  "image": [
    "https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg"
  ],
  "datePublished": "2024-08-26T15:56:36.000+02:00",
  "dateModified": "2024-08-27T20:10:53.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Bo Wang",
      "url": "https://jina-ai-gmbh.ghost.io/author/bo/"
    },
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><link prerender-ignore rel=preconnect href=//api.usercentrics.eu><link prerender-ignore rel=preconnect href=//privacy-proxy.usercentrics.eu><link prerender-ignore rel=preload href=//app.usercentrics.eu/browser-ui/latest/loader.js as=script><link prerender-ignore rel=preload href=//privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js as=script><script prerender-ignore id=usercentrics-cmp data-settings-id=w5v6v2pJsC3wdR src=https://app.usercentrics.eu/browser-ui/latest/loader.js async></script><script prerender-ignore src=https://privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div class="q-space"></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">notifications</i></div><div class="q-item__section column q-item__section--main justify-center">Noticias</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_eb850d7e-43a8-4daf-a9f6-2c619b0b9c46" aria-label="Expandir &quot;Productos&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">box</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Productos</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_eb850d7e-43a8-4daf-a9f6-2c619b0b9c46" style="display: none;"><div class="q-list q-list--dark" role="list" label="Productos"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Para Empresas</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incrustaciones</div><div class="q-item__label q-item__label--caption text-caption">Integraciones multilingües y multimodales de clase mundial.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">reclasificador</div><div class="q-item__label q-item__label--caption text-caption">Recuperador neuronal de clase mundial para maximizar la relevancia de la búsqueda.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lector</div><div class="q-item__label q-item__label--caption text-caption">Lea las URL y busque en la web para obtener una base más sólida para su LLM.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Clasificador</div><div class="q-item__label q-item__label--caption text-caption">Clasificación de cero disparos y pocos disparos para imágenes y texto.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentador</div><div class="q-item__label q-item__label--caption text-caption">Corta el texto largo en fragmentos y haz tokenización.</div></div></a><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Para usuarios avanzados</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://promptperfect.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://promptperfect.jina.ai/PromptPerfect-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">PromptPerfect</div><div class="q-item__label q-item__label--caption text-caption">Herramienta principal para ingeniería rápida</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_d2484d76-b545-42b5-ba5a-2b2015f7fa8b" aria-label="Expandir &quot;Más herramientas para usuarios avanzados&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Más herramientas para usuarios avanzados</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_d2484d76-b545-42b5-ba5a-2b2015f7fa8b" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://scenex.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://scenex.jina.ai/SceneX - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">SceneXplain</div><div class="q-item__label q-item__label--caption text-caption">Solución líder de IA para subtítulos de imágenes y resúmenes de vídeos</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://bestbanner.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://bestbanner.jina.ai/bestbanner-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">BestBanner</div><div class="q-item__label q-item__label--caption text-caption">¡Blog a banner, sin las indicaciones!</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://chat.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://chat.jina.ai/JinaChat - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JinaChat</div><div class="q-item__label q-item__label--caption text-caption">Más modalidad, más memoria, menos costo</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://rationale.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://rationale.jina.ai/Rationale-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Rationale</div><div class="q-item__label q-item__label--caption text-caption">Las mejores herramientas de toma de decisiones de IA</div></div></a></div></div></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_668ac788-e84b-46c9-a8df-f08161659361" aria-label="Expandir &quot;Compañía&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon" aria-hidden="true" role="presentation"><img src="/J.svg"></i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Compañía</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_668ac788-e84b-46c9-a8df-f08161659361" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Acceso"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div><div class="q-item__section column q-item__section--main justify-center">Acceso</div></a></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 56px;"><main data-v-225745e2="" class="q-page" style="min-height: 100vh;"><div data-v-225745e2="" class="row full-width relative-position justify-end"><div data-v-225745e2="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-225745e2="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-225745e2="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-225745e2="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-225745e2="" class="q-item__label">¿De dónde viene la brecha de modalidad?</div></div></div><div data-v-225745e2="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-225745e2="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-225745e2="" class="q-item__label">El Medio es el Mensaje</div></div></div></div></div><div data-v-225745e2="" class="col-12 col-md-10 col-lg-12"><div data-v-225745e2="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></div><div data-v-225745e2="" class="row justify-center"><div data-v-225745e2="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-225745e2="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">agosto 26, 2024</div><h1 data-v-225745e2="" class="text-weight-medium text-center q-px-md my-title">El Qué y Por qué de la Brecha de Modalidad Texto-Imagen en los Modelos CLIP</h1><div data-v-225745e2="" class="col row justify-center"><div data-v-225745e2="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">No puedes simplemente usar un modelo CLIP para recuperar texto e imágenes y ordenar los resultados por puntuación. ¿Por qué? Por la brecha de modalidad. ¿Qué es y de dónde proviene?</div></div><div data-v-225745e2="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-225745e2="" class="q-img q-img--menu" role="img" aria-label="Futuristic black image with &quot;modality gap&quot; in 3D purple letters, additional text, and a dynamic glass sphere effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Futuristic black image with &quot;modality gap&quot; in 3D purple letters, additional text, and a dynamic glass sphere effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/modality-gap-banner.jpg" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-225745e2="" class="row justify-center"><div data-v-225745e2="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-225745e2="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-225745e2="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Bo Wang"><div style="padding-bottom: 93.2813%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Bo Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-225745e2="" class="q-item__label">Bo Wang, Scott Martens • 13 minutos de lectura</div></div></div></div><div data-v-225745e2="" class="row justify-center"><div data-v-225745e2="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-225745e2="" class="article"><section data-v-225745e2="" class="gh-content"><p>Los <a href="https://jina.ai/news/embeddings-the-swiss-army-knife-of-ai">embeddings semánticos</a> son el núcleo de los modelos modernos de IA, incluso de los chatbots y modelos de arte con IA. A veces están ocultos para los usuarios, pero siguen ahí, acechando justo bajo la superficie.</p><p>La teoría de los embeddings tiene solo dos partes:</p><ol><li>Las cosas — elementos fuera del modelo de IA, como textos e imágenes — están representadas por vectores creados por modelos de IA a partir de datos sobre esas cosas.</li><li>Las relaciones entre las cosas fuera del modelo de IA están representadas por relaciones espaciales entre esos vectores. Entrenamos modelos de IA específicamente para crear vectores que funcionen de esa manera.</li></ol><p>Cuando creamos un modelo multimodal de imagen-texto, entrenamos el modelo para que los embeddings de las imágenes y los embeddings de los textos que describen o se relacionan con esas imágenes estén relativamente cerca entre sí. Las similitudes semánticas entre las cosas que esos dos vectores representan — una imagen y un texto — se reflejan en la relación espacial entre los dos vectores.</p><p>Por ejemplo, podríamos esperar razonablemente que los vectores de embedding para una imagen de una naranja y el texto "una naranja fresca" estén más cerca entre sí que la misma imagen y el texto "una manzana fresca".</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png" class="kg-image" alt="Illustration on a black background showing an orange and an apple with arrows between them and quotes reading &quot;A fresh orange" width="1000" height="500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare_2.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare_2.png 1000w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Ese es el propósito de un modelo de embedding: generar representaciones donde las características que nos importan — como qué tipo de fruta se muestra en una imagen o se nombra en un texto — se preserven en la distancia entre ellas.</p><p>Pero la multimodalidad introduce algo más. Podríamos encontrar que una imagen de una naranja está más cerca de una imagen de una manzana que del texto "una naranja fresca", y que el texto "una manzana fresca" está más cerca de otro texto que de una imagen de una manzana.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png" class="kg-image" alt="Black background featuring an apple on the left and an orange on the right with annotated arrows marked &quot;A fresh apple.&quot; and " width="1000" height="500" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/apple-orange-compare.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/apple-orange-compare.png 1000w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Resulta que esto es exactamente lo que sucede con los modelos multimodales, incluido el modelo <a href="https://jina.ai/news/jina-clip-v1-a-truly-multimodal-embeddings-model-for-text-and-image">Jina CLIP</a> (<code>jina-clip-v1</code>) de Jina AI.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2405.20204"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina CLIP: Your CLIP Model Is Also Your Text Retriever</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval systems that keep separate embeddings and models for text-only and multimodal tasks. We propose a novel, multi-task contrastive training method to address this issue, which we use to train the jina-clip-v1 model to achieve the state-of-the-art performance on both text-image and text-text retrieval tasks.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="" style="cursor: help;"></div></a></figure><p>Para probarlo, tomamos una muestra de 1,000 pares de texto-imagen del <a href="https://www.kaggle.com/datasets/adityajn105/flickr8k">conjunto de prueba Flickr8k</a>. Cada par contiene cinco textos de leyenda (así que técnicamente no es un par) y una sola imagen, con los cinco textos describiendo la misma imagen.</p><p>Por ejemplo, la siguiente imagen (<code>1245022983_fb329886dd.jpg</code> en el conjunto de datos Flickr8k):</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/1245022983_fb329886dd.jpg" class="kg-image" alt="A young girl in a pink skirt playing with a frisbee in an urban outdoor setting with cars and bikes present." width="334" height="500" style="cursor: help;"></figure><p>Sus cinco leyendas:</p><pre><code class="language-Text hljs">A child in all pink is posing nearby a stroller with buildings in the distance.
A little girl in pink dances with her hands on her hips.
A small girl wearing pink dances on the sidewalk.
The girl in a bright pink skirt dances near a stroller.
The little girl in pink has her hands on her hips.
</code></pre><p>Usamos Jina CLIP para incrustar las imágenes y textos y luego:</p><ol><li>Comparamos las similitudes de coseno de los embeddings de imágenes con los embeddings de sus textos de leyenda.</li><li>Tomamos los embeddings de los cinco textos de leyenda que describen la misma imagen y comparamos sus similitudes de coseno entre sí.</li></ol><p>El resultado es una brecha sorprendentemente grande, visible en la Figura 1:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png" class="kg-image" alt="Graph with two curves showing the distribution of Cosine Similarity for Image2Text and Text2Text pairs with labeled axes." width="1870" height="1130" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/jinaclip-cosine-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/jinaclip-cosine-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/jinaclip-cosine-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/jinaclip-cosine-alt--1-.png 1870w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 1: Distribución de valores de similitud de coseno entre pares coincidentes de imagen-texto y texto-texto en Jina CLIP.</span></figcaption></figure><p>Con pocas excepciones, los pares de texto coincidentes están mucho más cerca entre sí que los pares de imagen-texto coincidentes. Esto indica fuertemente que Jina CLIP está codificando textos en una parte del espacio de embedding e imágenes en una parte mayormente disjunta relativamente alejada de ella. Este espacio entre los textos y las imágenes es la <em>brecha multimodal</em>.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/2clusersGraph.png" class="kg-image" alt="Diagram on black background depicting 'Images' on left, 'Texts' on bottom, with labeled 'Multimodal Gap' in the center." width="493" height="479" style="cursor: help;"></figure><p>Los modelos de embedding multimodales están codificando más que la información semántica que nos importa: están codificando el medio de su entrada. Según Jina CLIP, una imagen no vale, como dice el dicho, mil palabras. Tiene un contenido que ninguna cantidad de palabras puede igualar realmente. Codifica el medio de entrada en la semántica de sus embeddings sin que nadie lo haya entrenado para hacerlo.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Mientras solo comparemos imágenes con textos y viceversa, esto no es un problema, pero un modelo verdaderamente multimodal debería poder decirnos que, por ejemplo, el texto "esto es una manzana" coincide mejor con una imagen de una manzana que con un texto sobre naranjas. Los modelos estilo CLIP en su forma actual no pueden hacer eso.</div></div><p>Este fenómeno ha sido investigado en el artículo <em>Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</em> [<a href="https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html">Liang et al., 2022</a>] que se refiere a él como la "brecha de modalidad". La brecha de modalidad es la separación espacial, en el espacio de embedding, entre entradas en un medio y entradas en otro. Aunque los modelos no están intencionalmente entrenados para tener tal brecha, son omnipresentes en los modelos multimodales.</p><p>Nuestras investigaciones sobre la brecha de modalidad en Jina CLIP se basan fuertemente en <a href="https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html">Liang et al. [2022]</a>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://papers.neurips.cc/favicon.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">NeurIPS Proceedings</span></div></div></a></figure><h2 id="where-does-the-modality-gap-come-from" style="position: relative;"><a href="#where-does-the-modality-gap-come-from" title="¿De dónde viene la brecha de modalidad?" id="anchor-where-does-the-modality-gap-come-from"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>¿De dónde viene la brecha de modalidad?</h2><p>Liang et al. [2022] identifican tres fuentes principales detrás de la brecha de modalidad:</p><ul><li>Un sesgo de inicialización que llaman el "efecto cono".</li><li>Reducciones en la temperatura (aleatoriedad) durante el entrenamiento que hacen muy difícil "desaprender" este sesgo.</li><li>Procedimientos de aprendizaje contrastivo, que son ampliamente utilizados en modelos multimodales, que involuntariamente refuerzan la brecha.</li></ul><p>Examinaremos cada uno por turno.</p><h3 id="cone-effect" style="position: relative;"><a href="#cone-effect" title="Efecto Cono" id="anchor-cone-effect"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Efecto Cono</h3><p>Un modelo construido con una arquitectura CLIP o tipo CLIP es en realidad dos modelos de embedding separados conectados entre sí. Para modelos multimodales de imagen-texto, esto significa un modelo para codificar textos y otro completamente separado para codificar imágenes, como se muestra en el esquema a continuación.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png" class="kg-image" alt="Diagram illustrating concepts of natural language processing with &quot;Embedding Space&quot;, &quot;Image Encoder&quot;, &quot;Text Encoder&quot;, and &quot;Di" width="1025" height="750" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--21-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--21-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--21-.png 1025w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Estos dos modelos son entrenados de manera que un embedding de imagen y un embedding de texto estén relativamente cerca cuando el texto describe bien la imagen.</p><p>Puedes entrenar un modelo como este aleatorizando los pesos en ambos modelos, luego presentando pares de imagen y texto juntos, entrenándolo desde cero para minimizar la distancia entre las dos salidas. El <a href="https://arxiv.org/abs/2103.00020">modelo CLIP original de OpenAI</a> fue entrenado de esta manera. Sin embargo, esto requiere muchos pares de imagen-texto y un entrenamiento computacionalmente costoso. Para el primer modelo CLIP, OpenAI extrajo 400 millones de pares de imagen-texto de materiales con subtítulos en Internet.</p><p>Los modelos más recientes tipo CLIP utilizan componentes pre-entrenados<a href="https://doi.org/10.1109/CVPR52688.2022.01759">.</a> Esto significa entrenar cada componente por separado como un buen modelo de embedding de modo único, uno para textos y otro para imágenes. Estos dos modelos luego se entrenan juntos usando pares de imagen-texto, un proceso llamado <em>ajuste contrastivo</em>. Los pares alineados de imagen-texto se utilizan para "empujar" lentamente los pesos para hacer que los embeddings de texto e imagen coincidentes estén más cerca entre sí, y los no coincidentes más separados.</p><p>Este enfoque generalmente requiere menos datos de pares imagen-texto, que son difíciles y costosos de obtener, y grandes cantidades de textos e imágenes sin subtítulos, que son mucho más fáciles de obtener. Jina CLIP (<code>jina-clip-v1</code>) fue entrenado usando este último método. Pre-entrenamos un modelo <a href="https://jina.ai/news/jina-embeddings-2-the-best-solution-for-embedding-long-documents/">JinaBERT v2</a> para codificación de texto usando datos de texto generales y utilizamos un <a href="https://github.com/baaivision/EVA/tree/master/EVA-02">codificador de imágenes EVA-02</a> pre-entrenado, luego los entrenamos más usando una variedad de técnicas de entrenamiento contrastivo, como se describe en <a href="https://arxiv.org/abs/2405.20204">Koukounas et al. [2024]</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png" class="kg-image" alt="UMAP scatter plot of jinaCLIP embeddings with text and image data points, labeled axes, and category distinctions." width="2000" height="1333" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-inherit_alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-inherit_alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-inherit_alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-inherit_alt--1-.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 2: Ubicaciones iniciales de los embeddings de imagen y texto antes del entrenamiento por pares en Jina CLIP, proyectadas en dos dimensiones.</span></figcaption></figure><p>Si tomamos estos dos modelos pre-entrenados y observamos su salida, antes de entrenarlos con pares de imagen-texto, notamos algo importante. La Figura 2 (arriba) es una <a href="https://umap-learn.readthedocs.io/en/latest/">proyección UMAP</a> en dos dimensiones de los embeddings de imagen producidos por el codificador EVA-02 pre-entrenado y los embeddings de texto producidos por JinaBERT v2 pre-entrenado, con las líneas grises indicando pares imagen-texto coincidentes. Esto es antes de cualquier entrenamiento cross-modal.</p><p>El resultado es una especie de "cono" truncado, con embeddings de imagen en un extremo y embeddings de texto en el otro. Esta forma de cono se traduce pobremente a proyecciones bidimensionales, pero se puede ver ampliamente en la imagen de arriba. Todos los textos se agrupan en una parte del espacio de embedding, y todas las imágenes en otra parte. Si, después del entrenamiento, los textos siguen siendo más similares a otros textos que a las imágenes coincidentes, este estado inicial es una gran razón por la que sucede. El objetivo de mejor coincidencia de imágenes con textos, textos con textos e imágenes con imágenes, es completamente compatible con esta forma de cono.</p><p>El modelo tiene prejuicios de nacimiento y lo que aprende no cambia eso. La Figura 3 (abajo) es el mismo análisis del modelo Jina CLIP tal como se lanzó, después del entrenamiento completo usando pares de imagen-texto. Si acaso, la brecha multimodal es aún más pronunciada.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png" class="kg-image" alt="UMAP projection chart of JinaCLIP trained weights with two distinct clusters for 'text' and 'image' embeddings." width="2000" height="1333" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-jinaclip-trained-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-jinaclip-trained-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-jinaclip-trained-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-jinaclip-trained-alt--1-.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 3: Ubicaciones de los embeddings de imagen y texto después del entrenamiento por pares en Jina CLIP, proyectadas en dos dimensiones.</span></figcaption></figure><p>Incluso después de un entrenamiento extensivo, Jina CLIP aún codifica el medio como parte del mensaje.</p><p>Usar el enfoque más costoso de OpenAI, con inicialización puramente aleatoria, no elimina este sesgo. Tomamos la arquitectura original de OpenAI CLIP y aleatorizamos completamente todos los pesos, luego hicimos el mismo análisis que arriba. El resultado sigue siendo una forma de cono truncado, como se ve en la Figura 4:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png" class="kg-image" alt="Scientific graph displaying UMAP projections of OpenAI CLIP data with blue and green dots indicating text and image embedding" width="2000" height="1333" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/umap-openai-random-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/umap-openai-random-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/umap-openai-random-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/umap-openai-random-alt--1-.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 4: Ubicaciones iniciales de los embeddings de imagen y texto en Jina CLIP con pesos completamente aleatorios y sin entrenamiento, proyectadas en dos dimensiones.</span></figcaption></figure><p>Este sesgo es un problema estructural y puede no tener solución. Si es así, solo podemos buscar formas de corregirlo o mitigarlo durante el entrenamiento.</p><h3 id="training-temperature" style="position: relative;"><a href="#training-temperature" title="Temperatura de Entrenamiento" id="anchor-training-temperature"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Temperatura de Entrenamiento</h3><p>Durante el entrenamiento de modelos de IA, típicamente agregamos algo de aleatoriedad al proceso. Calculamos cuánto debería cambiar un lote de muestras de entrenamiento los pesos en el modelo, luego agregamos un pequeño factor aleatorio a esos cambios antes de realmente cambiar los pesos. Llamamos a la cantidad de aleatoriedad la <em>temperatura</em>, por analogía con la forma en que usamos la aleatoriedad en termodinámica.</p><p>Las temperaturas altas crean cambios grandes en los modelos muy rápido, mientras que las temperaturas bajas reducen la cantidad que un modelo puede cambiar cada vez que ve algunos datos de entrenamiento. El resultado es que con temperaturas altas, podemos esperar que los embeddings individuales se muevan mucho en el espacio de embedding durante el entrenamiento, y con temperaturas bajas, se moverán mucho más lentamente.</p><p>La mejor práctica para entrenar modelos de IA es comenzar con una temperatura alta y luego reducirla progresivamente. Esto ayuda al modelo a hacer grandes saltos en el aprendizaje al principio cuando los pesos son aleatorios o están lejos de donde necesitan estar y luego le permite aprender los detalles de manera más estable.</p><p>El entrenamiento de pares imagen-texto de Jina CLIP comienza con una temperatura de 0.07 (esta es una temperatura relativamente alta) y la reduce exponencialmente durante el curso del entrenamiento a 0.01, como se muestra en la Figura 5 a continuación, un gráfico de temperatura vs. pasos de entrenamiento:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png" class="kg-image" alt="Line chart titled &quot;Learned temperature value w.r.t. steps&quot; with &quot;Steps&quot; on x-axis and &quot;Temperature&quot; on y-axis, demonstrating " width="1000" height="600" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/temperature-jina-clip-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/temperature-jina-clip-alt--1-.png 1000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 5: Disminución de temperatura durante el entrenamiento por pares en Jina CLIP.</span></figcaption></figure><p>Queríamos saber si aumentar la temperatura — añadiendo aleatoriedad — reduciría el efecto cono y acercaría los embeddings de imagen y texto en general. Así que reentrenamos Jina CLIP con una temperatura fija de 0.1 (un valor muy alto). Después de cada época de entrenamiento, verificamos la distribución de distancias entre pares de imagen-texto y pares de texto-texto, al igual que en la Figura 1. Los resultados se muestran a continuación en la Figura 6:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png" class="kg-image" alt="Six heatmaps showing cosine similarity distributions with varied color palettes, labeled by epochs and datasets." width="1999" height="1999" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/closing-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/closing-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/closing-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/closing-the-gap-alt--1-.png 1999w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 6: La brecha entre modalidades se reduce con el tiempo cuando la temperatura de entrenamiento es alta.</span></figcaption></figure><p>Como puede verse, mantener una temperatura alta reduce dramáticamente la brecha multimodal. Permitir que los embeddings se muevan mucho durante el entrenamiento ayuda significativamente a superar el sesgo inicial en la distribución de embeddings.</p><p>Sin embargo, esto tiene un costo. También probamos el rendimiento del modelo utilizando seis pruebas de recuperación diferentes: Tres pruebas de recuperación texto-texto y tres de texto-imagen, de los conjuntos de datos <a href="https://huggingface.co/datasets/HuggingFaceM4/COCO">MS-COCO</a>, <a href="https://www.kaggle.com/datasets/adityajn105/flickr8k">Flickr8k</a> y <a href="https://www.kaggle.com/datasets/adityajn105/flickr30k">Flickr30k</a>. En todas las pruebas, vemos que el rendimiento cae al principio del entrenamiento y luego sube muy lentamente, como puede verse en la Figura 7:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png" class="kg-image" alt="Set of six line graphs on a dark background, displaying data comparisons with labeled axes and varying conditions." width="2000" height="735" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/performance-close-the-gap-alt--1-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/performance-close-the-gap-alt--1-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/performance-close-the-gap-alt--1-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/performance-close-the-gap-alt--1-.png 2000w" sizes="(min-width: 1200px) 1200px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Figura 7: Rendimiento durante el entrenamiento. Al principio, hay una fuerte caída desde el estado inicial y luego una subida muy lenta.</span></figcaption></figure><p>Probablemente sería extremadamente largo y costoso entrenar un modelo como Jina CLIP usando esta temperatura alta constante. Aunque teóricamente es factible, no es una solución práctica.</p><h3 id="contrastive-learning-and-the-false-negative-problem" style="position: relative;"><a href="#contrastive-learning-and-the-false-negative-problem" title="Aprendizaje Contrastivo y el Problema de los Falsos Negativos" id="anchor-contrastive-learning-and-the-false-negative-problem"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Aprendizaje Contrastivo y el Problema de los Falsos Negativos</h3><p><a href="https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html">Liang et al. [2022]</a> también descubrieron que las prácticas estándar de aprendizaje contrastivo — el mecanismo que usamos para entrenar modelos multimodales tipo CLIP — tienden a reforzar la brecha multimodal.</p><p>El aprendizaje contrastivo es fundamentalmente un concepto simple. Tenemos un embedding de imagen y un embedding de texto y sabemos que deberían estar más cerca entre sí, así que ajustamos los pesos en el modelo durante el entrenamiento para lograr esto. Vamos despacio, ajustando los pesos en pequeñas cantidades, y los ajustamos en proporción a qué tan separados están los dos embeddings: Más cerca significa un cambio más pequeño que más lejos.</p><p>Esta técnica funciona mucho mejor si no solo acercamos los embeddings cuando coinciden, sino que también los alejamos cuando no coinciden. Queremos tener no solo pares de imagen-texto que pertenezcan juntos, sino también pares que sabemos que deben estar separados.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png" class="kg-image" alt="Black background with an illustration of a red apple and an orange, associated with arrows and quotes " a="" fresh="" apple"="" and="" "a="" "="" width="1020" height="600" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--2-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--2-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--2-.png 1020w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Esto plantea algunos problemas:</p><ol><li>Nuestras fuentes de datos consisten enteramente en pares coincidentes. Nadie haría una base de datos de textos e imágenes que un humano haya verificado como no relacionados, ni tampoco se podría construir fácilmente una mediante web scraping u otra técnica no supervisada o semi-supervisada.</li><li>Incluso los pares de imagen-texto que superficialmente parecen completamente disjuntos no necesariamente lo son. No tenemos una teoría de la semántica que nos permita hacer objetivamente tales juicios negativos. Por ejemplo, una imagen de un gato acostado en un porche no es una coincidencia completamente negativa para el texto "un hombre durmiendo en un sofá". Ambos involucran estar acostado sobre algo.</li></ol><p>Idealmente, querríamos entrenar con pares de imagen-texto que supiéramos con certeza que están relacionados <em>y no relacionados</em>, pero no hay una manera obvia de obtener pares conocidos no relacionados. Es posible preguntarle a la gente "¿Esta frase describe esta imagen?" y esperar respuestas consistentes. Es mucho más difícil obtener respuestas consistentes preguntando "¿Esta frase no tiene nada que ver con esta imagen?"</p><p>En su lugar, obtenemos pares de imagen-texto no relacionados seleccionando aleatoriamente imágenes y textos de nuestros datos de entrenamiento, esperando que prácticamente siempre sean malas coincidencias. En la práctica, esto funciona dividiendo nuestros datos de entrenamiento en lotes. Para entrenar Jina CLIP, usamos lotes que contenían 32,000 pares coincidentes de imagen-texto, pero para este experimento, los tamaños de lote fueron solo de 16.</p><p>La tabla siguiente muestra 16 pares de imagen-texto muestreados aleatoriamente de Flickr8k:</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png" class="kg-image" alt="Collage of various scenes including people, dogs engaging in activities like catching frisbees, and a boy skateboarding, with" width="1827" height="1245" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/08/image--3-.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/08/image--3-.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/08/image--3-.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--3-.png 1827w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Para obtener pares no coincidentes, combinamos cada imagen en el lote con cada texto <em>excepto con el que coincide</em>. Por ejemplo, el siguiente par es una imagen y texto que no coinciden:</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4-.png" class="kg-image" alt="Friendly brown dog playing in a shallow creek, shaking off water surrounded by natural greenery." width="500" height="500" style="cursor: help;"></figure><p><strong>Descripción:</strong> Una niña de rosa recoge flores.</p><p>Pero este procedimiento asume que todos los textos que coinciden con otras imágenes son igualmente malas coincidencias. Esto no siempre es cierto. Por ejemplo:</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/image--4--1.png" class="kg-image" alt="Brown or gray dog standing in water amidst tall grass, suggesting outdoor play or relaxation." width="500" height="500" style="cursor: help;"></figure><p><strong>Descripción:</strong> El perro se sienta junto a un montón de nieve.</p><p>Aunque el texto no describe esta imagen, tienen un perro en común. Tratar este par como no coincidente tenderá a alejar la palabra "perro" de cualquier imagen de un perro.</p><p><a href="https://papers.neurips.cc/paper_files/paper/2022/hash/702f4db7543a7432431df588d57bc7c9-Abstract-Conference.html">Liang et al. [2022]</a> muestran que estos pares no coincidentes imperfectos empujan todas las imágenes y textos a alejarse entre sí.</p><p>Nos propusimos verificar su afirmación con un modelo de imagen <code>vit-b-32</code> completamente inicializado aleatoriamente y un modelo de texto JinaBERT v2 similarmente aleatorizado, con la temperatura de entrenamiento establecida en una constante de 0.02 (una temperatura moderadamente baja). Construimos dos conjuntos de datos de entrenamiento:</p><ul><li>Uno con lotes aleatorios extraídos de Flickr8k, con pares no coincidentes construidos como se describió anteriormente.</li><li>Otro donde los lotes se construyen intencionalmente con múltiples copias de la misma imagen con diferentes textos en cada lote. Esto garantiza que un número significativo de pares "no coincidentes" son en realidad coincidencias correctas entre sí.</li></ul><p>Luego entrenamos dos modelos durante una época, uno con cada conjunto de datos de entrenamiento, y medimos la distancia coseno promedio entre 1,000 pares de texto-imagen en el conjunto de datos Flickr8k para cada modelo. El modelo entrenado con lotes aleatorios tuvo una distancia coseno promedio de 0.7521, mientras que el entrenado con muchos pares "no coincidentes" intencionalmente coincidentes tuvo una distancia coseno promedio de 0.7840. El efecto de los pares "no coincidentes" incorrectos es bastante significativo. Dado que el entrenamiento real del modelo es mucho más largo y usa muchos más datos, podemos ver cómo este efecto crecería y aumentaría la brecha entre imágenes y textos en su conjunto.</p><h2 id="the-medium-is-the-message" style="position: relative;"><a href="#the-medium-is-the-message" title="El Medio es el Mensaje" id="anchor-the-medium-is-the-message"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>El Medio es el Mensaje</h2><p>El teórico canadiense de las comunicaciones <a href="https://en.wikipedia.org/wiki/The_medium_is_the_message" rel="noopener noreferrer">Marshall McLuhan</a> acuñó la frase "El medio es el mensaje" en su libro de 1964 <a href="https://en.wikipedia.org/wiki/Understanding_Media" rel="noopener noreferrer"><em>Understanding Media: The Extensions of Man</em></a> para enfatizar que los mensajes no son autónomos. Nos llegan en un contexto que afecta fuertemente su significado, y él afirmó famosamente que una de las partes más importantes de ese contexto es la naturaleza del medio de comunicación.</p><p>La brecha de multimodalidad nos ofrece una oportunidad única para estudiar una clase de fenómenos semánticos emergentes en modelos de IA. Nadie le dijo a Jina CLIP que codificara el medio de los datos con los que fue entrenado — simplemente lo hizo de todos modos. Incluso si no hemos resuelto el problema para modelos multimodales, al menos tenemos una buena comprensión teórica de dónde proviene el problema.</p><p>Debemos asumir que nuestros modelos están codificando otras cosas que aún no hemos buscado debido al mismo tipo de sesgo. Por ejemplo, probablemente tenemos el mismo problema en modelos de embedding multilingües. El entrenamiento conjunto en dos o más idiomas probablemente conduce a la misma brecha entre idiomas, especialmente porque se utilizan ampliamente métodos de entrenamiento similares. Las soluciones al problema de la brecha pueden tener implicaciones muy amplias.</p><p>Una investigación sobre el sesgo de inicialización en una gama más amplia de modelos probablemente también conducirá a nuevos hallazgos. Si el medio es el mensaje para un modelo de embedding, ¿quién sabe qué más se está codificando en nuestros modelos sin que nos demos cuenta?</p></section></article><div data-v-225745e2="" class="row justify-between items-center q-py-md"><div data-v-225745e2=""><span data-v-225745e2="" class="text-weight-bold">Categorías:</span><span data-v-225745e2="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></span></div><div data-v-225745e2=""><div data-v-225745e2="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-225745e2="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fthe-what-and-why-of-text-image-modality-gap-in-clip-models%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-225745e2="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fthe-what-and-why-of-text-image-modality-gap-in-clip-models%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-225745e2="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fthe-what-and-why-of-text-image-modality-gap-in-clip-models%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-225745e2="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fthe-what-and-why-of-text-image-modality-gap-in-clip-models%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-225745e2="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fthe-what-and-why-of-text-image-modality-gap-in-clip-models%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-225745e2="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-225745e2="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-225745e2="" class="text-h5 q-my-xl">Leer más</div><a data-v-aa7e154f="" data-v-225745e2="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/still-need-chunking-when-long-context-models-can-do-it-all"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">diciembre 04, 2024 • 13 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Still Need Chunking When Long-Context Models Can Do It All?</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Comparing how long-context embedding models perform with different chunking strategies to find the optimal approach for your needs.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Michael Günther"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Michael Günther" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/profile_low_quality.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Artistic pixel art of two seagulls on colored pipes with speech bubbles; one reads &quot;Too long?&quot; and the other shows math equat"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Artistic pixel art of two seagulls on colored pipes with speech bubbles; one reads &quot;Too long?&quot; and the other shows math equat" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/long-context.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-225745e2="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/watermarking-text-with-embedding-models-to-protect-against-content-theft"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">noviembre 27, 2024 • 10 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Watermarking Text with Embedding Models to Protect Against Content Theft</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">You use our embedding models to do what? This might be the most "out-of-domain" applications of embeddings we learned at EMNLP 2024.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Two hands, each holding a key positioned to interact with each other, depicted against a deep blue background."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Two hands, each holding a key positioned to interact with each other, depicted against a deep blue background." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/banner--1-.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-225745e2="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/meta-prompt-for-better-jina-api-integration-and-codegen"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">noviembre 19, 2024 • 9 minutos de lectura</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Meta-Prompt for Better Jina API Integration and CodeGen</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Is Meta-Prompt the new norm for API specs? Feed it to LLMs and generate integration code that reliably integrates Jina's APIs, saving you from the usual trial-and-error process.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--58-.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div class="col-sm-12 col-md"><div class="q-list q-list--dark small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Oficinas</div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Sunnyvale, California</div><div class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, EE. UU.</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlín, Alemania (sede central)</div><div class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlín, Alemania</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Beijing, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">Piso 5, Edificio 6, No.48 Haidian West St. Pekín, China</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">Piso 402, Edificio de Tecnología Fu'an, Shenzhen, China</div></div></div></div></div><div class="col-sm-12 col-md row"><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fundación de búsqueda</div><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Incrustaciones</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">reclasificador</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Lector</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Clasificador</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Segmentador</div></a><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Obtenga la clave API de Jina AI</div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Límite de velocidad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-pa-none"><svg class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div class="q-item__section column q-item__section--main justify-center">Estado de la API</div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Compañía</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sala de prensa</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Términos</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/COMMERCIAL-LICENSE-TERMS.pdf" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Licencia comercial</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#security"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Seguridad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Privacidad</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Administrar cookies</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div class="row items-center justify-between q-gutter-x-sm col-12 col-md"><label class="q-field row no-wrap items-start q-field--outlined q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dense q-field--dark text-caption" for="f_2da25f7b-43cb-40be-9de6-2bd1fbf1aa76"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__prepend q-field__marginal row no-wrap items-center"><i class="q-icon text-white notranslate material-symbols material-symbols-sharp q-px-sm q-py-none" aria-hidden="true" role="presentation" style="font-size: 18px;">language</i></div><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_2da25f7b-43cb-40be-9de6-2bd1fbf1aa76" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_2da25f7b-43cb-40be-9de6-2bd1fbf1aa76_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">arrow_drop_down</i></div></div></div></label><div class="text-caption text-dim"> Jina AI © 2020-2024. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
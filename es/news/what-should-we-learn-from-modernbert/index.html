<!DOCTYPE html><html translate="no" dir="ltr" lang="es"><head><title>¿Qué deberíamos aprender de ModernBERT?</title><meta charset="utf-8"><meta name="title" content="¿Qué deberíamos aprender de ModernBERT?"><meta name="description" content="Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="og:title" content="¿Qué deberíamos aprender de ModernBERT?"><meta property="og:description" content="Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT."><meta property="og:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="twitter:title" content="¿Qué deberíamos aprender de ModernBERT?"><meta property="twitter:description" content="Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-CiEytmCC.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-Y1UNyCXI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-BsKN1HLB.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-DaoAOftR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-BEHP6A0H.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-CGX2y-nM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-M1pgyc3K.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/es-DJJjFCSD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BmZAuy21.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-DgmvbNyQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-DDeh-vn3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-nsxDuNnQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-C0rzMmjO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-8IkEyAIH.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-Diil_jLy.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-COe5ntuC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-D9GSqoFd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-eiyZAMgQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-HvSqLg1S.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-CWXlk5QY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-BWTzj2RV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-ZbetpOBh.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-cwlyY9j5.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-BDo0X9w9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DD_2uL_p.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-bh2PwKCC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-9NOczSkU.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-Si_JH0tJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-XUMM_ex3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-CzG4G6es.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DO9vo9XY.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-C4aG1vPL.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-C9gB2tY4.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-Bnw1nozX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-BBZh4Jag.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-CsArUnAW.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-DCqvwrUt.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-vSZoRHSY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Nan Wang, Alex C-G"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Nan Wang, Alex C-G"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="10 mins read"><meta property="article:published_time" content="2025-01-22T08:31:26.000+01:00"><meta property="article:modified_time" content="2025-01-22T08:31:26.000+01:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "¿Qué deberíamos aprender de ModernBERT?",
  "description": "Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT.",
  "image": [
    "https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"
  ],
  "datePublished": "2025-01-22T08:31:26.000+01:00",
  "dateModified": "2025-01-22T08:31:26.000+01:00",
  "author": [
    {
      "@type": "Person",
      "name": "Nan Wang",
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "@type": "Person",
      "name": "Alex C-G",
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-85e17eff="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-85e17eff="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-85e17eff="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-85e17eff="" class="q-space"></div><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-85e17eff="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-85e17eff="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-85e17eff="" class="q-list q-list--dark" role="list"><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Noticias</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Modelos</div></a><div data-v-85e17eff="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_56feed44-5cea-40d9-9ad4-af2e261f7ca7" aria-label="Expandir &quot;Productos&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Productos</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_56feed44-5cea-40d9-9ad4-af2e261f7ca7" style="display: none;"><div data-v-85e17eff="" class="q-list q-list--dark" role="list" label="Productos"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Búsqueda profunda</div><div class="q-item__label q-item__label--caption text-caption">Busca, lee y razona hasta encontrar la mejor respuesta.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lector</div><div class="q-item__label q-item__label--caption text-caption">Lea las URL y busque en la web para obtener una base más sólida para su LLM.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incrustaciones</div><div class="q-item__label q-item__label--caption text-caption">Integraciones multilingües y multimodales de clase mundial.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">reclasificador</div><div class="q-item__label q-item__label--caption text-caption">Recuperador neuronal de clase mundial para maximizar la relevancia de la búsqueda.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_4a8be54b-47a0-46c1-aa05-9f92c42bf6d7" aria-label="Expandir &quot;Más&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Más</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_4a8be54b-47a0-46c1-aa05-9f92c42bf6d7" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Clasificador</div><div class="q-item__label q-item__label--caption text-caption">Clasificación de cero disparos y pocos disparos para imágenes y texto.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentador</div><div class="q-item__label q-item__label--caption text-caption">Corta el texto largo en fragmentos y haz tokenización.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Documentación de la API</div><div class="q-item__label q-item__label--caption text-caption">Generación automática de código para su IDE o LLM de Copilot</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-85e17eff="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_0741dc2a-68d5-4326-ba60-31acf02f3620" aria-label="Expandir &quot;Compañía&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Compañía</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_0741dc2a-68d5-4326-ba60-31acf02f3620" style="display: none;"><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Acceso"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Acceso</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-85e17eff="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_19fa04eb-6373-43d2-bb94-3575493f0f95"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_19fa04eb-6373-43d2-bb94-3575493f0f95" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_19fa04eb-6373-43d2-bb94-3575493f0f95_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-85e17eff="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Eficiencia de Parámetros de ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Modelado de Código de ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Manejo de contexto largo de ModernBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">¿La lección amarga?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Conclusión</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">enero 22, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">¿Qué deberíamos aprender de ModernBERT?</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Más datos de entrenamiento, un dimensionamiento eficiente de parámetros y una arquitectura profunda pero delgada: ModernBERT marca una dirección para los futuros modelos tipo BERT.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert.png" style="object-fit: contain; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Nan Wang"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Nan Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Nan Wang, Alex C-G • 10 minutos de lectura</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>Allá por 2018, Google lanzó BERT y fue un cambio revolucionario para NLP, mucho antes de la actual ola de LLM. Incluso ahora, muchos Modelos de Lenguaje Pequeños están construidos sobre BERT. En diciembre de 2024, <a href="https://huggingface.co/blog/modernbert" rel="noreferrer">ModernBERT</a> toma lo que hemos aprendido de los recientes desarrollos de LLM y lo aplica a estos modelos más pequeños. ¿Las claves? Mejor eficiencia de parámetros, comprensión de código y manejo de contextos largos.</p><p>En esta publicación, analizaremos cómo se compara ModernBERT con dos modelos que conocemos a fondo: <code>jina-XLM-RoBERTa</code> (la columna vertebral multilingüe detrás de <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>) y <code>RobERTa-large</code>. Veamos cada modelo:</p><ul><li><strong>ModernBERT</strong> (dic. 2024) es un SLM recientemente lanzado, desarrollado en colaboración por Answer.AI, LightOn y HuggingFace. Aprovecha optimizaciones modernas como RoPE para una ventana de contexto de 8,192 tokens y capas <a href="https://arxiv.org/abs/2002.05202">GeGLU</a>, mejorando el rendimiento mientras mantiene la eficiencia.</li><li><a href="https://huggingface.co/jinaai/xlm-roberta-flash-implementation"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong> </strong>(sept. 2024) es un modelo de embeddings de texto multilingüe basado en <a href="https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta"><code>XLM-RoBERTa</code></a> de Meta. Mientras que el <code>XLM-RoBERTa</code> original mejora <code>RoBERTa</code> usando el conjunto de datos multilingüe XLM, <code>jina-XLM-RoBERTa</code> va más allá con entrenamiento de contexto extendido, implementación de <a href="https://arxiv.org/abs/2104.09864">RoPE</a> y soporte para <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a>. Este modelo sirve como base para <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>.</li><li><a href="https://huggingface.co/FacebookAI/roberta-large"><strong><code>RoBERTa-large</code></strong></a> (julio 2019) desarrollado por Meta, es una versión mejorada de BERT con 355 millones de parámetros. A través de entrenamiento extendido, conjuntos de datos más grandes e innovaciones como el enmascaramiento dinámico, ha logrado resultados impresionantes en benchmarks clave incluyendo <a href="https://gluebenchmark.com/">GLUE</a>, <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a> y <a href="https://arxiv.org/abs/1704.04683">RACE</a>. Esto lo hace adecuado para varias tareas de NLP, desde clasificación de texto hasta respuesta de preguntas.</li></ul><p>Al comparar estos modelos en tres aspectos centrales, buscamos destacar las efectivas decisiones de diseño de ModernBERT para otros desarrolladores de modelos e identificar insights clave de desarrollo para futuros modelos tipo BERT. También compartiremos nuestros aprendizajes del desarrollo de <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a> y discutiremos mejoras planeadas para <code>jina-embeddings-v4</code> y <code>jina-reranker-v3</code>.</p><h2 id="modernberts-parameter-efficiency" style="position: relative;"><a href="#modernberts-parameter-efficiency" title="Eficiencia de Parámetros de ModernBERT" id="anchor-modernberts-parameter-efficiency"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Eficiencia de Parámetros de ModernBERT</h2><p>Primero examinemos el enfoque de ModernBERT hacia la eficiencia de parámetros - incorpora varios insights clave de desarrollos recientes de LLM. ModernBERT aprovecha tres estrategias principales: una arquitectura más profunda pero más delgada, tamaño de vocabulario controlado, y escalamiento progresivo del modelo comenzando desde modelos más pequeños.</p><h3 id="deep-and-thin-architecture" style="position: relative;"><a href="#deep-and-thin-architecture" title="Arquitectura Profunda y Delgada" id="anchor-deep-and-thin-architecture"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Arquitectura Profunda y Delgada</h3><p>ModernBERT-large se hace más profundo con 28 capas, mientras que <code>jina-XLM-RoBERTa</code> y <code>RoBERTa-large</code> funcionan con 24. Pero aquí está lo interesante - iguala a <code>RoBERTa-large</code> en cantidad de parámetros a pesar de esas capas extra. <code>jina-XLM-RoBERTa</code> necesita más parámetros ya que maneja 89 idiomas, mientras que los otros dos se enfocan solo en inglés.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg" class="kg-image" alt="" width="1389" height="547" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">La profundidad (número de capas) es más importante que el ancho (número de unidades ocultas) para LLMs pequeños. Una estructura de modelo profunda y delgada sobresale en capturar conceptos abstractos, resultando en un rendimiento final superior.</span></figcaption></figure><p>La mayoría de los parámetros de un transformer provienen de las capas de atención y completamente conectadas. ModernBERT se mantiene competitivo en tamaño haciéndose "más delgado" - ejecuta 2,624 unidades ocultas a través de 28 capas, comparado con las 4,096 unidades de RoBERTa-large a través de 24 capas. Esta configuración más "profunda" pero delgada les permite alcanzar sus objetivos de rendimiento sin inflar el modelo.</p>


<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Parámetros</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Estados ocultos</td>
<td>1,024</td>
<td>1,024</td>
<td>1,024</td>
</tr>
<tr>
<td>Dimensiones intermedias</td>
<td>2,624</td>
<td>4,096</td>
<td>4,096</td>
</tr>
<tr>
<td>Cabezas de atención</td>
<td>16</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>Capas</td>
<td>28</td>
<td>24</td>
<td>24</td>
</tr>
<tr>
<td>Tamaño de vocabulario</td>
<td>50,368</td>
<td>250,002</td>
<td>50,265</td>
</tr>
</tbody>
</table>

<p>Este enfoque se alinea con la investigación <a href="https://openreview.net/pdf?id=EIGbXbxcUQ">MobileLLM</a> de Meta, que encontró que para modelos más pequeños, la profundidad importa más que el ancho cuando se trata de capturar patrones complejos y mejorar el rendimiento. Esencialmente, la capacidad de procesar información a través de más capas transformer resulta más valiosa que tener capas más anchas para procesamiento paralelo.</p><p>Veamos los datos sobre cómo se desempeña esta arquitectura profunda y delgada.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg" class="kg-image" alt="" width="872" height="371" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Comparado con modelos similares que usan la arquitectura tradicional superficial-gruesa, ModernBERT está entregando mejores resultados en tareas clave como recuperación y STS - todo mientras mantiene un conteo similar de parámetros.</span></figcaption></figure>


<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>STS12</td>
<td>72.6</td>
<td><strong>72.7</strong></td>
<td>68.9</td>
</tr>
<tr>
<td>STS13</td>
<td><strong>84.9</strong></td>
<td>83.9</td>
<td>81.0</td>
</tr>
<tr>
<td>STS14</td>
<td>77.5</td>
<td><strong>77.7</strong></td>
<td>74.8</td>
</tr>
<tr>
<td>STS15</td>
<td>84.8</td>
<td><strong>85.8</strong></td>
<td>84.1</td>
</tr>
<tr>
<td>STS16</td>
<td>79.4</td>
<td><strong>79.6</strong></td>
<td>78.6</td>
</tr>
<tr>
<td>STS17</td>
<td><strong>87.5</strong></td>
<td>87.2</td>
<td>87.2</td>
</tr>
<tr>
<td>TRECCOVID</td>
<td><strong>61.1</strong></td>
<td>59.6</td>
<td>49.3</td>
</tr>
<tr>
<td>FiQA</td>
<td><strong>44.4</strong></td>
<td>40.0</td>
<td>40.7</td>
</tr>
<tr>
<td>NFCorpus</td>
<td><strong>32.6</strong></td>
<td>30.6</td>
<td>27.9</td>
</tr>
<tr>
<td>SciFact</td>
<td><strong>68.6</strong></td>
<td>65.5</td>
<td>63.1</td>
</tr>
<tr>
<td>Promedio</td>
<td><strong>69.3</strong></td>
<td>68.2</td>
<td>65.6</td>
</tr>
</tbody>
</table>

<p>Tome <code>jina-XLM-RoBERTa</code> - se basa en la arquitectura superficial-gruesa de <code>RoBERTa-large</code> pero aumenta el vocabulario de 50K a 250K tokens y entrena con más datos. Sin embargo, ModernBERT aún lo supera, sugiriendo que el cambio arquitectónico está marcando una diferencia real en eficiencia.</p><h3 id="vocabulary-size-matters" style="position: relative;"><a href="#vocabulary-size-matters" title="El Tamaño del Vocabulario Importa" id="anchor-vocabulary-size-matters"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>El Tamaño del Vocabulario Importa</h3><p>Primero, veamos cómo se cuentan los parámetros de vocabulario en transformers. Para cualquier transformer, <code>parámetros de vocabulario = número de tokens distintos × tamaño oculto</code>. Tome <code>jina-XLM-RoBERTa</code>: con 250K tokens y 1,024 dimensiones, necesita 256M parámetros solo para codificación de vocabulario - ¡antes de manejar cualquier tarea de lenguaje real!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg" class="kg-image" alt="" width="3757" height="715" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">En los transformers, la primera capa mapea tokens a estados ocultos usando una matriz de pesos, específicamente los pesos del vocabulario. Considerando usar todos los puntos de código UTF-8 (1.112.064) con 1.024 dimensiones ocultas - necesitarías masivos </span><code spellcheck="false" style="white-space: pre-wrap;"><span>1,112,064 × 1,024 = 1 B</span></code><span style="white-space: pre-wrap;"> parámetros solo para la conversión de tokens. Mientras que los LLM más grandes (100B+ parámetros) pueden manejar esta sobrecarga, es una restricción seria para modelos más pequeños. Es exactamente por esto que usamos tokenizadores como BPE, que fusionan eficientemente puntos de código UTF-8 comunes en tokens únicos.</span></figcaption></figure><p>Pero aquí está el asunto: <strong>los pesos del vocabulario no contribuyen a los mecanismos de atención - son solo tablas de búsqueda.</strong> Para los SLM que trabajan con presupuestos fijos de parámetros, un vocabulario más grande significa menos parámetros disponibles para las capas de atención, que realizan el procesamiento real del lenguaje. Esto explica por qué ModernBERT-large en inglés supera al multilingüe <code>jina-XLM-RoBERTa</code> a pesar de ser más pequeño - <code>jina-XLM-RoBERTa</code> asigna más parámetros (¡47%!) para soportar múltiples idiomas. El vocabulario enfocado de ModernBERT no solo mejora el rendimiento sino que también acelera la inferencia, haciéndolo particularmente efectivo para aplicaciones con recursos limitados.</p><p>Así que ahora si miramos <em>solo</em> los parámetros del modelo central (excluyendo los pesos del vocabulario), ModernBERT en realidad tiene más poder computacional que sus pares: ModernBERT dedica 19% más parámetros al modelado de lenguaje <em>real</em> que <code>jina-XLM-RoBERTa</code> y 15% más que <code>RoBERTa-large</code>!</p>

<table>
<thead>
<tr>
<th>Especificaciones del Modelo</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Soporte de Idiomas</td>
<td>Solo Inglés</td>
<td>89 Idiomas</td>
<td>Solo Inglés</td>
</tr>
<tr>
<td>Tamaño del Vocabulario</td>
<td>50.4K</td>
<td>250K</td>
<td>50.3K</td>
</tr>
<tr>
<td>Parámetros Totales</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Parámetros de Vocabulario</td>
<td>51M</td>
<td>256M</td>
<td>51M</td>
</tr>
<tr>
<td>Ratio de Parámetros de Vocabulario</td>
<td>13%</td>
<td>47%</td>
<td>14%</td>
</tr>
<tr>
<td>Parámetros del Modelo Central</td>
<td><b>349M</b></td>
<td>294M</td>
<td>304M</td>
</tr>
</tbody>
</table>

<h3 id="model-upscaling-by-weight-tiling" style="position: relative;"><a href="#model-upscaling-by-weight-tiling" title="Escalado del Modelo mediante &quot;Weight Tiling&quot;" id="anchor-model-upscaling-by-weight-tiling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Escalado del Modelo mediante "Weight Tiling"</h3><p>Al construir el backbone de <a href="https://huggingface.co/jinaai/jina-bert-implementation"><code>jina-BERT-v2</code></a>, descubrimos que entrenar SLM desde cero era intensivo en recursos y complejo. ModernBERT aborda esto con un enfoque inteligente de inicialización llamado <strong>weight tiling</strong> - esencialmente inicializando ModernBERT-large desde los pesos de su versión base más pequeña.</p><p>Esta técnica no es completamente nueva - se basa en el trabajo de DeepMind con <a href="https://gpt3demo.com/apps/deepmind-gopher">Gopher</a> y también aparece en los modelos <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2</a> de Microsoft. Pero su aplicación aquí es particularmente efectiva para abordar el cuello de botella del entrenamiento de SLM.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg" class="kg-image" alt="" width="1877" height="1308" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT escala de 22 a 28 capas usando la estrategia de inicialización de profundidad del equipo de Gopher. Para esas capas extra (23-28), inicializan cada una usando pesos de las 22 capas originales de ModernBERT-base. Para las matrices de pesos de cada capa, usan el enfoque de tiling central de Phi-2. Funciona así: toman los pesos de ModernBERT-base y los colocan justo en el medio de las matrices de ModernBERT-large. ¿Para los bordes que siguen vacíos? Envuelven los pesos originales cíclicamente para llenarlos.</span></figcaption></figure><p>Esta estrategia de inicialización le da a ModernBERT-large una ventaja significativa - en lugar de comenzar desde cero, aprovecha los patrones pre-aprendidos de su contraparte más pequeña. Ha demostrado ser particularmente <a href="https://arxiv.org/pdf/2112.11446">efectiva para escalar modelos de lenguaje en este rango de tamaño</a>.</p><blockquote>Encontramos que un modelo con inicio en caliente se recupera rápidamente de una pérdida inicial alta (debido a los parámetros añadidos) a una pérdida bastante cercana a la del modelo base. Podemos expandir 417M parámetros por más de 3× en tamaño y mantener un rendimiento superior al de un modelo equivalente entrenado desde cero hasta la convergencia, lo que implica que las ganancias no se limitaron al inicio del entrenamiento. Sin embargo, en tamaños más grandes, las ganancias relativas logradas en la convergencia disminuyen, especialmente con expansiones en anchura.</blockquote><p>El envolvimiento cíclico de pesos no es solo por conveniencia - se alinea bien con cómo las matrices de atención naturalmente exhiben patrones periódicos. La investigación de Gopher muestra que este enfoque realmente brilla para SLM (menos de 9B parámetros), aunque los beneficios comienzan a disminuir a medida que te mueves hacia territorios de modelos más grandes.</p><h2 id="modernberts-code-modeling" style="position: relative;"><a href="#modernberts-code-modeling" title="Modelado de Código de ModernBERT" id="anchor-modernberts-code-modeling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Modelado de Código de ModernBERT</h2><p>ModernBERT trae un enfoque especializado para la comprensión de código con su tokenizador optimizado para código y datos de entrenamiento. Esta optimización para el procesamiento de código rinde frutos tanto en tareas de comprensión como de recuperación.</p><p>Ejecutamos un benchmark usando el corpus <code>jina-embeddings-v2-code</code>, comparando tres modelos como backbones: <code>ModernBERT</code>, <code>jina-XLM-RoBERTa</code>, y <code>RoBERTa-large</code>. ¿La prueba? <a href="https://github.com/github/CodeSearchNet">CodeSearchNet</a> - emparejar descripciones de texto con fragmentos de código. ModernBERT superó a ambas alternativas en toda la línea.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg" class="kg-image" alt="" width="787" height="489" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">La brecha tiene sentido - ni </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> ni </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;"> vieron lenguajes de programación durante el entrenamiento. Mientras tanto, ModernBERT-large se entrenó con dos billones de tokens, incluyendo una cantidad sustancial de código. Esta exposición a la sintaxis y patrones de programación le da una clara ventaja en tareas relacionadas con código. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> supera ligeramente a </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;">, probablemente debido a sus datos de entrenamiento multilingües más grandes - misma arquitectura, más exposición. Sin embargo, ambos quedan significativamente por detrás de ModernBERT-large.</span></figcaption></figure>

<table>
<thead>
<tr>
<th>Tarea</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>AdvRetrieval</td>
<td>0.342</td>
<td><strong>0.363</strong></td>
<td>0.331</td>
</tr>
<tr>
<td>QueryRetrieval.python</td>
<td>0.521</td>
<td><strong>0.530</strong></td>
<td>0.525</td>
</tr>
<tr>
<td>QueryRetrieval java</td>
<td><strong>0.679</strong></td>
<td>0.633</td>
<td>0.644</td>
</tr>
<tr>
<td>QueryRetrieval.javascript</td>
<td>0.755</td>
<td><strong>0.768</strong></td>
<td>0.732</td>
</tr>
<tr>
<td>QueryRetrieval.php</td>
<td><strong>0.815</strong></td>
<td>0.781</td>
<td>0.755</td>
</tr>
<tr>
<td>QueryRetrieval.ruby</td>
<td>0.729</td>
<td><strong>0.744</strong></td>
<td>0.722</td>
</tr>
<tr>
<td>QueryRetrieval.go</td>
<td><strong>0.833</strong></td>
<td>0.809</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.go</td>
<td><strong>0.778</strong></td>
<td>0.750</td>
<td>0.759</td>
</tr>
<tr>
<td>Retrieval.java</td>
<td><strong>0.840</strong></td>
<td>0.792</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.javascript</td>
<td><strong>0.817</strong></td>
<td>0.792</td>
<td>0.757</td>
</tr>
<tr>
<td>Retrieval.php</td>
<td><strong>0.852</strong></td>
<td>0.805</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.python</td>
<td><strong>0.849</strong></td>
<td>0.816</td>
<td>0.787</td>
</tr>
<tr>
<td>Retrieval.ruby</td>
<td><strong>0.849</strong></td>
<td>0.796</td>
<td>0.803</td>
</tr>
<tr>
<td>Avg.</td>
<td><strong>0.743</strong></td>
<td>0.721</td>
<td>0.708</td>
</tr>
</tbody>
</table>

<h3 id="the-tokenizer-edge" style="position: relative;"><a href="#the-tokenizer-edge" title="La ventaja del tokenizador" id="anchor-the-tokenizer-edge"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>La ventaja del tokenizador</h3><p>Veamos por qué ModernBERT maneja tan bien el código - utiliza el <a href="https://huggingface.co/docs/transformers/en/model_doc/olmo" rel="noreferrer">tokenizador OLMo</a>, que fue entrenado específicamente en código, en lugar de los tokenizadores estándar de BERT/RoBERTa.</p><p>Un tokenizador divide el texto UTF-8 en tokens que se mapean a vectores - estos son los que el modelo realmente procesa. Durante el entrenamiento, aprende a combinar secuencias de caracteres que ocurren frecuentemente en tokens únicos. ¿La diferencia? Un tokenizador estándar podría dividir <code>init</code> en <code>in</code> + <code>it</code>, perdiendo el contexto de programación. Pero el tokenizador de ModernBERT, consciente del código, lo obtiene sin dividirlo.</p><p>Aquí es donde se pone interesante con el manejo de espacios: ModernBERT preserva los espacios iniciales de Python como tokens únicos y diferencia entre 4 y 8 espacios - crucial para la estructura del código. Mientras tanto, <strong><code>jina-XLM-RoBERTa</code> colapsa todos los espacios continuos en un solo <code>_</code>, y RoBERTa-large trata cada espacio como su propio token.</strong> Esto significa que el codificador de ModernBERT recibe una entrada más limpia y significativa al procesar código, mientras que los otros trabajan con tokens fracturados y menos coherentes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg" class="kg-image" alt="" width="3156" height="1247" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT preserva los espacios iniciales de Python como tokens únicos y diferencia entre 4 y 8 espacios - crucial para la estructura del código; mientras que los otros trabajan con tokens fracturados y menos coherentes.</span></figcaption></figure><h2 id="modernberts-long-context-handling" style="position: relative;"><a href="#modernberts-long-context-handling" title="Manejo de contexto largo de ModernBERT" id="anchor-modernberts-long-context-handling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Manejo de contexto largo de ModernBERT</h2><p>ModernBERT ha logrado avances significativos en el procesamiento de texto largo, gracias a su extenso corpus de entrenamiento (300B tokens con muestras de 8,192 tokens) y técnicas avanzadas como la atención combinada global y local.</p><p>Para evaluar las capacidades de manejo de documentos largos, usamos el <a href="https://huggingface.co/datasets/Shitao/MLDR">conjunto de datos MLDR</a> - un benchmark exhaustivo de texto largo que abarca 13 idiomas. Dado que ModernBERT actualmente solo admite inglés, nos centramos en el subconjunto en inglés de MLDR para comparar ModernBERT con <code>jina-XLM-RoBERTa</code>. Si bien ambos modelos pueden manejar entradas de 8K tokens, <code>RoBERTa-large</code> fue excluido de este benchmark debido a su límite de 512 tokens, que es insuficiente para el análisis de texto largo.</p>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>MLDR-en</td>
<td><strong>0.351</strong></td>
<td>0.290</td>
</tr>
</tbody>
</table>

<p>El rendimiento superior de ModernBERT no se debe solo a su extenso entrenamiento en texto largo - se debe en gran parte a su innovadora combinación de mecanismos de atención global y local. A diferencia de <code>jina-XLM-RoBERTa</code>, que aplica una atención global computacionalmente costosa a cada capa, ModernBERT adopta un enfoque más eficiente. Alterna entre atención global (usada cada tercera capa con un <code>theta</code> de 160,000) y atención local (usando una ventana deslizante de 128 tokens con un <code>theta</code> de 100,000). Esta estrategia híbrida mantiene un alto rendimiento mientras reduce dramáticamente el tiempo de entrenamiento.</p><blockquote>En ModernBERT, cada tercera capa emplea atención global con un theta RoPE de 160,000 y las capas restantes usan una ventana deslizante de atención local de 128 tokens con un theta RoPE de 10,000. —— <a href="https://arxiv.org/pdf/2412.13663">ModernBERT</a></blockquote><h2 id="the-bitter-lesson" style="position: relative;"><a href="#the-bitter-lesson" title="¿La lección amarga?" id="anchor-the-bitter-lesson"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>¿La lección amarga?</h2><p>La ley de escalado y <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">la lección amarga</a> sugieren que las mejoras significativas de rendimiento provienen principalmente del aumento en el número de parámetros y datos de entrenamiento. Este principio guió nuestro enfoque de expandir el corpus y usar LoRA para adaptaciones específicas de tareas.</p><p>Sin embargo, el éxito de ModernBERT ha revelado que subestimamos el poder de la optimización arquitectónica. Demuestra que los SLMs pueden lograr resultados excepcionales a través de una mejor eficiencia datos-modelo, sin necesariamente aumentar los parámetros. Un reciente <a href="https://arxiv.org/pdf/2408.11868">informe técnico de Stella Embeddings</a> refuerza este hallazgo, indicando que los métodos actuales de entrenamiento de modelos de embedding pueden mejorarse sin aumentar el corpus o el tamaño del modelo.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg" class="kg-image" alt="Gráfico que muestra la Ley de Escalado de Modelos de Embedding con 'Tamaño de Parámetros' en el eje x y 'Rendimiento MTEB' en el eje y" width="949" height="949" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Ley de escalado de modelos de embedding. El rendimiento promedio MTEB en tareas en inglés se grafica contra el número de parámetros del modelo. Cada punto representa un modelo de embedding. La línea de tendencia, que representa todos los modelos, está resaltada, con los modelos multilingües enfatizados en cian. Se puede ver que </span><a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a><span style="white-space: pre-wrap;"> demuestra un rendimiento superior comparado con modelos de tamaño similar, mostrando también una mejora superlineal sobre su predecesor, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2</span></code><span style="white-space: pre-wrap;">. Este gráfico fue creado seleccionando los 100 mejores modelos de embedding del ranking MTEB, excluyendo aquellos sin información de tamaño, típicamente modelos cerrados o propietarios. También se filtraron las presentaciones identificadas como trolleo obvio.</span></figcaption></figure><p>Avanzando, anticipamos costos computacionales más bajos y tamaños de modelo más pequeños a medida que obtenemos conocimientos más profundos sobre la utilización de datos e implementamos las técnicas de ModernBERT. A corto plazo, podemos implementar mejoras directas descritas en el paper de ModernBERT - específicamente integrando más datos relacionados con código y adoptando un tokenizador amigable con el código. Cambios más complejos, como cambiar a una arquitectura profunda y delgada o arrancar modelos grandes desde otros más pequeños, requerirán construir modelos base desde cero - una iniciativa a mediano plazo.</p><p>Si bien la eficiencia de ModernBERT es notable, su limitación a solo texto apunta a desafíos futuros. A medida que los modelos de embedding multimodales ganan popularidad, nuestro próximo desafío es desarrollar modelos base de búsqueda más inteligentes, rápidos y capaces que puedan manejar entradas para aplicaciones multimodales. Estas aplicaciones demandan ventanas de contexto aún más largas - un desafío de eficiencia que queda por resolver.</p><h2 id="conclusion" style="position: relative;"><a href="#conclusion" title="Conclusión" id="anchor-conclusion"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Conclusión</h2><p>A lo largo de esta publicación, hemos explorado cómo ModernBERT hace avanzar los modelos de la familia BERT a través de tres innovaciones clave: su arquitectura profunda y delgada, tokenizador optimizado, y escalado eficiente usando mosaico de pesos. Estas mejoras permiten a ModernBERT ofrecer un rendimiento sobresaliente en un tamaño relativamente compacto, superando tanto a <code>RoBERTa-large</code> como a <code>jina-XLM-RoBERTa</code> en varias tareas. ModernBERT demuestra que las mejoras arquitectónicas pueden importar más que el tamaño de los parámetros, abriendo puertas para modelos más eficientes. Su uso exitoso del mosaico de pesos muestra cómo el escalado progresivo puede reducir los costos de entrenamiento mientras preserva o incluso mejora el rendimiento. Además, su vocabulario compacto y optimizaciones dirigidas sugieren oportunidades crecientes para SLMs especializados en entornos con recursos limitados.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categorías:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog de tecnología</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-85e17eff="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-85e17eff="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-85e17eff="" class="col-sm-12 col-md"><div data-v-85e17eff="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Oficinas</div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Sunnyvale, California</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, EE. UU.</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Berlín, Alemania (sede central)</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlín, Alemania</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Beijing, China</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">Piso 5, Edificio 6, No.48 Haidian West St. Pekín, China</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Shenzhen, China</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">Piso 402, Edificio de Tecnología Fu'an, Shenzhen, China</div></div></div></div></div><div data-v-85e17eff="" class="col-sm-12 col-md row"><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fundación de búsqueda</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Búsqueda profunda</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Lector</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Incrustaciones</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">reclasificador</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Clasificador</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Segmentador</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Documentación API</div></a><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Obtener la clave API de Jina</div></div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Límite de velocidad</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-85e17eff="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Estado de la API</div></a></div><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Compañía</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Sala de prensa</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Términos</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Seguridad</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Privacidad</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Administrar cookies</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-85e17eff="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-85e17eff="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-85e17eff="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-85e17eff="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
<!DOCTYPE html><html translate="no" dir="ltr" lang="es"><head><title>Lo que aprendimos en ICLR2025</title><meta charset="utf-8"><meta name="title" content="Lo que aprendimos en ICLR2025"><meta name="description" content="Recopilamos algunos de los artículos más interesantes de ICLR 2025, que incluyen TIPS, FlexPrefill, Reordenadores de Cero Disparos (Zero-Shot Rerankers), SVD-LLM, Hymba, etc."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="og:title" content="Lo que aprendimos en ICLR2025"><meta property="og:description" content="Recopilamos algunos de los artículos más interesantes de ICLR 2025, que incluyen TIPS, FlexPrefill, Reordenadores de Cero Disparos (Zero-Shot Rerankers), SVD-LLM, Hymba, etc."><meta property="og:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="twitter:title" content="Lo que aprendimos en ICLR2025"><meta property="twitter:description" content="Recopilamos algunos de los artículos más interesantes de ICLR 2025, que incluyen TIPS, FlexPrefill, Reordenadores de Cero Disparos (Zero-Shot Rerankers), SVD-LLM, Hymba, etc."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-BE_VvhCJ.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-Dnc_3s_N.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-BC2j9cis.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-Ca8gI6TH.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-DLvKPACl.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-BRZtijun.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-Bh6U6CTA.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/es-DJJjFCSD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-DrGjSCxj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-DziTukrQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-ComJNpP6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-DezJcSU2.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-Cnb6hlLH.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-D2jCsV4F.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-BPWWpmsr.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-Cfo5skPQ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-BbqDTRSJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-CgR6RIri.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-CH6K5iw-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-CpHOIpaC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-DM_q7MQU.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-DGEx9EaX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-cQiGtrxM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-DG1X7fzt.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-9hs37axG.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-BvY2fySK.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-BATrK9sO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-YZNUpv0W.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-BqpMvn3V.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-CRzN7cOR.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-C3V2v7vC.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-pQtGM68P.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-DuBZ7Zbi.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-iHjGH0oX.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-Ctoj8MDT.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-DctY5SC3.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-0v_6KiP0.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Jina AI"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Jina AI"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="21 mins read"><meta property="article:published_time" content="2025-05-26T00:06:37.000+02:00"><meta property="article:modified_time" content="2025-05-26T00:06:37.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Lo que aprendimos en ICLR2025",
  "description": "Recopilamos algunos de los artículos más interesantes de ICLR 2025, que incluyen TIPS, FlexPrefill, Reordenadores de Cero Disparos (Zero-Shot Rerankers), SVD-LLM, Hymba, etc.",
  "image": [
    "https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"
  ],
  "datePublished": "2025-05-26T00:06:37.000+02:00",
  "dateModified": "2025-05-26T00:06:37.000+02:00",
  "author": [
    {
      "@type": "Organization",
      "name": "Jina AI",
      "url": "https://jina.ai"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Noticias</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Modelos</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_6e4b11d9-2574-4448-b76f-cca4fdb1788f" aria-label="Expandir &quot;Productos&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Productos</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_6e4b11d9-2574-4448-b76f-cca4fdb1788f" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Productos"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lector</div><div class="q-item__label q-item__label--caption text-caption">Lea las URL y busque en la web para obtener una base más sólida para su LLM.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Incrustaciones</div><div class="q-item__label q-item__label--caption text-caption">Integraciones multilingües y multimodales de clase mundial.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">reclasificador</div><div class="q-item__label q-item__label--caption text-caption">Recuperador neuronal de clase mundial para maximizar la relevancia de la búsqueda.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Búsqueda profunda</div><div class="q-item__label q-item__label--caption text-caption">Busca, lee y razona hasta encontrar la mejor respuesta.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_2d74f327-6bb2-442f-b51e-b0a58f731490" aria-label="Expandir &quot;Más&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Más</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_2d74f327-6bb2-442f-b51e-b0a58f731490" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Clasificador</div><div class="q-item__label q-item__label--caption text-caption">Clasificación de cero disparos y pocos disparos para imágenes y texto.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmentador</div><div class="q-item__label q-item__label--caption text-caption">Corta el texto largo en fragmentos y haz tokenización.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Documentación de la API</div><div class="q-item__label q-item__label--caption text-caption">Generación automática de código para su IDE o LLM de Copilot</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_862f1c6a-6444-4df0-9b3d-7472170c99d8" aria-label="Expandir &quot;Compañía&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Compañía</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_862f1c6a-6444-4df0-9b3d-7472170c99d8" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Acceso"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Acceso</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_fe4fae0d-a2f9-42d5-89bb-c8a79dfc9e40"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_fe4fae0d-a2f9-42d5-89bb-c8a79dfc9e40" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_fe4fae0d-a2f9-42d5-89bb-c8a79dfc9e40_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Mitigar la brecha: mejorar la alineación intermodal en CLIP</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">jina-clip-v2: Vectores modelo (Embeddings) multilingües y multimodales para texto e imágenes</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ReaderLM-V2: Modelo de lenguaje pequeño para HTML a Markdown y JSON</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TIPS: Preentrenamiento de Texto-Imagen con Conciencia Espacial</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Cut Cross-Entropy: Computación de Pérdida con Eficiencia de Memoria para Grandes Vocabularios</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">FlexPrefill: Atención Esparsa Consciente del Contexto para Secuencias Largas</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Compresión eficaz de *Embeddings* (Embeddings) posteriores al entrenamiento mediante el control de la temperatura</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">La atención en los modelos de lenguaje grandes produce *Rerankers* (Reranker) eficientes de cero disparos</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Puente y modelado de correlaciones en datos por pares para la optimización directa de preferencias</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TAID: Destilación Interpolada Adaptativa Temporalmente para la Transferencia Eficiente de Conocimiento</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">SVD-LLM: Descomposición de Valores Singulares Consciente del Truncamiento para la Compresión de Modelos de Lenguaje Grandes</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Vea Lo Que Se Le Dice: Sumidero de Atención Visual en Modelos Multimodales Grandes</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Hacia la equivalencia semántica de la tokenización en LLM multimodal</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Hymba: una arquitectura de cabeza híbrida para modelos de lenguaje pequeños</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Evento</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">mayo 25, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">Lo que aprendimos en ICLR2025</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Recopilamos algunos de los artículos más interesantes de ICLR 2025, que incluyen TIPS, FlexPrefill, Reordenadores de Cero Disparos (Zero-Shot Rerankers), SVD-LLM, Hymba, etc.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp" style="object-fit: contain; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Jina AI"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Jina AI" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Jina AI • 21 minutos de lectura</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>ICLR 2025 es una de las conferencias de aprendizaje automático más grandes e influyentes del mundo, junto con NeurIPS e ICML como los tres principales lugares para la investigación de IA de alto impacto. Este año marcó un hito histórico ya que ICLR se celebró en Asia por primera vez, teniendo lugar en el Singapore EXPO del 24 al 28 de abril. El momento no podría haber sido más perfecto: solo meses después del "momento DeepSeek" a finales de enero de 2025 que envió ondas de choque a través de Silicon Valley y demostró el rápido avance de la investigación de IA de China. Combinado con el nuevo acuerdo de exención mutua de visa de 30 días entre China y Singapur que entró en vigor en febrero de 2024, fuimos testigos de un aumento sin precedentes en la participación china en la conferencia.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png" class="kg-image" alt="" width="2000" height="1106" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png 2126w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Este año, nuestro equipo estaba emocionado de hacer el viaje a Singapur, con Sedigheh Eslami, Andreas Koukounas, Wang Feng y el CEO Han Xiao presentando tres artículos de investigación que muestran nuestra última investigación sobre <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a> y <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a> para una mejor búsqueda. Si bien el resto del mundo de la IA parece estar encerrado en una carrera armamentista por modelos cada vez más grandes, decidimos nadar contra la norma, demostrando que los modelos más pequeños e inteligentes pueden superar con creces su peso cuando se acierta con el diseño.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png" class="kg-image" alt="" width="2000" height="1391" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png 2108w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Así que toma tu café, ponte cómodo y exploremos algunas de las investigaciones de ICLR que nos parecieron interesantes, comenzando con nuestra propia opinión sobre por qué lo pequeño puede ser poderoso.</p><h2 id="mitigate-the-gap-improving-cross-modal-alignment-in-clip" style="position: relative;"><a href="#mitigate-the-gap-improving-cross-modal-alignment-in-clip" title="Mitigar la brecha: mejorar la alineación intermodal en CLIP" id="anchor-mitigate-the-gap-improving-cross-modal-alignment-in-clip"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mitigar la brecha: mejorar la alineación intermodal en CLIP</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.17639"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</div><div class="kg-bookmark-description">Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding space overly sparse and disconnected, with different modalities being densely distributed in distinct subregions of the hypersphere. In this work, we aim at answering three main questions: 1. Does sharing the parameter space between the multi-modal encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart the uni-modal embeddings via intra-modality separation? 3. How do these gap reduction approaches affect the downstream performance? We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings, and thereby, reduces the modality gap, while improving the performance across several zero-shot and fine-tuning downstream evaluations.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-21.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Sedigheh Eslami</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-17.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png" class="kg-image" alt="" width="2000" height="1279" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png 2120w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Los modelos CLIP sobresalen en las tareas de imagen-texto, pero sufren de una <a href="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models">"brecha de modalidad"</a>: los vectores modelo (Embeddings) de imagen y texto se agrupan en regiones separadas, lo que limita el rendimiento. Este trabajo, dirigido por nuestra pasante Sedigheh Eslami durante su doctorado en el Hasso Plattner Institute, aborda este problema fundamental.</p><p>Descubrimos que la traducción simple de vectores rompe la estructura de los vectores modelo (Embeddings). En cambio, <strong>AlignCLIP</strong> utiliza parámetros de codificador compartidos con objetivos de separación semánticamente regularizados. Este enfoque dual reduce con éxito la brecha de modalidad al tiempo que mejora el rendimiento en las tareas de ajuste fino y cero disparos.</p><p><strong>Conclusiones:</strong></p><ul><li>La brecha de modalidad es un cuello de botella crítico en el rendimiento de CLIP</li><li>El intercambio de parámetros + la separación semántica une eficazmente las diferencias modales</li><li>El enfoque ofrece ganancias medibles en las evaluaciones posteriores</li></ul><h2 id="jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" style="position: relative;"><a href="#jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" title="jina-clip-v2: Vectores modelo (Embeddings) multilingües y multimodales para texto e imágenes" id="anchor-jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>jina-clip-v2: Vectores modelo (Embeddings) multilingües y multimodales para texto e imágenes</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2412.08802"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model’s performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-22.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-18.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png" class="kg-image" alt="" width="2000" height="1115" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png 2268w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Este es el documento detrás de <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a>, un modelo de vector modelo (Embedding) multimodal multilingüe que admite tareas solo de texto e intermodales utilizando un enfoque de aprendizaje contrastivo multietapa y multitarea. El modelo combina un codificador de texto (Jina XLM-RoBERTa, 561 millones de parámetros) y un codificador de visión (EVA02-L14, 304 millones de parámetros) para un total de 865 millones de parámetros. Entrenamos con textos multilingües de 29 idiomas no ingleses y documentos visualmente ricos, empleando Matryoshka Representation Learning para una dimensionalidad de vector modelo (Embedding) flexible.</p><p><strong>Conclusiones:</strong></p><ul><li>Mezclar datos de imagen-texto y texto-texto en lotes individuales con parámetros de temperatura compartidos funciona peor que el entrenamiento separado debido a la asimetría de la información de modalidad.</li><li>El entrenamiento para la alineación intermodal compromete inherentemente la calidad del vector modelo (Embedding) de texto puro, lo que muestra una compensación fundamental.</li><li>Recortar los vectores modelo (Embeddings) de 1024 a 256 dimensiones causa menos del 1% de pérdida de rendimiento, lo que revela una ineficiencia masiva en las representaciones de alta dimensión.</li></ul><h2 id="readerlm-v2-small-language-model-for-html-to-markdown-and-json" style="position: relative;"><a href="#readerlm-v2-small-language-model-for-html-to-markdown-and-json" title="ReaderLM-V2: Modelo de lenguaje pequeño para HTML a Markdown y JSON" id="anchor-readerlm-v2-small-language-model-for-html-to-markdown-and-json"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ReaderLM-V2: Modelo de lenguaje pequeño para HTML a Markdown y JSON</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.01151"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</div><div class="kg-bookmark-description">We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model’s effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-23.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Feng Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-19.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png" class="kg-image" alt="" width="1196" height="912" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png 1196w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Este es el artículo detrás de <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a>, un modelo de lenguaje compacto de 1500 millones de parámetros diseñado para la extracción eficiente de contenido web. El modelo procesa documentos de hasta 512 000 *tokens* (Tokens), transformando HTML desordenado en formatos Markdown o JSON limpios. Nuestro enfoque combina un *pipeline* de síntesis de datos de tres etapas (DRAFT-REFINE-CRITIQUE) que genera datos de entrenamiento de alta calidad a través del refinamiento iterativo con un marco de entrenamiento unificado que combina el preentrenamiento continuo, el ajuste fino supervisado, la optimización directa de preferencias y el ajuste iterativo de auto-juego. ReaderLM-v2 supera a GPT-4o y a otros modelos más grandes en un 15-20% en los *benchmarks*, destacando especialmente en documentos que superan los 100 000 *tokens*, al tiempo que mantiene requisitos computacionales significativamente más bajos.</p><p><strong>Conclusiones:</strong></p><ul><li>Un modelo de 1500 millones de parámetros supera a los modelos GPT-4o y de 32 000 millones en un 15-20% en la extracción de HTML, lo que demuestra que el ajuste fino específico de la tarea supera la escala bruta para la experiencia en el dominio.</li><li>El modelo genera sus propios datos de entrenamiento en la etapa 4 de "auto-juego", creando mejores conjuntos de datos que los seleccionados por humanos y mejorando continuamente el rendimiento a través de la retroalimentación recursiva.</li><li>El modelo sufrió una repetición catastrófica de *tokens* durante el entrenamiento, pero la adición de pérdida contrastiva para fomentar representaciones discriminativas eliminó por completo este problema de degeneración.</li></ul><h2 id="tips-text-image-pretraining-with-spatial-awareness" style="position: relative;"><a href="#tips-text-image-pretraining-with-spatial-awareness" title="TIPS: Preentrenamiento de Texto-Imagen con Conciencia Espacial" id="anchor-tips-text-image-pretraining-with-spatial-awareness"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TIPS: Preentrenamiento de Texto-Imagen con Conciencia Espacial</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.16512"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TIPS: Preentrenamiento de Texto-Imagen con Conciencia Espacial</div><div class="kg-bookmark-description">Si bien el aprendizaje de la representación de imagen-texto se ha vuelto muy popular en los últimos años, los modelos existentes tienden a carecer de conciencia espacial y tienen una aplicabilidad directa limitada para tareas de comprensión densa. Por esta razón, el preentrenamiento auto-supervisado solo con imágenes sigue siendo el método preferido para muchas aplicaciones de visión densa (por ejemplo, estimación de profundidad, segmentación semántica), a pesar de la falta de señales de supervisión explícitas. En este artículo, cerramos esta brecha entre el aprendizaje de imagen-texto y el auto-supervisado, proponiendo un nuevo modelo de imagen-texto de propósito general, que se puede utilizar de forma eficaz para tareas de visión densa y global. Nuestro método, al que nos referimos como Preentrenamiento de Texto-Imagen con Conciencia Espacial (TIPS), aprovecha dos ideas simples y eficaces. Primero, en la supervisión textual: revelamos que reemplazar los subtítulos de imágenes web ruidosas por descripciones textuales generadas sintéticamente aumenta significativamente el rendimiento de la comprensión densa, debido a una señal mucho más rica para el aprendizaje de representaciones con conciencia espacial. Proponemos un método de entrenamiento adaptado que combina subtítulos ruidosos y sintéticos, lo que resulta en mejoras en las tareas de comprensión densa y global. En segundo lugar, en la técnica de aprendizaje: proponemos combinar el aprendizaje contrastivo de imagen-texto con el modelado de imágenes enmascaradas auto-supervisado, para fomentar la coherencia espacial, desbloqueando mejoras sustanciales para las aplicaciones posteriores. Basándonos en estas dos ideas, escalamos nuestro modelo utilizando la arquitectura *transformer*, entrenado en un conjunto curado de imágenes públicas. Nuestros experimentos se llevan a cabo en 8 tareas que involucran 16 conjuntos de datos en total, lo que demuestra un sólido rendimiento inmediato tanto en la comprensión densa como en la global, para varias tareas solo de imagen e imagen-texto. El código y los modelos se publican en https://github.com/google-deepmind/tips.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-24.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Kevis-Kokitsi Maninis</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-20.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png" class="kg-image" alt="" width="2000" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png 2210w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Los modelos de visión-lenguaje entrenados con aprendizaje contrastivo sobresalen en la alineación global de imagen-texto, pero fallan en las tareas de comprensión espacial densa. TIPS combina el aprendizaje contrastivo con el modelado de imágenes enmascaradas y utiliza subtítulos generados sintéticamente que codifican relaciones espaciales, creando *embeddings* (Vectoriales) adecuados tanto para la comprensión densa como para la global sin un ajuste fino específico de la tarea. El enfoque demuestra cómo la conciencia espacial se puede incorporar en los modelos de *embedding* para una mejor comprensión de documentos y aplicaciones de recuperación multimodal.</p><p><strong>Conclusiones:</strong></p><ul><li>Los subtítulos sintéticos con descripciones espaciales proporcionan señales de entrenamiento más ricas que los subtítulos web ruidosos para aprender representaciones con conciencia espacial</li><li>La combinación del aprendizaje contrastivo de imagen-texto con objetivos auto-supervisados cierra la brecha entre la comprensión global y la densa</li><li>El rendimiento inmediato en diversas tareas elimina la necesidad de un ajuste fino especializado en diferentes aplicaciones de visión</li></ul><h2 id="cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" style="position: relative;"><a href="#cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" title="Cut Cross-Entropy: Computación de Pérdida con Eficiencia de Memoria para Grandes Vocabularios" id="anchor-cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Cut Cross-Entropy: Computación de Pérdida con Eficiencia de Memoria para Grandes Vocabularios</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.09009"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reduce Tus Pérdidas en Modelos de Lenguaje de Gran Vocabulario</div><div class="kg-bookmark-description">A medida que los modelos de lenguaje crecen cada vez más, también lo hacen sus vocabularios. Esto ha desplazado la huella de memoria de los LLM durante el entrenamiento de forma desproporcionada a una sola capa: la entropía cruzada en el cálculo de la pérdida. La entropía cruzada construye una matriz *logit* con entradas para cada par de *tokens* de entrada y elementos de vocabulario y, para los modelos pequeños, consume un orden de magnitud más memoria que el resto del LLM combinado. Proponemos Cut Cross-Entropy (CCE), un método que calcula la pérdida de entropía cruzada sin materializar los *logits* para todos los *tokens* en la memoria global. Más bien, CCE solo calcula el *logit* para el *token* correcto y evalúa el log-sum-exp sobre todos los *logits* sobre la marcha. Implementamos un *kernel* personalizado que realiza las multiplicaciones de matrices y la reducción log-sum-exp sobre el vocabulario en la memoria *flash*, lo que hace que el consumo de memoria global para el cálculo de la entropía cruzada sea insignificante. Esto tiene un efecto dramático. Tomando el modelo Gemma 2 (2B) como ejemplo, CCE reduce la huella de memoria del cálculo de la pérdida de 24 GB a 1 MB, y el consumo total de memoria en tiempo de entrenamiento del encabezado del clasificador de 28 GB a 1 GB. Para mejorar el rendimiento de CCE, aprovechamos la escasez inherente de *softmax* y proponemos omitir elementos del cálculo del gradiente que tienen una contribución insignificante (es decir, por debajo de la precisión numérica) al gradiente. Los experimentos demuestran que la dramática reducción en el consumo de memoria se logra sin sacrificar la velocidad de entrenamiento o la convergencia.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-25.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Erik Wijmans</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-21.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png" class="kg-image" alt="" width="1904" height="1226" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png 1904w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>La computación de entropía cruzada domina el uso de memoria en los modelos de lenguaje de gran vocabulario, requiriendo la materialización de matrices *logit* proporcionales a batch_size × vocabulary_size. CCE reformula el cálculo para computar solo los componentes necesarios sobre la marcha utilizando *kernels* CUDA personalizados, reduciendo el consumo de memoria de gigabytes a megabytes mientras se mantienen dinámicas de entrenamiento idénticas. Esto permite el entrenamiento de modelos de *embedding* y *reranking* (Reorganización) con vocabularios más grandes en *hardware* limitado, particularmente beneficioso para aplicaciones multilingües y específicas de dominio.</p><p><strong>Conclusiones:</strong></p><ul><li>La computación de pérdida de entropía cruzada puede consumir el 90% de la memoria de entrenamiento para modelos de gran vocabulario, convirtiéndose en el principal cuello de botella</li><li>La computación sobre la marcha de los términos log-sum-exp elimina la necesidad de materializar matrices *logit* completas sin aproximaciones matemáticas</li><li>La implementación de *kernel* personalizado permite una reducción dramática de la memoria mientras se preservan las propiedades de convergencia exactas</li></ul><h2 id="flexprefill-context-aware-sparse-attention-for-long-sequences" style="position: relative;"><a href="#flexprefill-context-aware-sparse-attention-for-long-sequences" title="FlexPrefill: Atención Esparsa Consciente del Contexto para Secuencias Largas" id="anchor-flexprefill-context-aware-sparse-attention-for-long-sequences"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>FlexPrefill: Atención Esparsa Consciente del Contexto para Secuencias Largas</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2502.20766"><div class="kg-bookmark-content"><div class="kg-bookmark-title">FlexPrefill: Un Mecanismo de Atención Esparsa Consciente del Contexto para la Inferencia Eficiente de Secuencias Largas</div><div class="kg-bookmark-description">Los modelos de lenguaje grandes (LLM) se enfrentan a desafíos computacionales durante la inferencia de secuencias largas, especialmente en la fase de prellenado de atención, donde la complejidad crece cuadráticamente con la longitud del *prompt* (prompt). Los esfuerzos anteriores para mitigar estos desafíos se han basado en patrones de atención dispersos fijos o en la identificación de patrones de atención dispersos basados en casos limitados. Sin embargo, estos métodos carecían de la flexibilidad necesaria para adaptarse de manera eficiente a las diferentes demandas de entrada. En este artículo, presentamos FlexPrefill, un mecanismo de prellenado disperso flexible que ajusta dinámicamente los patrones de atención dispersos y el presupuesto computacional en tiempo real para satisfacer los requisitos específicos de cada entrada y encabezado de atención. La flexibilidad de nuestro método se demuestra a través de dos innovaciones clave: 1) Determinación de patrones dispersos conscientes de la consulta: al medir la divergencia de Jensen-Shannon, este componente cambia adaptativamente entre patrones de atención diversos específicos de la consulta y patrones de atención predefinidos. 2) Selección de índice basada en la atención acumulativa: este componente selecciona dinámicamente los índices de consulta-clave que se calcularán en función de diferentes patrones de atención, asegurando que la suma de las puntuaciones de atención cumpla con un umbral predefinido. FlexPrefill optimiza adaptativamente el patrón disperso y la relación dispersa de cada encabezado de atención basándose en el *prompt* (prompt), lo que mejora la eficiencia en las tareas de inferencia de secuencias largas. Los resultados experimentales muestran mejoras significativas tanto en velocidad como en precisión con respecto a los métodos anteriores, lo que proporciona una solución más flexible y eficiente para la inferencia de LLM.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-26.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xunhao Lai</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-22.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png" class="kg-image" alt="" width="1882" height="1254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png 1882w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>La inferencia del transformador de secuencia larga sufre de complejidad de atención cuadrática. FlexPrefill determina dinámicamente los patrones de atención dispersos por encabezado utilizando la divergencia de Jensen-Shannon y asigna adaptativamente el presupuesto computacional en función de las puntuaciones de atención acumulativas, logrando aceleraciones significativas con una pérdida de precisión mínima en diversos tipos de contenido. El método permite el procesamiento eficiente de documentos largos para sistemas de búsqueda y recuperación, lo que permite que los modelos de lenguaje más pequeños manejen contextos extendidos para una mejor comprensión del documento.</p><p><strong>Conclusiones:</strong></p><ul><li>Los patrones de atención dispersos dinámicos adaptados al tipo de contenido superan a las estrategias de dispersión fija en diferentes características de entrada.</li><li>La asignación de presupuesto adaptable por encabezado basada en la acumulación de puntuación de atención optimiza la distribución de la computación en tiempo real.</li><li>La dispersión consciente del contexto logra una aceleración de 13,7× con una pérdida de precisión del 0,1% sin necesidad de volver a entrenar el modelo.</li></ul><h2 id="effective-post-training-embedding-compression-via-temperature-control" style="position: relative;"><a href="#effective-post-training-embedding-compression-via-temperature-control" title="Compresión eficaz de *Embeddings* (Embeddings) posteriores al entrenamiento mediante el control de la temperatura" id="anchor-effective-post-training-embedding-compression-via-temperature-control"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Compresión eficaz de *Embeddings* (Embeddings) posteriores al entrenamiento mediante el control de la temperatura</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://openreview.net/forum?id=szRmEM8Kx5"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Effective post-training embedding compression via temperature...</div><div class="kg-bookmark-description">Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates…</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-37.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">OpenReview.net</span><span class="kg-bookmark-publisher">Georgiana Dinu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/pdf_icon_blue.svg" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png" class="kg-image" alt="" width="1230" height="906" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png 1230w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>El escalado de la temperatura en el aprendizaje contrastivo influye significativamente en la dimensionalidad intrínseca de los *embeddings* (Embeddings) aprendidos, con temperaturas más bajas que producen representaciones más compresibles. El artículo demuestra que los métodos de agregación de temperatura pueden reducir las dimensiones de los *embeddings* (Embeddings) en un orden de magnitud, manteniendo al mismo tiempo el rendimiento de la recuperación, lo que revela la compensación entre la eficacia de la agrupación y la precisión de la recuperación. Esto permite el despliegue eficiente de sistemas de recuperación densa donde las limitaciones de memoria son críticas para las aplicaciones de producción.</p><p><strong>Conclusiones:</strong></p><ul><li>Los valores de temperatura más bajos en el entrenamiento contrastivo producen *embeddings* (Embeddings) con una dimensionalidad intrínseca más baja que se comprimen de forma más eficaz.</li><li>Las técnicas de agregación de temperatura logran ratios de compresión de 10× con una degradación mínima de la calidad en las tareas de recuperación.</li><li>El control sistemático de la temperatura durante el entrenamiento proporciona un mecanismo directo para optimizar la compensación entre compresión y rendimiento.</li></ul><h2 id="attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" style="position: relative;"><a href="#attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" title="La atención en los modelos de lenguaje grandes produce *Rerankers* (Reranker) eficientes de cero disparos" id="anchor-attention-in-large-language-models-yields-efficient-zero-shot-re-rankers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>La atención en los modelos de lenguaje grandes produce *Rerankers* (Reranker) eficientes de cero disparos</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.02642"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers</div><div class="kg-bookmark-description">Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more directly leverage such signals, we propose in-context re-ranking (ICR), a novel method that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>) forward passes to re-rank <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> documents, making it substantially more efficient than generative re-ranking methods that require at least <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mclose">)</span></span></span></span></span> forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR’s performance is specially strong on tasks that require more complex re-ranking signals. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-27.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shijie Chen</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-23.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd_nbgJG03yk0oOma7ozLQ6zutKGy3ngkVLKUwmp3ie6UPp2RR6qjwsWzwwtP0QzyAneCTD24nrPXpA085rkjtS_HmlrkHbrksxSjsaknx1lb9OtgtlACmwoOZkoRK9oPb4Haf?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>La re-clasificación en contexto (ICR) aprovecha los cambios en el patrón de atención en los LLM para realizar la re-clasificación de documentos sin generación de texto, reduciendo la complejidad computacional de O(N log N) a O(1). El método agrega pesos de atención a través de capas y encabezados para calcular las puntuaciones de relevancia, con calibración de consulta sin contenido para mitigar los sesgos del LLM. Este enfoque permite una re-clasificación eficiente con modelos de peso abierto, eliminando la necesidad de un ajuste fino especializado o procesos de generación costosos.</p><p><strong>Conclusiones:</strong> </p><ul><li>Los patrones de atención en los LLM contienen señales suficientes para la re-clasificación eficaz de documentos sin necesidad de generación de texto.</li><li>La calibración de consultas sin contenido mitiga con éxito los sesgos intrínsecos en los mecanismos de puntuación basados en la atención.</li><li>ICR logra un rendimiento y una eficiencia superiores en comparación con los métodos generativos, particularmente en tareas complejas de recuperación multi-salto.</li></ul><h2 id="bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" style="position: relative;"><a href="#bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" title="Puente y modelado de correlaciones en datos por pares para la optimización directa de preferencias" id="anchor-bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Puente y modelado de correlaciones en datos por pares para la optimización directa de preferencias</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2408.07471"></a><div class="kg-bookmark-content"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2408.07471"><div class="kg-bookmark-title">Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization</div></a><div class="kg-bookmark-description"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2408.07471">La optimización directa de preferencias (DPO, Direct Preference Optimization), un algoritmo de optimización de preferencias offline ampliamente adoptado, tiene como objetivo alinear los modelos de lenguaje grandes (LLM, Large Language Models) con los comportamientos deseados por los humanos utilizando datos de preferencias por pares. Sin embargo, la generación de la respuesta ganadora y la respuesta perdedora dentro de los datos por pares suelen estar aisladas, lo que conduce a correlaciones débiles entre ellas, así como a un rendimiento de alineación subóptimo. Para abordar este problema, proponemos un marco eficaz para "Puente y Modelado de Correlaciones" (Bridging and Modeling Correlations) en datos por pares, denominado BMC. En primer lugar, aumentamos la coherencia y el poder informativo de las señales de preferencia por pares a través de modificaciones específicas, sintetizando una respuesta pseudo-ganadora mejorando la respuesta perdedora con la respuesta ganadora como referencia. En segundo lugar, identificamos que el DPO por sí solo es insuficiente para modelar estas correlaciones y capturar variaciones matizadas. Por lo tanto, proponemos aprender las correlaciones a nivel de "词元 (Tokens)" aprovechando dinámicamente la confianza del modelo de política durante el entrenamiento. Experimentos exhaustivos en tareas de QA, matemáticas y seguimiento de instrucciones demuestran la eficacia de nuestro enfoque, superando significativamente las líneas de base competitivas, incluido el DPO. Además, nuestro análisis cuantitativo en profundidad revela las razones detrás del rendimiento superior de nuestro método sobre el DPO y muestra su versatilidad a otras variantes de DPO. Lanzamos nuestro repositorio en </a><a href="https://github.com/YJiangcm/BMC">https://github.com/YJiangcm/BMC</a>.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-28.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Yuxin Jiang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-24.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd94pEqUtljUS8gF3KonDFqs9umcVCfXEASHlAeCjl07YMviucHiIj1doZIe5_VHSVxthzhgA_ta0E90vQVcunSRj0UnHsubFzD75ow-EfNICcDadQvdtUx-WOZGt9v9rFB_4E?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>El DPO tradicional sufre de correlaciones débiles entre las respuestas elegidas y las rechazadas en los pares de preferencias, lo que limita la eficacia de la alineación. BMC aborda esto sintetizando respuestas pseudo-preferidas que interpolan entre las respuestas ganadoras y perdedoras, luego aplica el modelado de correlación a nivel de "词元 (Tokens)" utilizando la confianza del modelo de política. El enfoque de dos fases primero une los pares de preferencias a través de modificaciones específicas, luego modela las correlaciones de grano fino durante el entrenamiento para mejorar la calidad de la señal de aprendizaje.</p><p><strong>Conclusiones:</strong></p><ul><li>Las correlaciones débiles entre las respuestas elegidas y las rechazadas en los datos de preferencias limitan significativamente la eficacia del DPO para la alineación del modelo</li><li>La síntesis de respuestas pseudo-preferidas como interpolaciones entre pares de preferencias proporciona señales de aprendizaje más ricas para la optimización</li><li>El modelado de correlación a nivel de "词元 (Tokens)" que utiliza la confianza de la política pondera dinámicamente las señales de entrenamiento para capturar variaciones matizadas en los datos de preferencias</li></ul><h2 id="taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" style="position: relative;"><a href="#taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" title="TAID: Destilación Interpolada Adaptativa Temporalmente para la Transferencia Eficiente de Conocimiento" id="anchor-taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TAID: Destilación Interpolada Adaptativa Temporalmente para la Transferencia Eficiente de Conocimiento</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2501.16937"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TAID: Destilación Interpolada Adaptativa Temporalmente para la Transferencia Eficiente de Conocimiento en Modelos de Lenguaje</div><div class="kg-bookmark-description">Los modelos de lenguaje causales han demostrado capacidades notables, pero su tamaño plantea desafíos importantes para el despliegue en entornos con recursos limitados. La destilación de conocimiento, una técnica ampliamente utilizada para transferir conocimiento de un modelo maestro grande a un modelo estudiante pequeño, presenta un enfoque prometedor para la compresión de modelos. Un problema importante que persiste radica en las principales diferencias entre los modelos maestro y estudiante, a saber, la importante brecha de capacidad, el promedio de modo y el colapso de modo, que plantean barreras durante la destilación. Para abordar estos problemas, presentamos <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">Temporally</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">Adaptive</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">Interpolated</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">Distillation</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">(TAID)</mtext></mrow><annotation encoding="application/x-tex">\textit{Temporally Adaptive Interpolated Distillation (TAID)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord textit">Temporally&nbsp;Adaptive&nbsp;Interpolated&nbsp;Distillation&nbsp;(TAID)</span></span></span></span></span></span>, un nuevo enfoque de destilación de conocimiento que interpola dinámicamente las distribuciones de estudiante y maestro a través de una distribución intermedia adaptativa, cambiando gradualmente de la distribución inicial del estudiante hacia la distribución del maestro. Proporcionamos un análisis teórico que demuestra la capacidad de TAID para prevenir el colapso de modo y mostramos empíricamente su eficacia para abordar la brecha de capacidad al tiempo que equilibra el promedio de modo y el colapso de modo. Nuestros exhaustivos experimentos demuestran el rendimiento superior de TAID en varios tamaños y arquitecturas de modelos tanto en el ajuste de instrucciones como en escenarios de preentrenamiento. Además, mostramos el impacto práctico de TAID al desarrollar dos modelos de base compactos de última generación: <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-LLM-1.5B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-LLM-1.5B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-LLM-1.5B</span></span></span></span></span></span> para tareas de lenguaje y <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-VLM-2B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-VLM-2B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-VLM-2B</span></span></span></span></span></span> para tareas de visión-lenguaje. Estos resultados demuestran la eficacia de TAID en la creación de modelos eficientes y de alto rendimiento, lo que avanza el desarrollo de tecnologías de IA más accesibles.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-29.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Makoto Shing</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-25.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe9EWtq20jDVieU8M2BPDP5kENd3oSJKwIKNnKq_9bB4mb9vtNvjK-RMx8ZB29EhnyjIST90b2HRNek6bSkPXFlOxzTPhUAjf86d6iBCphJtgjfcxrCdY__HcDW9ADgVla1mVWBpQ?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="312" style="cursor: help;"></figure><p>La destilación de conocimiento enfrenta desafíos debido a las brechas de capacidad, el promedio de modo y el colapso de modo al transferir conocimiento entre modelos grandes y pequeños. TAID introduce un maestro intermedio dinámico que interpola entre las distribuciones del estudiante y del maestro, adaptando gradualmente la distribución objetivo en función del progreso del entrenamiento. Este enfoque previene el colapso de modo a través de garantías teóricas y logra un rendimiento superior en varios tamaños de modelo, lo que permite el desarrollo de modelos de lenguaje compactos pero capaces.</p><p><strong>Conclusiones:</strong></p><ul><li>Los maestros intermedios dinámicos que se adaptan durante el entrenamiento proporcionan trayectorias de aprendizaje más suaves en comparación con la destilación de maestros fijos</li><li>TAID previene el colapso de modo a través de la interpolación adaptativa al tiempo que equilibra la transferencia de conocimiento a través de diferentes brechas de capacidad</li><li>El método permite el entrenamiento de modelos compactos de última generación sin requerir arquitecturas especializadas o un ajuste extenso de hiperparámetros</li></ul><h2 id="svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" style="position: relative;"><a href="#svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" title="SVD-LLM: Descomposición de Valores Singulares Consciente del Truncamiento para la Compresión de Modelos de Lenguaje Grandes" id="anchor-svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>SVD-LLM: Descomposición de Valores Singulares Consciente del Truncamiento para la Compresión de Modelos de Lenguaje Grandes</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2403.07378"></a><div class="kg-bookmark-content"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2403.07378"><div class="kg-bookmark-title">SVD-LLM: Descomposición de Valores Singulares Consciente del Truncamiento para la Compresión de Modelos de Lenguaje Grandes</div></a><div class="kg-bookmark-description"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2403.07378">Los avances en los Modelos de Lenguaje Grandes (LLM, Large Language Models) se han visto obstaculizados por sus tamaños sustanciales, lo que requiere métodos de compresión de LLM para un despliegue práctico. La Descomposición de Valores Singulares (SVD, Singular Value Decomposition) ofrece una solución prometedora para la compresión de LLM. Sin embargo, los métodos de compresión de LLM basados en SVD de última generación tienen dos limitaciones clave: truncar valores singulares más pequeños puede conducir a una mayor pérdida de compresión y la falta de actualización de los pesos comprimidos después del truncamiento de SVD. En este trabajo, proponemos SVD-LLM, un método de compresión de LLM posterior al entrenamiento basado en SVD que aborda las limitaciones de los métodos existentes. SVD-LLM incorpora una técnica de blanqueamiento de datos consciente del truncamiento para garantizar una asignación directa entre los valores singulares y la pérdida de compresión. Además, SVD-LLM adopta una actualización de parámetros con aproximación secuencial de bajo rango para compensar la degradación de la precisión después de la compresión SVD. Evaluamos SVD-LLM en 10 conjuntos de datos y siete modelos de tres familias diferentes de LLM en tres escalas diferentes. Nuestros resultados demuestran la superioridad de SVD-LLM sobre el estado del arte, especialmente en altas tasas de compresión de modelos. Nuestro código está disponible en </a><a href="https://github.com/AIoT-MLSys-Lab/SVD-LLM">https://github.com/AIoT-MLSys-Lab/SVD-LLM</a></div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-30.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-26.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXenhj46Ar7NKFevDTmA3FK2dnjd7nQxdhULJ1H3Je-2OKoQN6_Ov8km-AvIldpEriENz2Q465hq2yoOZ1lLAle7ijbMgSK0ME9UxNeIN3yqyRFtRO_3FFXEyXdI04wndPS17a-3?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Los métodos de compresión basados en SVD existentes no tienen en cuenta las activaciones de entrada durante la aproximación y carecen de un ajuste fino posterior al truncamiento. SVD-LLM incorpora un blanqueamiento de datos consciente del truncamiento que considera las distribuciones de activación y aplica un ajuste fino basado en LoRA después de la compresión. El método establece conexiones teóricas entre los valores singulares y la pérdida de compresión, lo que permite tomar decisiones de compresión más fundamentadas que superan los enfoques de poda estructurada y cuantificación.</p><p><strong>Conclusiones:</strong></p><ul><li>El blanqueamiento de datos consciente del truncamiento que tiene en cuenta las activaciones de entrada mejora significativamente la eficacia de la compresión SVD en comparación con los métodos agnósticos a la activación.</li><li>El ajuste fino LoRA posterior a la compresión compensa la degradación de la precisión al tiempo que mantiene los beneficios de la factorización de bajo rango.</li><li>El análisis teórico que vincula los valores singulares con la pérdida de compresión permite tomar decisiones de truncamiento fundamentadas que superan los enfoques heurísticos.</li></ul><h2 id="see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" style="position: relative;"><a href="#see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" title="Vea Lo Que Se Le Dice: Sumidero de Atención Visual en Modelos Multimodales Grandes" id="anchor-see-what-you-are-told-visual-attention-sink-in-large-multimodal-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Vea Lo Que Se Le Dice: Sumidero de Atención Visual en Modelos Multimodales Grandes</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.03321"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Vea Lo Que Se Le Dice: Sumidero de Atención Visual en Modelos Multimodales Grandes</div><div class="kg-bookmark-description">Los modelos multimodales grandes (LMMs) "ven" las imágenes aprovechando el mecanismo de atención entre los tokens de texto y visuales en el decodificador del transformador. Idealmente, estos modelos deberían centrarse en la información visual clave relevante para el token de texto. Sin embargo, hallazgos recientes indican que los LMMs tienen una tendencia extraordinaria a asignar consistentemente altos pesos de atención a tokens visuales específicos, incluso cuando estos tokens son irrelevantes para el texto correspondiente. En este estudio, investigamos la propiedad detrás de la aparición de estos tokens visuales irrelevantes y examinamos sus características. Nuestros hallazgos muestran que este comportamiento surge debido a la activación masiva de ciertas dimensiones del estado oculto, que se asemeja al sumidero de atención encontrado en los modelos de lenguaje. Por lo tanto, nos referimos a este fenómeno como el sumidero de atención visual. En particular, nuestro análisis revela que la eliminación de los tokens de sumidero visual irrelevantes no afecta el rendimiento del modelo, a pesar de recibir altos pesos de atención. En consecuencia, reciclamos la atención a estos tokens como recursos excedentes, redistribuyendo el presupuesto de atención para mejorar el enfoque en la imagen. Para lograr esto, introducimos la Redistribución de la Atención Visual (VAR), un método que redistribuye la atención en los encabezados centrados en la imagen, que identificamos como inherentemente enfocados en la información visual. VAR se puede aplicar sin problemas en diferentes LMMs para mejorar el rendimiento en una amplia gama de tareas, incluidas las tareas generales de visión-lenguaje, las tareas de alucinación visual y las tareas centradas en la visión, todo ello sin la necesidad de capacitación, modelos o pasos de inferencia adicionales. Los resultados experimentales demuestran que VAR permite a los LMMs procesar la información visual de manera más eficaz ajustando sus mecanismos de atención internos, ofreciendo una nueva dirección para mejorar las capacidades multimodales de los LMMs.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-31.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Seil Kang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-27.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrHjLu7qX5TOjtDH10svBjs-6rihxNRpgS3Bq9r8qtY9UvOC4LqyBo-NDWeESuRrv-vj6btANt6doA4IneaENN1712o3kzHhQwx20PR62b8JKDA5jIjCNgKAhXoCp9bEcbadyfPA?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Los modelos multimodales grandes exhiben un fenómeno llamado "sumidero de atención visual" donde consistentemente asignan altos pesos de atención a tokens visuales específicos que son irrelevantes para los tokens de texto correspondientes. Estos tokens visuales irrelevantes emergen de la activación masiva en dimensiones de estado oculto específicas, similar a los sumideros de atención en los modelos de lenguaje. El método de Redistribución de la Atención Visual (VAR) identifica los encabezados de atención centrados en la imagen y redistribuye el presupuesto de atención de los tokens de sumidero al contenido visual significativo, mejorando el rendimiento en las tareas de visión-lenguaje sin necesidad de capacitación adicional.</p><p><strong>Conclusiones clave:</strong> </p><ul><li>Los tokens de sumidero visual se pueden identificar por magnitudes de activación extremas en dimensiones fijas heredadas de los modelos de lenguaje base</li><li>La eliminación de los tokens de sumidero visual no afecta el rendimiento del modelo a pesar de recibir altos pesos de atención, lo que indica un desperdicio de recursos computacionales</li><li>VAR redistribuye la atención de los tokens de sumidero al contenido visual significativo, mejorando el rendimiento en visión-lenguaje general, la reducción de alucinaciones y las tareas centradas en la visión</li></ul><h2 id="towards-semantic-equivalence-of-tokenization-in-multimodal-llm" style="position: relative;"><a href="#towards-semantic-equivalence-of-tokenization-in-multimodal-llm" title="Hacia la equivalencia semántica de la tokenización en LLM multimodal" id="anchor-towards-semantic-equivalence-of-tokenization-in-multimodal-llm"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Hacia la equivalencia semántica de la tokenización en LLM multimodal</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.05127"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hacia la equivalencia semántica de la tokenización en LLM multimodal</div><div class="kg-bookmark-description">Los modelos de lenguaje grandes multimodales (MLLMs) han demostrado capacidades excepcionales en el procesamiento de tareas de visión-lenguaje. Uno de los puntos cruciales de los MLLMs radica en la tokenización de la visión, que implica transformar eficientemente las señales visuales de entrada en representaciones de características que son más beneficiosas para los LLMs. Sin embargo, los tokenizadores de visión existentes, esenciales para la alineación semántica entre la visión y el lenguaje, siguen siendo problemáticos. Los métodos existentes fragmentan agresivamente la entrada visual, corrompiendo la integridad semántica visual. Para abordar esto, este documento propone un nuevo tokenizador de visión semántico-equivalente dinámico (SeTok), que agrupa las características visuales en unidades semánticas a través de un algoritmo de agrupación dinámica, determinando de manera flexible el número de tokens en función de la complejidad de la imagen. Los tokens de visión resultantes preservan eficazmente la integridad semántica y capturan tanto las características visuales de baja frecuencia como las de alta frecuencia. El MLLM propuesto (Setokim) equipado con SeTok demuestra significativamente un rendimiento superior en varias tareas, como lo demuestran nuestros resultados experimentales. La página del proyecto está en https://chocowu.github.io/SeTok-web/.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-32.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shengqiong Wu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-28.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXtLgOhJPcWaPKTtGm7D3lK6tc7EhPQGXleGMHMqYG_KFVTxCSBGOd8z6xovad6UMgDjTWPBFfKqD4J2gSD6L6YXpSaTlGNNrLWiViAlfPkKinc9jNjsD2Ulnrh0tZQ74RR62tvQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Los métodos tradicionales de tokenización de visión en los LLMs multimodales fragmentan la entrada visual utilizando parches fijos, corrompiendo la integridad semántica y conduciendo a una alineación visión-lenguaje deficiente. SeTok (Semantic-Equivalent Vision Tokenizer) aborda esto a través de la agrupación dinámica que agrupa las características visuales en unidades semánticas coherentes, con un recuento de tokens que se adapta a la complejidad de la imagen. El sistema utiliza objetivos de entrenamiento duales: pérdida contrastiva para la alineación semántica con el lenguaje y pérdida de reconstrucción para preservar los detalles a nivel de píxel para la reconstrucción de la imagen.</p><p><strong>Conclusiones clave:</strong> </p><ul><li>La tokenización de parche fijo interrumpe la integridad semántica visual al fragmentar los objetos a través de límites de parche arbitrarios</li><li>Los algoritmos de agrupación dinámica pueden determinar adaptativamente los recuentos de tokens óptimos en función de la complejidad semántica de la imagen en lugar de las estructuras de cuadrícula fija</li><li>El entrenamiento de objetivos duales equilibra la alineación semántica con el lenguaje al tiempo que preserva suficientes detalles visuales para las tareas de reconstrucción</li></ul><h2 id="hymba-a-hybrid-head-architecture-for-small-language-models" style="position: relative;"><a href="#hymba-a-hybrid-head-architecture-for-small-language-models" title="Hymba: una arquitectura de cabeza híbrida para modelos de lenguaje pequeños" id="anchor-hymba-a-hybrid-head-architecture-for-small-language-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Hymba: una arquitectura de cabeza híbrida para modelos de lenguaje pequeños</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.13676"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hymba: una arquitectura de cabeza híbrida para modelos de lenguaje pequeños</div><div class="kg-bookmark-description">Proponemos Hymba, una familia de modelos de lenguaje pequeños que presenta una arquitectura paralela de cabeza híbrida que integra los mecanismos de atención del transformador con los modelos de espacio de estado (SSMs) para mejorar la eficiencia. Los encabezados de atención proporcionan una recuperación de alta resolución, mientras que los encabezados SSM permiten una resumen eficiente del contexto. Además, introducimos meta tokens aprendibles que se anteponen a los prompts, almacenando información crítica y aliviando la carga de "obligado a asistir" asociada con los mecanismos de atención. Este modelo se optimiza aún más mediante la incorporación de intercambio de clave-valor (KV) entre capas y atención de ventana deslizante parcial, lo que resulta en un tamaño de caché compacto. Durante el desarrollo, realizamos un estudio controlado que comparaba varias arquitecturas en configuraciones idénticas y observamos ventajas significativas de nuestra arquitectura propuesta. En particular, Hymba logra resultados de vanguardia para los LMs pequeños: Nuestro modelo Hymba-1.5B-Base supera a todos los modelos públicos sub-2B en rendimiento e incluso supera a Llama-3.2-3B con un 1,32% más de precisión promedio, una reducción del tamaño de la caché de 11,67x y un rendimiento de 3,49x.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-33.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Dong</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-29.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqTXEnWvnqhnbdUKsRw-mZ8hzuqwIbX_dnFwiY4BHa8DF4ViWiekeIRVlRBtQkJF8a2EPv5U_H5kvqxFQfCg0jGplWefzce1RHzHBd17D93k6DpE3vNurR0Ufg7kMEJ_C4IeDBZQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="552" height="573" style="cursor: help;"></figure><p>Hymba introduce una arquitectura de cabeza híbrida que combina mecanismos de atención de transformador con modelos de espacio de estado (SSMs) en paralelo dentro de cada capa, lo que permite la recuperación simultánea de alta resolución y la resumen eficiente del contexto. La arquitectura incorpora meta tokens aprendibles, intercambio de clave-valor entre capas y atención de ventana deslizante parcial para lograr tamaños de caché compactos. Hymba-1.5B supera a todos los modelos sub-2B y supera a Llama-3.2-3B al tiempo que logra una reducción de caché de 11.67 × y una mejora del rendimiento de 3.49 ×.</p><p><strong>Conclusiones clave:</strong> </p><ul><li>La arquitectura de cabeza híbrida paralela supera el apilamiento secuencial de componentes de atención y SSM al permitir el procesamiento simultáneo de mecanismos complementarios</li><li>Los meta tokens aprendibles actúan como conocimiento mundial comprimido y alivian la carga de "obligado a asistir" de los mecanismos de atención softmax</li><li>Las optimizaciones de intercambio de clave-valor entre capas y atención de ventana deslizante logran reducciones dramáticas del tamaño de la caché sin sacrificar el rendimiento</li></ul></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categorías:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Evento</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fes%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Oficinas</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Sunnyvale, California</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, EE. UU.</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Berlín, Alemania (sede central)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlín, Alemania</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Beijing, China</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Piso 5, Edificio 6, No.48 Haidian West St. Pekín, China</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Shenzhen, China</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Piso 402, Edificio de Tecnología Fu'an, Shenzhen, China</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fundación de búsqueda</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Lector</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Incrustaciones</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">reclasificador</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Búsqueda profunda</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Clasificador</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Segmentador</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Documentación API</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Obtener la clave API de Jina</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Límite de velocidad</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Estado de la API</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Compañía</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sobre nosotros</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contactar con ventas</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sala de prensa</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programa de prácticas</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Únete a nosotros</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Descargar logotipo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Términos</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Seguridad</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Términos y condiciones</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Privacidad</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Administrar cookies</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
{"pageProps":{"post":{"title":"Cross-modal Search with Jina","date":"2020-10-02T08:05:45.631Z","slug":"Cross-modal-Search-with-Jina","author":"Joan Fontanals Martínez","content":"<h2>Cross-modal search</h2>\n<p>In this post I will explain how we implemented a search engine in Jina for cross-modal content using the paper <a href=\"https://arxiv.org/abs/1707.05612\">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>.</p>\n<p>The result is an application that allows:</p>\n<ul>\n<li>Searching images, using descriptive captions as input query, or</li>\n<li>Searching text captions, using an image as input query</li>\n</ul>\n<figure>\n        <img src=\"https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif\" alt>\n    <figcaption class=\"figure-caption\">The Finished Application</figcaption>\n</figure>\n<p>The code and instructions to run the application can be found in <a href=\"https://github.com/jina-ai/examples/tree/master/cross-modal-search\">https://github.com/jina-ai/examples/tree/master/cross-modal-search</a></p>\n<h2>Cross-modality</h2>\n<p>First, we need to understand the concept of modality: Given our example, one may think that different modalities correspond to different kinds of data (images and text in this case). However, this is not accurate. For example, one can do cross-modal search by searching images from different points of view, or searching for matching titles for given paragraph text.</p>\n<p>Therefore, one can consider that a modality is related to a given data distribution from which input may come. For this reason, and to have first-class support for cross and multi-modal search, Jina offers modality as an attribute from its Document protobuf definition.</p>\n<p>Now that we are agreed on the concept of modality, we can describe cross-modal search as a set of retrieval applications that try to effectively find relevant documents of modality <em>A</em> by querying with documents from modality <em>B</em>.</p>\n<h2>Semantic Search</h2>\n<p>Compared to keyword-based search, the main requirement for content-based search is the ability to extract a meaningful <strong>semantic</strong> representation of the documents both at index and query time. This implies the projection of documents into a <strong>high dimensional vector embedding space</strong> where distances (or similarities) between these vectors are considered the measure of relevance between queries and indexed documents.</p>\n<p>With current advances in performance of all the Deep Learning methods, even general purpose models (e,g. CNN models trained on ImageNet) can be used to extract meaningful feature vectors <em>(</em><a href=\"https://github.com/jina-ai/examples/tree/master/pokedex-with-bit\"><em>Here Jina uses simple feature vectors from mobilenet pretrained for classification tasks on ImageNet to build a working Pokemon search application</em></a><em>)</em>.</p>\n<p>However, models trained using <strong>Deep Metric Learning</strong> are especially suited for retrieval. In opposition to common classification architectures (usually trained using <strong>Cross-Entropy Loss)</strong>, these deep metric models tend to optimize a <strong>Contrastive Loss</strong> metric which tries to put similar objects close to each other and non-related objects further away.</p>\n<figure>\n        <img src=\"https://miro.medium.com/max/328/1*hW_5S5kSmas__mGlBR9qDg.gif\" alt>\n    <figcaption class=\"figure-caption\">Cross-Entropy Loss</figcaption>\n</figure>\n<p>In contrastive loss, the intention is to minimize the distance for positive pairs <em>(y = 1)</em> and to maximize the distance (with some margin <em>m</em>) when negative pairs <em>(y = 0)</em></p>\n<figure>\n        <img src=\"https://miro.medium.com/max/298/1*7ak-Os2FKJu9JKHFKCwOhw.gif\" alt>\n    <figcaption class=\"figure-caption\">Contrastive Loss</figcaption>\n</figure>\n<h2>Siamese and Triplet Networks</h2>\n<p>Two very common architectures for Deep Metric Learning are Siamese and Triplet Networks. They both share the idea that different sub-networks (which may or may not share weights) receive different inputs at the same time (<strong>positive</strong> and <strong>negative</strong> pairs for Siamese Networks; positive, negative and <strong>anchor</strong> documents for Triplets), and try to project their own feature vectors onto a common latent space where the <strong>contrastive</strong> loss is computed and its error propagated to all the sub-networks.</p>\n<p>Positive pairs are pairs of objects (images, text, any document) that are semantically related and expected to remain close in the projection space. On the other hand, negative pairs are pairs of documents that should be apart.</p>\n<figure>\n        <img src=\"https://miro.medium.com/max/945/1*84InYr1UPrVgg6uyMjA3wQ.png\" alt>\n    <figcaption class=\"figure-caption\">Schema of the deep metric learning process with a triplet network and anchor. Image taken from https://gombru.github.io/2019/04/03/ranking_loss/</figcaption>\n</figure>\n<p>In the example, the sub-network used to extract image features is a <strong>VGG19</strong> architecture with weights pre-trained on ImageNet, while for the text embedding, output of a hidden layer from a Gated Recurrent Unit (<strong>GRU</strong>) are used.</p>\n<h2>Hard Negatives</h2>\n<p>Besides all the common tricks and techniques to improve the learning of neural networks, for Deep Metric Learning, a key aspect of performance is the choice of positive and negative pairs. It is important for the model to see negative pairs that are not easy to split, which is achieved using <strong>Hard Negative Mining</strong>. This can impact some evaluation metrics, especially <em>Recall@k</em> with small values of <em>k</em>. Without emphasis on negative pairs the model will be able to extract meaningful neighborhoods but will find it hard to really extract true nearest neighbors, and then underperforming when evaluated at very low <em>ks</em>.</p>\n<figure>\n        <img src=\"https://miro.medium.com/max/948/1*fsGvQnPbK-r2xD2jt8CHzg.png\" alt>\n    <figcaption class=\"figure-caption\">The different types of negatives. Source: https://omoindrot.github.io/triplet-loss</figcaption>\n</figure>\n<p>The paper in which the example is based (<a href=\"https://arxiv.org/abs/1707.05612\">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a>) proposes an advanced hard negative mining strategy that increases the probability of sampling hard negatives at training time, thus obtaining a significant boost on <em>R@1</em> for both image-to-caption and caption-to-image retrieval.</p>\n<h2><strong>The Search Flow in Jina</strong></h2>\n<p>Jina is a useful choice for this implementation. It is a framework for developing semantic search applications with first-class support for cross-modality. Plus, it makes it easy to plug in your own models and to distribute them with the use of Docker containers, which leads to a very smooth development experience and reduces the boilerplate of complex dependency management.</p>\n<p>It allows the description of complex AI-powered search pipelines from simple YAML Flow descriptions: In this example, two Flows are created, one for indexing images and captions and another one for querying.</p>\n<p>At index time, images are pre-processed and normalized before being embedded in a vector space. In parallel, images are indexed without any crafting into a Key-Value database so that the user can retrieve and render them.</p>\n<p>On the other branch of the Flow, text does not require any preprocessing before encoding (vocabulary lookup and word embedding are done during encoding), so the text indexer takes care of both vector and key-value indexing.</p>\n<figure>\n\t<img src=\"https://miro.medium.com/max/948/1*2Uram5hzmTmW7dcU4uDOoQ.png\">\n\t<figcaption class=\"figure-caption\">Jina Index flow for Cross modal search visualization on dashboard</figcaption>\n</figure>\n<p>Query time is where the “cross” in cross-modality shines, the key aspect of the design of the Flow is that the branch responsible for obtaining semantic embeddings for images is connected to the text embedding index and vice-versa. This way, images are extracted by providing text as input and captions are retrieved by providing input images.</p>\n<figure>\n\t<img src=\"https://miro.medium.com/max/948/1*L0LMehwNU6CzeFAu4mwPkQ.png\">\n\t<figcaption class=\"figure-caption\">Jina Query Flow for Cross modal search visualization in Dashboard</figcaption>\n</figure>\n<p>In both cases, there are two branches of the Flow: One will process images, and the other text. This is controlled by a filter applied at the beginning of each branch to select which inputs can be processed.</p>\n<p>Filter modality in Flow:</p>\n<pre><code class=\"language-yaml\">- !FilterQL\n   with:\n     lookups: {'modality': 'image'}\n</code></pre>\n<h2>Plug the Visual Semantic Embedding Models in Jina</h2>\n<p>As stated, Jina makes it easy to plug in different models, and turns out to be a very suitable tool to transfer this research into a real-world search application.</p>\n<p>In order to use the model resulting from the papers’ model, two different encoders <a href=\"https://docs.jina.ai/chapters/101/.sphinx.html#executor\">executors</a> (called <strong>VSEImageEmbedding</strong> and <strong>VSETextEmbedding</strong>) were developed. Each of them just use a specific branch of the original common embedding network.</p>\n<p>Since they rely on pickled weights and models, the main challenge is getting the right models and vocabulary files to load the right models. All this boilerplate is abstracted from the user by building the <strong>Docker</strong> images that will deploy these models very easily.</p>\n<h2>Results</h2>\n<p>The example has been run with <em>Flickr8k</em> dataset with good results, although the models have been trained using <em>Flickr30k.</em> This shows the ability of the model to generalize to unseen data, and the ability to work on general-purpose datasets. These models can be easily retrained and fine-tuned for specific use-cases scenarios.</p>\n<p>The results are shown using <a href=\"https://github.com/jina-ai/jinabox.js\">jinabox</a>, which allows to interact with jina directly from the browser and inputing multiple kinds of data.</p>\n<p><img src=\"https://miro.medium.com/max/948/1*T4zHySDQhPiPlubRGkxb0w.gif\" alt=\"\"></p>\n<h2><strong>References</strong></h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1707.05612\">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</a></li>\n<li><a href=\"https://www.kaggle.com/shadabhussain/flickr8k\">Flickr8K</a></li>\n<li><a href=\"https://github.com/jina-ai/jina\">Jina</a></li>\n<li><a href=\"https://github.com/jina-ai/examples\">Jina examples</a></li>\n<li><a href=\"https://github.com/jina-ai/jinabox.js\">Jinabox.js</a></li>\n</ul>\n<p>By <a href=\"https://www.linkedin.com/in/joanfontanalsmartinez/\">Joan Fontanals Martínez</a> on October 2, 2020.</p>\n","coverImage":"https://miro.medium.com/max/948/1*8c_OGapkhI0uVzuxCLiDWA.png","tags":["cross-modal","VSE++","image&text search"]}},"__N_SSG":true}
{"pageProps":{"post":{"title":"How to build a production-ready Financial Question Answering system with Jina and BERT","date":"2021-01-07T10:00:39.923Z","slug":"financial-qa-tutorial","author":"Bithiah Yuan","content":"<p>Learn how to use the neural search framework, <strong><a href=\"https://github.com/jina-ai/jina\">Jina</a></strong>, to build a <strong>Financial Question Answering (QA) search application</strong>\nusing the <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset, <a href=\"https://pytorch.org\">PyTorch</a>, and <a href=\"https://github.com/huggingface/transformers\">Hugging Face transformers</a>.</p>\n<p>For my master’s thesis, I built a Financial QA system using a fine-tuned BERT model called\n<a href=\"https://github.com/yuanbit/FinBERT-QA\">FinBERT-QA</a>. Motivated by the emerging demand in the financial industry for\nthe automatic analysis of unstructured and structured data at scale, <strong>QA systems can provide lucrative and competitive\nadvantages</strong> to companies by facilitating the decision making of financial advisers.</p>\n<p>The goal of my thesis was to search for a ranked list of relevant answer passages given a question.\nHere is an example of a financial domain-based question and a ground truth answer from the <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset:</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/sample-qa.png\" alt=\"performance\" width=\"450px\">\n<figcaption>Sample QA from the financial domain</figcaption>\n</figure>\n</p>\n<p>Here is a list of other questions from <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a>:</p>\n<pre><code>• What does it mean that stocks are “memoryless”?\n• What would a stock be worth if dividends did not exist?\n• What are the risks of Dividend-yielding stocks?\n• Why do financial institutions charge so much to convert currency?\n• Is there a candlestick pattern that guarantees any kind of future profit?\n• 15 year mortgage vs 30 year paid off in 15\n• Why is it rational to pay out a dividend?\n• Why do companies have a fiscal year different from the calendar year?\n• What should I look at before investing in a start-up?\n• Where do large corporations store their massive amounts of cash?\n</code></pre>\n<p>Financial QA is <strong>hard</strong> because the vocabularies are context specific, for example, a machine would\nhave a hard time understanding what an <em>ETF</em> is. Nevertheless, <strong>with the power of BERT, I improved the state-of-the-art (SOTA) results by an average of 19% on three\nevaluation metrics (Precision, MRR, NDCG)</strong>.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/model-performance.png\" alt=\"performance\" width=\"600px\">\n<figcaption>Evaluation results from <a href=\"https://github.com/yuanbit/FinBERT-QA\">FinBERT-QA</a></figcaption>\n</figure>\n</p>\n<p>Even though my thesis was about QA in the financial domain, the approach that I have used can be applied to a\n<a href=\"https://github.com/microsoft/MSMARCO-Passage-Ranking\">general QA dataset</a> or QA in other domains such as\n<a href=\"https://github.com/shuzi/insuranceQA\">insurance</a>.</p>\n<p>After finishing my thesis, I realized that <strong>just having a model and SOTA results is not good enough</strong> because there\nwas a gap between my research and business needs. This was when I discovered <strong>Jina, a framework designed to\nhelp me bridge this gap</strong>. To help people better understand Jina, I prepared this tutorial to demonstrate how\nI used Jina to easily <strong>transform my research into a production-ready system</strong>.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#background\">Background</a>\n<ul>\n<li><a href=\"#what-is-jina\">What is Jina?</a></li>\n<li><a href=\"#financial-qa-with-bert\">Financial QA with BERT</a></li>\n<li><a href=\"#why-jina\">Why Jina?</a></li>\n</ul>\n</li>\n<li><a href=\"#tutorial\">Tutorial</a>\n<ul>\n<li><a href=\"#set-up\">Set Up</a></li>\n<li><a href=\"#index-flow\">Index Flow</a>\n<ul>\n<li><a href=\"#step-1-define-our-data\">Step 1. Define our data</a></li>\n<li><a href=\"#step-2-encode-answer-passages\">Step 2. Encode Answer Passages</a></li>\n<li><a href=\"#step-3-indexing\">Step 3. Indexing</a></li>\n<li><a href=\"#build-an-indexer-application\">Build an Indexer Application</a></li>\n</ul>\n</li>\n<li><a href=\"#query-flow\">Query Flow</a>\n<ul>\n<li><a href=\"#step-1-encode-question\">Step 1. Encode Question</a></li>\n<li><a href=\"#step-2-search-indexes\">Step 2. Seach Indexes</a></li>\n<li><a href=\"#step-3-reranking\">Step 3. Reranking</a>\n<ul>\n<li><a href=\"#build-a-custom-executor\">Build a Custom Executor</a></li>\n</ul>\n</li>\n<li><a href=\"#build-a-search-application\">Build a Search Application</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#summary\">Summary</a></li>\n<li><a href=\"#next-steps-evaluation\">Next Steps: Evaluation</a></li>\n<li><a href=\"#learn-more\">Learn More</a></li>\n</ul>\n<h2>Background</h2>\n<h3>What is Jina?</h3>\n<p align=\"center\">\n<img src=\"/assets/images/jina_banner_new.png\" width=\"500\">\n</p>\n<p>Open-source deep learning frameworks such as TensorFlow and PyTorch provide building blocks for designing and quickly implementing\nneural network-based applications through a high level programming interface.</p>\n<p>Similarly, <strong>Jina</strong> is an <strong>open-source neural search framework</strong> that offers the building blocks for designing and implementing <strong>neural network-based\nsearch applications</strong>.</p>\n<p>Co-founded by the creator of <a href=\"https://github.com/hanxiao/bert-as-service\">bert-as-service</a> and\n<a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST</a>, Jina enables developers to build\n<strong>production-ready cloud-native search systems</strong> using\n<strong>SOTA pre-trained deep learning models</strong>, in which each component of the system is a <strong>microservice</strong> that can be deployed, scaled, and maintained independently.</p>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/cloud.png\" width=\"350\">\n</p>\n<p>If you come from a data science or academic background like me, the terms <strong>cloud-native</strong> and <strong>microservices</strong> may sound\ndaunting. That's why we will learn by example in this tutorial and use the NLP task, Financial QA, to familiarize ourselves\nwith Jina's core concepts!</p>\n<h3>Financial QA with BERT</h3>\n<p>Before we jump into the tutorial, let's first understand how to build a QA system with BERT. Our goal is to\nsearch for the top-k most relevant answer passages when given a question from task 2 of the <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset.</p>\n<p>In 2018, Google's pre-trained BERT models, used for transfer learning, shook the NLP world and achieved the SOTA\nresults on numerous tasks, marking NLP's <a href=\"https://ruder.io/nlp-imagenet/\">ImageNet moment</a>.</p>\n<p>What is neat about BERT is that we can fine-tune a pre-trained BERT model on our QA task by simply transforming\nit into a <strong>binary classification task</strong>, where the input is the <strong>concatenation of a question and an answer</strong> and the\noutput is a binary label indicating the <strong>relevancy score</strong> of the QA pair. We can then take the softmax scores of\neach QA pair to get a probability of relevancy and rank these scores.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/BERT-QA.png\" width=\"400\">\n<figcaption>Fine-tuning method for our QA task</figcaption>\n</figure>\n</p>\n<p>The <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset has roughly 6,000 questions and 57,000 answers. Instead of\ncomputing a probability for each question 57,000 times, we can adapt a <a href=\"https://arxiv.org/pdf/1901.04085.pdf\">passage reranking</a>\napproach. We first use a <strong>Retriever</strong> to return the top-50 candidate answers for each question,\nand then use <strong>FinBERT-QA</strong>, a BERT-based model fine-tuned on the <a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset\nas a <strong>Reranker</strong> to compute the relevancy scores and rerank the top-50 QA pairs to get the top-10 answers.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/qa-pipeline-simple.png\" width=\"1000\">\n<figcaption>QA pipeline with reranking</figcaption>\n</figure>\n</p>\n<p>If you are interested in the details of my thesis, you can learn more <a href=\"https://github.com/yuanbit/FinBERT-QA\">here</a>.</p>\n<h2>Why Jina?</h2>\n<p>Why is having the SOTA model and results is not good enough?</p>\n<h3>Jina as a bridge between research and industry</h3>\n<p>The motivation behind my research was to be able to help financial advisor answer questions\nfrom large-scale reports. However, the way I implemented the QA pipeline is not reusable and it won't scale\nto business demands. <strong>By industry standards, it is not production-ready.</strong></p>\n<p>Since <strong>Jina enables us to build cloud-native systems</strong>, which embrace\nmicroservices, instead of wrapping my entire pipeline in a single Docker container, Jina will break down the pipeline\ninto components (preprocessor, encoder, indexer, etc.). Moreover, each of these components will be a microservice in its own isolated Docker\ncontainer managed by the <a href=\"https://docs.jina.ai/chapters/flow/index.html\">Flow API</a>.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/cloud2.svg\" width=\"350\">\n<figcaption>Illustration from <a href=\"https://www.manypixels.co/gallery/\">manypixels</a></figcaption>\n</figure>\n</p>\n<p>For those of you new to cloud-native concepts, you can think of a microservice as an independent component of your\napplication, for example, using FinBERT-QA to encode our questions and answers. You can then create multiple independent components\nor microservices to construct an application like a BERT-powered QA system. Since each of the components of the application can be deployed independently,\nthey can also scale individually and respond to rapid changes and business needs.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/business.svg\" width=\"350\">\n<figcaption>Illustration from <a href=\"https://www.manypixels.co/gallery/\">manypixels</a></figcaption>\n</figure>\n</p>\n<p>Being cloud-native is a modern design that more and more businesses are adapting to because it can help them save resources\nand grow. However, designing such systems is not easy. We need to consider many principles, patterns and best practices,\nfor example, how will the each component communicate with each other? How can they work in parallel? Luckily, instead of starting\nfrom scratch, <strong>Jina does all the hard work for us by providing us the building blocks so that we can easily construct\na cloud-native BERT-powered QA system using an reranking approach that is ready to serve in production!</strong></p>\n<h2>Tutorial</h2>\n<p>Now that we have an overview, let's learn how to build a production-ready Financial QA system using the reranking approach and\ndive deeper into some new Jina terminologies. We will use <a href=\"https://github.com/ProsusAI/finBERT\">FinBERT</a> to encode our\nquestions and answer passages into embeddings and <a href=\"https://github.com/yuanbit/FinBERT-QA\">FinBERT-QA</a> to rerank\nthe top-50 answer matches.</p>\n<p><strong>The final code of this tutorial can be found <a href=\"https://github.com/yuanbit/jina-financial-qa-search\">here</a>.</strong></p>\n<h3>Set up</h3>\n<p><strong>Clone the repository</strong> that we will be working together with here:</p>\n<pre><code>git clone https://github.com/yuanbit/jina-financial-qa-search-template.git\n</code></pre>\n<p>We will use <code>financial-qa-search/</code> as our working directory.</p>\n<p><strong>Install the requirements</strong></p>\n<pre><code>pip install -r requirements.txt\n</code></pre>\n<p><strong>Download data and model</strong></p>\n<pre><code>bash get_data.sh\n</code></pre>\n<p>For this tutorial, we won't be searching through all 57,000 answer passages from the\n<a href=\"https://sites.google.com/view/fiqa/home\">FiQA</a> dataset. We will work with a sample dataset called <code>test_answers.csv</code>,\ncontaining about 800 answer passages. If you want to experiment with the full dataset, you can use <code>answer_collection.tsv</code>.</p>\n<p><strong>Flows</strong></p>\n<p>In Jina, we will build a Financial QA system with two pipelines, one for indexing our answer passages and\nthe other for querying. These pipelines are called <strong>Flows</strong>, which also serve to manage the state and context\nof the microservices as well as orchestrating them. Let's see what an overview of\nthe <strong>Index Flow</strong> and <strong>Query Flow</strong>, you can click on the images to see the details:</p>\n<p align=\"center\">\n<figure>\n<a href=\"https://raw.githubusercontent.com/yuanbit/financial-qa-search/master/img/index-flow.png\">\n<img src=\"/assets/images/blog/financial-qa/index-flow.png\" width=\"1000\">\n</a>\n<figcaption>Index Flow</figcaption>\n</figure>\n</p>\n<p align=\"center\">\n<figure>\n<a href=\"https://raw.githubusercontent.com/yuanbit/financial-qa-search/master/img/query-flow.png\">\n<img src=\"/assets/images/blog/financial-qa/query-flow.png\" width=\"1000\">\n</a>\n<figcaption>Query Flow</figcaption>\n</figure>\n</p>\n<p>To understand these Flows, let's start with the <strong>Index Flow</strong> and look into the individual components one by one.</p>\n<h3>Index Flow</h3>\n<p>The main idea behind the Index Flow is to use a pre-trained BERT model to encode all of our answer\npassages into embeddings then indexing these embeddings so that they can be searched in the Query Flow.</p>\n<h4>Step 1. Define our data</h4>\n<p>We want to index a subset of  the answer passages from the FiQA dataset, <code>dataset/test_answers.csv</code>:</p>\n<pre><code>398960\tFrom  http://financial-dictionary.thefreedictionary.com/Business+Fundamentals  The  facts  that  affect  a  company's      underlying  value.  Examples  of  business      fundamentals  include  debt,  cash  flow,      supply  of  and  demand  for  the  company's      products,  and  so  forth.  For  instance,      if  a  company  does  not  have  a      sufficient  supply  of  products,  it  will      fail.  Likewise,  demand  for  the  product      must  remain  at  a  certain  level  in      order  for  it  to  be  successful.  Strong      business  fundamentals  are  considered      essential  for  long-term  success  and      stability.  See  also:  Value  Investing,      Fundamental  Analysis.  For  a  stock  the  basic  fundamentals  are  the  second  column  of  numbers  you  see  on  the  google  finance  summary  page,    P/E  ratio,  div/yeild,  EPS,  shares,  beta.      For  the  company  itself  it's  generally  the  stuff  on  the  'financials'  link    (e.g.  things  in  the  quarterly  and  annual  report,    debt,  liabilities,  assets,  earnings,  profit  etc.\n19183\tIf  your  sole  proprietorship  losses  exceed  all  other  sources  of  taxable  income,  then  you  have  what's  called  a  Net  Operating  Loss  (NOL).  You  will  have  the  option  to  \"carry  back\"  and  amend  a  return  you  filed  in  the  last  2  years  where  you  owed  tax,  or  you  can  \"carry  forward\"  the  losses  and  decrease  your  taxes  in  a  future  year,  up  to  20  years  in  the  future.  For  more  information  see  the  IRS  links  for  NOL.  Note:  it's  important  to  make  sure  you  file  the  NOL  correctly  so  I'd  advise  speaking  with  an  accountant.  (Especially  if  the  loss  is  greater  than  the  cost  of  the  accountant...)\n327002\tTo  be  deductible,  a  business  expense  must  be  both  ordinary  and  necessary.  An  ordinary  expense  is  one  that  is  common  and  accepted  in  your  trade  or  business.  A  necessary  expense  is  one  that  is  helpful  and  appropriate  for  your  trade  or  business.  An  expense  does  not  have  to  be  indispensable  to  be  considered  necessary.    (IRS,  Deducting  Business  Expenses)  It  seems  to  me  you'd  have  a  hard  time  convincing  an  auditor  that  this  is  the  case.    Since  business  don't  commonly  own  cars  for  the  sole  purpose  of  housing  $25  computers,  you'd  have  trouble  with  the  \"ordinary\"  test.    And  since  there  are  lots  of  other  ways  to  house  a  computer  other  than  a  car,  \"necessary\"  seems  problematic  also.\n</code></pre>\n<p>Our dataset consists of a column of answer id and text, which we will denote as docid and doc respectively in this tutorial.\nIn order to index our data, we need to first define it in a Jina data type called <strong>Document</strong>.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-step1.png\" width=\"400\">\n<figcaption>Index Flow - Step 1</figcaption>\n</figure>\n</p>\n<p>In programming languages there are data types such as int, float, boolean, and more. In NumPy, TensorFlow, and PyTorch,\nwe manipulate and pass around objects such as ndarray and tensor, which are referred to as <strong>primitive data types</strong>.\nSimilarly, a <strong>Document</strong> is a Jina-specific data type for representing data.</p>\n<p><strong>Defining our data in a Document</strong></p>\n<p>In our project directory <code>financial-qa-search/</code> the <code>app.py</code> file consists of the Financial QA search application\nthat we will build.  Notice that we set our data path in the <code>config</code> function as follows:</p>\n<script src=\"https://gist.github.com/yuanbit/f8bc065dbba8382b1dadd0c75203cf9e.js\"></script>\n<p>You can change the path to <code>answer_collection.tsv</code> to index with the full dataset.</p>\n<p>Let's first make sure we import <code>Document</code> from jina:</p>\n<script src=\"https://gist.github.com/yuanbit/f0f5d0a43738f50140e47fa2af6e8fcc.js\"></script>\n<p>After the <code>config</code> function, let's create a Python generator and define the Document to contain the id and text\ncorresponding to the answer passages:</p>\n<script src=\"https://gist.github.com/yuanbit/91effc3288862727199a13813f5c33a3.js\"></script>\n<p>A Document is a high-level way for us to <strong>define and view the contents stored in Protobuf</strong>, which is what Jina uses to\n<strong>enable the microservices in the Flow to communicate with each other</strong>. It is like an envelope\ncontaining our data and is used to send messages between the microservices of our Flow. Instead of directly\ndealing with Protobuf, which serializes our data into bytes, we can simply print our Document and see that a single answer\npassage will look as follows:</p>\n<pre><code>id: \"13755c6081bebe1a\"\nmime_type: \"text/plain\"\ntags {\n  fields {\n    key: \"id\"\n    value {\n      number_value: 398960.0\n    }\n  }\n}\ntext: \"From  http://financial-dictionary.thefreedictionary.com/Business+Fundamentals  The  facts  that  affect  a  \ncompany\\'s underlying  value. Examples  of  business fundamentals  include  debt,  cash  flow, supply  of  and  demand  \nfor  the  company\\'s      products,  and  so  forth.  For  instance, if  a  company  does  not  have  a sufficient  \nsupply  of  products,  it  will      fail.  Likewise,  demand  for  the  product      must  remain  at  a  certain  \nlevel  in      order  for  it  to  be  successful.  Strong      business  fundamentals  are  considered essential  for  \nlong-term  success  and      stability.  See  also:  Value  Investing, Fundamental  Analysis.  For  a  stock  the  basic\nfundamentals  are  the  second  column  of  numbers  you  see  on  the  google  finance  summary  page, P/E  ratio,  \ndiv/yeild,  EPS,  shares,  beta.      For  the  company  itself  it\\'s  generally  the  stuff  on  the  \\'financials\\'  \nlink    (e.g.  things  in  the  quarterly  and  annual  report,    debt,  liabilities,  assets,  earnings,  profit  etc.\"\n</code></pre>\n<p><strong>As we move along the Index Flow, the contents of the Document will be changed</strong>, for example, we can see in the Index Flow that\nthe embeddings of the answer passages are added to the Document after the encoding step.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-step2-2.png\" width=\"500\">\n<figcaption>Embeddings of the answer passages are added to the Document after the encoding step1</figcaption>\n</figure>\n</p>\n<p>The encoding step uses an <strong>Executor</strong>, namely the <strong>Encoder</strong>. Let's understand this more next.</p>\n<h4>Step 2. Encode Answer Passages</h4>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Encoder.png\" width=\"300\">\n</p>\n<p>After defining the Document for the <strong>Index Flow</strong>, the next step is to encode the answer text into embeddings\nusing a pre-trained BERT model. The logic that does the encoding is called an <strong>Encoder</strong>, which is part of Jina's family\nof <strong>Executors</strong>.</p>\n<p>We will look at other Executors later and only focus on the Encoder for now. Instead using TensorFlow\nor PyTorch with the combination of Hugging Face transformers and implementing the Encoder ourselves, we can simply take advantage\nof <a href=\"https://github.com/jina-ai/jina-hub\">Jina Hub</a>, an <strong>open-registry for hosting Jina Executors via container images</strong>.</p>\n<p>There are all kinds of Encoders and other types of Executors in Jina Hub for different tasks and data types\n(e.g. image, video, audio, multimodal), allowing us to <strong>ship and exchange reusable component and build various\ndeep learning-based search engines, e.g. text-image, cross-modal, and multi-modal searches.</strong>\nSince our task is a text-to-text search, we will use the\n<a href=\"https://github.com/jina-ai/jina-hub/tree/master/encoders/nlp/TransformerTorchEncoder\">TransformerTorchEncoder</a> for this tutorial.</p>\n<p>Before we talk about how to use the Encoder in our Index Flow, let's understand three more important Jina concepts in this\nstep:</p>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Driver.png\" width=\"300\">\n</p>\n<ul>\n<li><strong>Driver:</strong> Recall Jina uses Protobuf to send messages between the microservices in the Flow, which are in the form of bytes.\nWe would have a problem if we were to pass the Document directly to the Encoder because the Encoder needs the\nanswer text as input instead of bytes. Instead of dealing directly with Protobuf, Jina uses <strong>Drivers</strong> to <strong>translate\ndata for an Executor</strong>, so that we only need to work with data types that we are familiar with (e.g. text, image, np,array, etc...).\n<strong>The Driver interprets messages in the Flow and passes the appropriate data to the Executor.</strong></li>\n</ul>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-step2.png\" width=\"550\">\n<figcaption>Encoder - the Driver receives the Document in byes and passes the text to the Encoder. The Encoder outputs the embeddings of the text and the Driver adds them to the Document.</figcaption>\n</figure>\n</p>\nFor example, in the encoding step, the Driver receives the Document in bytes, interprets it as a Document, and passes the text in the Document to the Encoder.\nAfter the Encoder outputs the embeddings for the corresponding text, the Driver \nagain interprets the embeddings, and adds them to the Document. The Document below shows how it has be transformed by\nthe Driver in the encoding step and will serve as the input for the next indexing step.\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-step2-2.png\" width=\"500\">\n<figcaption>The Driver transformed the Document by adding the embeddings in the encoding step.</figcaption>\n</figure>\n</p>\n<ul>\n<li><strong>Pea:</strong> Since an Executor needs a Driver to be able to process our data, they are both necessary components of a\nmicroservice in the Flow. Therefore, we use a <strong>Pea</strong> to wrap the Executor and Driver together to get our <strong>Encoder Microservice</strong>.\nThe Pea is, therefore, a <strong>microservice</strong> that constantly listens for incoming messages from the gateway or other Peas in\nthe Flow and calls the Driver when it receives a message. As a microservice, <strong>Peas can also run in Docker, containing all dependencies and context in one place.</strong></li>\n</ul>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Pea.png\" width=\"300\">\n</p>\n<ul>\n<li>\n<p><strong>Pod:</strong> To optimize our neural search application, <strong>Jina provides parallelization out of the box.</strong> Instead of having a\nsingle Encoder, we can split it into <strong>multiple processes</strong>. The visualization of the Encoding step shows the Encoder being split into three\nprocesses with each process wrapped by a Pea.</p>\n<p><strong>In order for our multiple Encoder microservices to behave functionally as one Encoder, we wrap the group of homogeneous (identical) Peas in a Pod</strong>.\nThe Pod is, therefore, a <strong>group of homogeneous microservices</strong> that is also responsible for load balancing, further control and context management.\nThe beauty about this design is that a Pod can either run on the local host or on different computers over a network,\nmaking our application <strong>distributed, efficient, and scalable</strong>.</p>\n</li>\n</ul>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Pod.png\" width=\"300\">\n</p>\n<p>Now that we understand these foundational concepts, <strong>how do we create a Pod for the Encoder?</strong></p>\n<p>It may all sound extremely\ncomplicated, but with the building blocks provided by Jina we can <strong>(1) design an Index Flow and (2) create an Encoder\nPod with two simple YAML files.</strong> These YAML files will allow us to customize our neural search application without\ntouching the core of Jina's code.</p>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/YAML.png\" width=\"300\">\n</p>\n<p><strong>I. Create a Pod for the Encoder</strong></p>\n<p>Let's first create a file <code>encode.yml</code> inside the folder called <code>pods</code>. In <code>encode.yml</code>\nwe first specify the name of the Encoder we want to use from Jina Hub <code>TransformerTorchEncoder</code>. We can\nchoose the model we want to use, in our case we use <a href=\"https://github.com/ProsusAI/finBERT\">FinBERT</a>, which further\npre-trained <code>bert-base-uncased</code> on a large financial corpus. Since <code>TransformerTorchEncoder</code> was implemented\nusing Hugging Face transformers, you can also directly use the model by specifying its name if it is available on\nthe <a href=\"https://huggingface.co/docs\">Hugging Face Model Hub</a>. We can also include other hyperparameters such as the\nmaximum sequence length or pooling strategy.</p>\n<script src=\"https://gist.github.com/yuanbit/2890f74af6ae9d15bc316cc9ef4ec8aa.js\"></script>\n<p>Simple as that! 🐣 <strong>We just created a deep learning-based Encoder microservice ready to be parallelized!</strong>\nThe <code>pods</code> folder will also be the home to other Pods that we will need, which will\nalso be defined using YAML files.</p>\n<p><strong>II. Add the Encoder to the Index Flow</strong></p>\n<p>Now that we have our Encoder ready, let's put it in our Index Flow. Let's create a file <code>index.yml</code> inside a\nfolder called <code>flows</code>. In <code>index.yml</code>, we specify our first Pod in the Index Flow which is the <code>encoder</code> by\ngiving the path to our <code>pods/encode.yml</code> file. We can specify how many processes we want to split the Encoder into\nby using the <code>parallel</code> parameter. This will be an environment variable specified in <code>app.py</code>, which we will\nlook at in the end. <code>parallel</code> also <strong>determines how many Peas we will have in each Pod.</strong></p>\n<script src=\"https://gist.github.com/yuanbit/e7cbcdcc83c0f03d0582b9cc58b18294.js\"></script>\n<p>Well done! 💪 You've just created a pipeline for deep learning-powered microservices! Next, let's finish the design of the Index\nFlow by adding another Pod containing the Indexer.</p>\n<h4>Step 3. Indexing</h4>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Indexer.png\" width=\"300\">\n</p>\n<p>After obtaining the embeddings for the answer passages, we will create another Executor called the <strong>Indexer</strong>\nto store our data so that they can be retrieved in query time. Similar to the previous step, the Driver receives the\nDocument and passes the <code>docid</code>, <code>doc</code>, and <code>embeddings</code> to the Indexer.</p>\n<p>We will use the <strong>Compound Indexer</strong>,\nwhich acts as a single indexer using both the <strong>(1) Vector</strong> and <strong>(2) Key-Value Indexers</strong> from Jina Hub:</p>\n<ol>\n<li>\n<p><strong>Vector Indexer:</strong> Stores the answer embeddings and is queried by the question embedding to retrieve\nthe closest answer embeddings using the k-nearest neighbors algorithm.</p>\n</li>\n<li>\n<p><strong>Key-Value (KV) Indexer:</strong> Stores the Document data (text, blob, metadata) and is queried by the Document id (normally\nextracted from the Vector Indexer) to retrieve the information of the data such as answer id and text.</p>\n</li>\n</ol>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-step3.png\" width=\"600\">\n<figcaption>The Indexer will store our data so that they can be retrieved in query time</figcaption>\n</figure>\n</p>\n<p>We again wrap the Driver and Indexer in a Pea, group identical Peas in a Pod, and define them using YAML files.</p>\n<p><strong>I. Create a Pod for the Indexer</strong></p>\n<p>Let's create the file <code>pods/doc.yml</code> and define our compound indexer as <code>!CompoundIndexer</code> with the components\n<code>!NumpyIndexer</code> which is the Vector Indexer and <code>!BinaryPbIndexer</code> which is the KV Indexer. The indexed data\nwill be stored in <code>vec.gz</code> and <code>doc.gz</code> respectively. The <code>workspace</code>\nis the directory where the indexes will be stored, which will be inside our working directory.</p>\n<script src=\"https://gist.github.com/yuanbit/6538a696b012e565d45f960b400e396f.js\"></script>\n<p><strong>II. Add the Indexer to the Index Flow</strong></p>\n<p>Now let's go back to <code>flows/index.yml</code> and add our Indexer to the Index Flow as <code>doc_indexer</code>. If our data is\nbig, we can also add sharding to our application for optimization. This will also be used as an environment variable\nin <code>app.py</code>, which we will see later.</p>\n<script src=\"https://gist.github.com/yuanbit/ba5310ace80201c69e6cff498a1f3905.js\"></script>\n<p>Great job! 👏 You have just designed a cloud-native pipeline for indexing financial answer passages! We can\nalso use Jina's <a href=\"https://docs.jina.ai/chapters/flow/index.html\">Flow API</a> to\nvisualize the Index Flow. First let's set our environment variables in the terminal for parallel and shards:</p>\n<pre><code>export JINA_PARALLEL='1'\nexport JINA_SHARDS='1'\n</code></pre>\n<p>Next, let's open a <code>jupyter notebook</code> in our working directory and do the following:</p>\n<script src=\"https://gist.github.com/yuanbit/66d4df6a197e107b9be6b4ef495c7fa2.js\"></script>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/index-flow-jina.png\" width=\"1000\">\n<figurecaption>Index Flow visualzation</figurecaption>\n</figure>\n</p>\n<p>Here we see our Index Flow with two Pods - the Encoder, <code>encoder</code> and Indexer, <code>doc_indexer</code>.</p>\n<h4>Build an Indexer Application</h4>\n<p>Let's see how we can use the Index Flow as our application. In <code>app.py</code>, we can change\n<code>parallel</code> in the <code>config</code> function to indicate how many Peas (processes)\nwe want to split each microservice in for each Pod. We can also change <code>shards</code>\nto indicate parallelization during the indexing step. We will leave both of them unchanged for now.\nThis means that we will only have one Pea in each Pod.</p>\n<p>Let's first import <code>Flow</code> from Jina's <a href=\"https://docs.jina.ai/chapters/flow/index.html\">Flow API</a>:</p>\n<script src=\"https://gist.github.com/yuanbit/752eef2f8355d5243202c00949dbfcb0.js\"></script>\n<p>After the <code>index_generator</code> function that we added in <a href=\"#step-1-define-our-data\">Step 1. Define our data</a>,\nlet's add the <code>index</code> function which will first load the Index Flow that we have created in <code>flows/index.yml</code>\nand pass the input Document from <code>index_generator</code> to the flow. We set our <code>batch_size=16</code> for encoding\nthe answer passages into embeddings.</p>\n<script src=\"https://gist.github.com/yuanbit/c2ffa20fbce1a7ce8fc55597d32723e3.js\"></script>\n<p>We are now ready to index our data. In the our working directory run:</p>\n<pre><code>python app.py index\n</code></pre>\n<p><a href=\"https://asciinema.org/a/RDQvKPeibRRURYtpQWqmkCUsH\"><img src=\"https://asciinema.org/a/RDQvKPeibRRURYtpQWqmkCUsH.svg\" alt=\"asciicast\"></a></p>\n<p>At the end you will see the following:</p>\n<pre><code>✅ done in ⏱ 1 minute and 54 seconds 🐎 7.7/s\n        gateway@18904[S]:terminated\n    doc_indexer@18903[I]:recv ControlRequest from ctl▸doc_indexer▸⚐\n    doc_indexer@18903[I]:Terminating loop requested by terminate signal RequestLoopEnd()\n    doc_indexer@18903[I]:#sent: 56 #recv: 56 sent_size: 1.7 MB recv_size: 1.7 MB\n    doc_indexer@18903[I]:request loop ended, tearing down ...\n    doc_indexer@18903[I]:indexer size: 865 physical size: 3.1 MB\n    doc_indexer@18903[S]:artifacts of this executor (vecidx) is persisted to ./workspace/doc_compound_indexer-0/vecidx.bin\n    doc_indexer@18903[I]:indexer size: 865 physical size: 3.2 MB\n    doc_indexer@18903[S]:artifacts of this executor (docidx) is persisted to ./workspace/doc_compound_indexer-0/docidx.bin\n</code></pre>\n<p>Hooray 🙌 we finished the first part of our application! The embedding indexes and Document data will be stored in a directory called <code>workspace</code>.</p>\n<h3>Query Flow</h3>\n<p>After indexing our data, we need to create a <strong>Query Flow</strong>. The main idea behind the Query Flow is to use the\nsame BERT-based model to <strong>encode a given question into an embedding</strong> and use the Indexer to <strong>search for the most\nsimilar answer embeddings</strong>. To further improve the search results, we will use the same reranking technique as my thesis\nTherefore, we will need to add another a reranking step using FinBERT-QA to recompute the scores of the answer matches returned by Jina.</p>\n<p align=\"center\">\n<figure>\n<a href=\"https://raw.githubusercontent.com/yuanbit/financial-qa-search/master/img/query-flow.png\">\n<img src=\"/assets/images/blog/financial-qa/query-flow.png\" width=\"1000\">\n</a>\n<figurecaption>Query Flow</figurecaption>\n</figure>\n</p>\n<p>Let us again walk through the steps one by one.</p>\n<h4>Step 1. Encode Question</h4>\n<p>Let's assume that the question text will be a user input. Jina will take this input and\ndefine a new Document.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/query-flow-step1.png\" width=\"500\">\n<figurecaption>Encoder in the Query Flow</figurecaption>\n</figure>\n</p>\n<p><strong>I. Add the Encoder to the Query Flow</strong></p>\n<p>Just like the encoding step of the Index Flow, we encode the questions using the same Encoder. Therefore, we can use the same Encoder from <code>pods/encode.yml</code>\nin our <strong>Query Flow</strong>. We will create a new <code>query.yml</code> file in the <code>flows</code> folder and\nadd the Encoder Pod to it:</p>\n<script src=\"https://gist.github.com/yuanbit/768020d68924e16cefeb052602ecf4f2.js\"></script>\n<h4>Step 2. Search Indexes</h4>\n<p>After encoding the questions, the question embeddings will be added to the Document by the Driver. This Document\nis then sent to the Indexer in the next Pod and the Driver will pass the question embeddings to the Indexer.\nThe Indexer will then search for the answers with the most similar embeddings using the k-nearest neighbors algorithm and\npass a list of top-k answer matches to the Driver to be added to the Document.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/query-flow-step2.png\" width=\"500\">\n<figurecaption>The Indexer will search for the answers with the most similar embeddings</figurecaption>\n</figure>\n</p>\n<p>The matches will contain data\nsuch as the <code>docid</code>, <code>doc</code>, and match <code>scores</code>. Since we are also using the same\nIndexer from the Index Flow, all we need to do again is add the Indexer Pod to <code>flows/query.yml</code>:</p>\n<script src=\"https://gist.github.com/yuanbit/f1358c0b825caf2fe406605ec0ae583c.js\"></script>\n<h4>Step 3. Reranking</h4>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/Ranker.png\" width=\"300\">\n</p>\n<p>Let's assume that the Indexer returns the top-k answer matches at this point and we want to recompute the match scores to\nget better results. Jina has a class of Executors called the <strong>Rankers</strong>, in particular, the <strong>Match2DocRankers</strong>\nre-scores the matches for a query by calculating new scores. If you look at the Rankers on Jina Hub, the\n<a href=\"https://github.com/jina-ai/jina-hub/tree/master/rankers/LevenshteinRanker\">Levenshtein Ranker</a> uses the\nLevenshtein distance to recompute the match scores.</p>\n<p>However, instead of using a distance metric to recompute the scores, we want to load our fined-tuned BERT model,\nFinBERT-QA, in the Ranker and recompute the scores by using the concatenation of the question and the\ncurrent match answers as inputs into a binary classification task.</p>\n<p>In order to do this we need to <strong>create our own custom\nExecutor</strong> and implement our own logic. In this section we will use <strong>PyTorch</strong> and <strong>Hugging Face transformers</strong>\nto implement our custom Ranker.</p>\n<p>The main idea here is to pass our query text and the matches (containing the answer text and match scores) to the\nRanker to return a reordered list of matches based on the relevancy scores computed by FinBERT-QA. The Driver will\nthen update the matches in the Document based on this reordered list.</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/query-flow-step3.png\" width=\"550\">\n<figurecaption>The Ranker recomputes the scores of the matches using FinBERT-QA</figurecaption>\n</figure>\n</p>\n<p>Recall that Peas can run in Docker, this means that we can simply <strong>build a Docker image with our implementation\nof the Ranker and use the image in the Query Flow.</strong> The Jina Hub API let's use to Cookiecutter to create the templates of all\nthe files we will need to do this. Let's get started by making sure that the Jina Hub extension is installed:</p>\n<pre><code>pip install \"jina[hub]\"\n</code></pre>\n<h4>Build a Custom Executor</h4>\n<p>Let's first create the templates that we will need to build a Docker image for our custom Ranker.</p>\n<p><strong>1.) Set up.</strong></p>\n<p>In the\n<code>financial-qa-search/</code>\ndirectory type:</p>\n<pre><code>jina hub new\n</code></pre>\n<p>This will pop up a wizard that helps you walk through the process. Let's give our Executor the name <code>FinBertQARanker</code>\nand make sure to select <code>4 - Ranker</code> for the Executor type. We will use <code>jinaai/jina</code> as our base image for\nthe Docker image that we will build.</p>\n<pre><code>You've downloaded /Users/bithiah/.cookiecutters/cookiecutter-jina-hub before. Is it okay to delete and re-download it? [yes]: yes\nexecutor_name [The class name of the executor (UpperCamelCase)]: FinBertQARanker\nSelect executor_type:\n1 - Encoder\n2 - Crafter\n3 - Indexer\n4 - Ranker\n5 - Evaluator\nChoose from 1, 2, 3, 4, 5 [1]: 4\ndescription [What does this executor do?]: recomputes match scores using FinBERT-QA                \nkeywords [keywords to describe the executor, separated by commas]: \npip_requirements []: \nbase_image [jinaai/jina]: \nauthor_name [Jina AI Dev-Team (dev-team@jina.ai)]: \nauthor_url [https://jina.ai]: \nauthor_vendor [Jina AI Limited]: \ndocs_url [https://github.com/jina-ai/jina-hub]: \nversion [0.0.1]: \nlicense [apache-2.0]: \n</code></pre>\n<p>After pressing Enter, you will see a new directory called <code>FinBertQARanker</code>. Your file structure should now look\nas follows:</p>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/file-structure.png\" width=\"650\">\n<figurecaption>Project folder structure</figurecaption>\n</figure>\n</p>\n<p>We will the implement our logic of the Ranker in <code>__init__.py</code>, write some tests in <code>tests/test_finbertqaranker.py</code>, and\nchange the <code>Dockerfile</code> to contain everything we need to build the image.</p>\n<p><strong>The code for the Ranker can be found <a href=\"https://github.com/jina-ai/examples/tree/example-financial-qa-search/financial-qa-search/FinBertQARanker\">here</a>.</strong></p>\n<p><strong>2.) Fill in the logic for reranking.</strong></p>\n<p>We will now implement our logic in <code>__init__.py</code>, which should look like the following:</p>\n<script src=\"https://gist.github.com/yuanbit/f0fb64b29b46f45c59469fe4ef3495b9.js\"></script>\n<p>Jina contains different base classes for the Executors with different functionalities. The base Ranker class\nthat we will use is called <strong>Match2DocRankers</strong>, which has the functionality to recompute the match scores.</p>\n<p>Let's first change the base class of <code>BaseRanker</code> to <code>Match2DocRanker</code>.\nLet's also import <strong>PyTorch</strong> using Jina and some other modules that we will need as well as define our current directory.</p>\n<script src=\"https://gist.github.com/yuanbit/6969968f9d5a6e4a081347ff272edce0.js\"></script>\n<p>Our logic will be implemented in the <code>FinBertQARanker</code> class which will use <code>TorchDevice</code> and\n<code>Match2DocRanker</code> from Jina. We will download the models that we need in the <code>Dockerfile</code> later.\nLet us assume now we have two models in the folder <code>models/</code>: (1) <code>bert-qa/</code> and (2) <code>2_finbert-qa-50_512_16_3e6.pt</code>.</p>\n<p>(1) <code>bert-qa</code>: bert-base-uncased fine-tuned on the MS Macro dataset\nfrom <a href=\"https://github.com/nyu-dl/dl4marco-bert\">Passage Re-ranking with BERT</a></p>\n<p>(2) <code>2_finbert-qa-50_512_16_3e6.pt</code>: FinBERT-QA model - fine-tuned <code>bert-qa</code> on the FiQA dataset.</p>\n<p>We first specify <code>bert-qa/</code> as the the pre-trained model that would be used for initialization,\n<code>2_finbert-qa-50_512_16_3e6.pt</code> as the model that would be used to compute the QA relevancy scores,\nand the maximum sequence length for the QA pairs:</p>\n<script src=\"https://gist.github.com/yuanbit/47faac3565dda220b71a77fe3a120d61.js\"></script>\n<p>Then we add a <code>post_init</code> function to the class to load the models for the binary classification task. Make sure to\nset the model in evaluation mode.</p>\n<script src=\"https://gist.github.com/yuanbit/5db6428baf752471c4326d1d1e650906.js\"></script>\n<p>Now let's implement a private <code>_get_score</code> function to compute each of the relevancy scores of the question\nand the top-k answer matches. We first concatenate the question and each top-k answer and encode them to get the\ninputs (<code>input_ids</code>, <code>token_type_ids</code>, <code>att_mask</code>) that the model needs using the\ntokenizer from transformers. We then feed the inputs into the model and get the prediction scores\nthat the QA pairs are relevant (<code>label = 1</code>). We apply the softmax function to the scores to transform the\nprediction scores into probabilities between 0 and 1. The output would then be the relevancy score in the form of a\nprobability for the QA pair.</p>\n<script src=\"https://gist.github.com/yuanbit/5efb50d17b97f7eb594c7f88ee116c3a.js\"></script>\n<p>Lastly, let's fill in the scoring function that takes the question from the user and Jina's match scores as input\nand uses <code>_get_scores</code> to recompute new scores:</p>\n<script src=\"https://gist.github.com/yuanbit/b2be4165f2151fa69e9c88a21334c898.js\"></script>\n<p><strong>3.) Write a Unit Test</strong></p>\n<p>In order to create a new Executor and build a Docker image with the Jina Hub API, we need to write a unit test. We can\nfind a template for this in <code>tests/test_finbertqaranker.py</code>. I wrote a simple check to compute the relevance\nprobability for two answer matches given a query and to check to see if <code>FinBertQARanker</code> computes the same\nscore as our expectation:</p>\n<script src=\"https://gist.github.com/yuanbit/8ce93efa42f7c56a2a224e7b8c8e86cb.js\"></script>\n<p><strong>4.) Add Requirements</strong></p>\n<p>Other than Jina we are also using PyTorch and transformers for <code>FinBertQARanker</code>, so let's add them\nto <code>FinBertQARanker/requirements.txt</code>:</p>\n<pre><code>torch==1.7.1\ntransformers==4.0.1\n</code></pre>\n<p><strong>5.) Prepare Dockerfile</strong></p>\n<p>Let's change our <code>Dockerfile</code> to the contents below, which will download the models into a folder called <code>models/</code>.</p>\n<script src=\"https://gist.github.com/yuanbit/b226b930c0850b92eee9a25852278388.js\"></script>\n<p><strong>6.) Build Docker image with Jina Hub API</strong></p>\n<p>We are finally ready to build <code>FinBertQARanker</code> into a Docker image. In our working directory, let's type:</p>\n<pre><code>jina hub build FinBertQARanker/ --pull --test-uses --timeout-ready 60000\n</code></pre>\n<p><code>--pull</code> downloads our Jina base image if it is not already local.</p>\n<p><code>--test-uses</code> adds an extra test to check if the built image can dry-run successfully via Jina's Flow API.\n<code>--timeout-ready</code> gives our <code>post_init</code> function time to load the models.</p>\n<p>If the build is successful, you will see this message:</p>\n<pre><code> HubIO@10240[I]:Successfully built ba3fac0f3a46\n HubIO@10240[I]:Successfully tagged jinahub/pod.ranker.finbertqaranker:0.0.1-0.8.13\n HubIO@10240[I]:building FinBertQARanker/ takes 6 minutes and 12 seconds (372.31s)\n HubIO@10240[S]:🎉 built jinahub/pod.ranker.finbertqaranker:0.0.1-0.8.13 (sha256:ba3fac0f3a) uncompressed size: 3.3 GB\n</code></pre>\n<p>Congratulations 🥳, you have successfully built a custom Executor in the form of a Docker image with the tag name\n<code>jinahub/pod.ranker.finbertqaranker:0.0.1-0.8.23</code>! Let's see how we can use it in the Query Flow next.</p>\n<p><strong>I. Create a custom Ranker Pod</strong></p>\n<p>To use our custom Ranker, <code>FinBertQARanker</code>, we need to first create a new Pod for the Ranker. Let's create\nthe file <code>rank.yml</code> in the <code>pods</code> folder. Next, let's copy the contents from <code>FinBertQARanker/config.yml</code>\nto <code>pods/rank.yml</code> and you should have the following:</p>\n<script src=\"https://gist.github.com/yuanbit/24c93873f7594ee0a097ea1550cc1d66.js\"></script>\n<p>This is going to tell the Query Flow to use the logic we have implemented in our Exectuor, <code>FinBertQARanker/__init__.py</code>.\nSince the code for this implementation is loaded inside the <code>workspace</code> folder in the Docker image, let's add\n<code>workspace/</code> before <code>__init__.py</code>.</p>\n<p>The Encoder and Indexer Executors that we have used so far all use default Drivers in the Pods. Since we created our custom\nExecutor, we need to tell the Ranker Pod which Driver to use. In this case we will use the <code>Matches2DocRankDriver</code> for the\n<code>Match2DocRanker</code> base Ranker class. Hence, our <code>rank.yml</code> will look as follows:</p>\n<script src=\"https://gist.github.com/yuanbit/7757c64f850b4dd43c95e56d2f0a2909.js\"></script>\n<p>Hooray 🎊 we now have a custom Ranker Pod! Let's see next how we can use it in the Query Flow.</p>\n<p><strong>II. Use Custom Ranker in the Query Flow</strong></p>\n<p>Like the other Executor Pods, we just need to add <code>ranker</code> after the <code>doc_indexer</code> and tell the Query Flow to use\nthe Docker image and Ranker Pod that we have just created by specifying the prefix <code>docker://</code> in front of the tag name. The final <code>flows/query.yml</code> should look as follows:</p>\n<script src=\"https://gist.github.com/yuanbit/f5e5291f7a30e4c43da63fb9ef537608.js\"></script>\n<p><strong>Be aware that the tag name of the Docker image might change</strong> depending the current Jina release. Make sure to change the\ntag name that accordingly to your build message.</p>\n<p>We can again visualize the Query Flow using the Flow API in a  <code>jupyter notebook</code> as follows:</p>\n<script src=\"https://gist.github.com/yuanbit/13e879fa8de427c7b8be8eadfc995ac6.js\"></script>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/query-flow-jina.png\" width=\"1000\">\n<figurecaption>Query Flow visualization</figurecaption>\n</figure>\n</p>\n<p>Here we see our Query Flow with three Pods containing the Encoder, <code>encoder</code> and Indexer, <code>doc_indexer</code>,\nand Ranker, <code>ranker</code>.\nAt the end of the Query Flow, the Driver from the Ranker Pod will have changed the matches in the Document to an reordered list of\nmatches based on the probabilities computed by our custom Ranker, <code>FinBertQARanker</code>. Next, we will see how we can\naccess this list of final matches in our <code>app.py</code>.</p>\n<h4>Build a Search Application</h4>\n<p align=\"center\">\n<figure>\n<img src=\"/assets/images/blog/financial-qa/query-flow-step4.png\" width=\"550\">\n<figurecaption>Get matches and scores stored in the Document</figurecaption>\n</figure>\n</p>\n<p>Since our final matches and their relevancy probability are stored in the Document, in <code>app.py</code>, we can\nwrite a function to print out the response to a question from the user input. We can loop through the matches in our\nDocument, <code>d.matches</code>, and print out the values of the scores and the matching answer text.</p>\n<script src=\"https://gist.github.com/yuanbit/1e18bcb2e7dbc77812a94ccefc6d1fcc.js\"></script>\n<p>We can then write our <code>search</code> method that uses the Query Flow from <code>flows/query.yml</code> and passes\nthe user inputs into <code>print_resp</code>. In <code>f.search_lines()</code>, we specify the input as our user query,\nthe output as the response to be printed, and the top-k answers we want to retrieve. The cool thing about\n<code>f.search_lines()</code> is that it automatically creates a Document for the user query, like sugar magic 🍬!</p>\n<script src=\"https://gist.github.com/yuanbit/434ef56655bea8daf8b8275348387e36.js\"></script>\n<p>Hooray! 🎉🎉🎉 We have just finished building our Financial QA search engine! We can now run:</p>\n<p><code>python app.py search</code></p>\n<p>and try out different questions! The Ranker might take some time to compute the relevancy\nscores since it is using a BERT-based model. Here is a list of sample questions:</p>\n<pre><code>• What does it mean that stocks are “memoryless”?\n• What would a stock be worth if dividends did not exist?\n• What are the risks of Dividend-yielding stocks?\n• Why do financial institutions charge so much to convert currency?\n• Is there a candlestick pattern that guarantees any kind of future profit?\n• 15 year mortgage vs 30 year paid off in 15\n• Why is it rational to pay out a dividend?\n• Why do companies have a fiscal year different from the calendar year?\n• What should I look at before investing in a start-up?\n• Where do large corporations store their massive amounts of cash?\n</code></pre>\n<p align=\"center\">\n<img src=\"/assets/images/blog/financial-qa/search-results.png\" width=\"900\">\n</p>\n<h2>Summary</h2>\n<p>In this blog, I introduced core Jina concepts and demonstrated how to build a production-ready Financial QA system using Jina.\nI also explained how to use the Jina Hub API to create a BERT-powered Ranker Executor. Thanks to\nthe building blocks that Jina provides, we could easily use the SOTA and powerful model, FinBERT-QA, in production.</p>\n<p>The neural search application we have just built with Jina runs locally on our own machines, but can also\nbe completely distributed and run on multiple machines in a network, making our application highly reusable, scalable,\nand efficient. On top of that, common cloud-native features such as persistence, scheduling, chaining, grouping, and\nparallelization all come out of the box.</p>\n<p>Moreover, there are variants of pre-trained BERT models for other domains such as <a href=\"https://github.com/dmis-lab/biobert\">biomedical</a>,\n<a href=\"https://github.com/allenai/scibert\">science</a>, and <a href=\"https://huggingface.co/nlpaueb/legal-bert-base-uncased\">legal</a>.\nYou can use these models to build a QA search application and experiment with the results!</p>\n<h2>Next Step: Evaluation</h2>\n<p>If you made it all the way through this tutorial, you might be wondering, <strong>\"how do I evaluate the search results?\"</strong>.\nGreat question! Jina has a class of Executors called the <strong>Evaluator</strong> and has implementations of common evaluation\nmetrics like Precision and Reciprocal Error. Evaluation is an important step and will allow us to optimize\nthe search results and design the most effective Flows. We will see in the next tutorial how we can add the\nEvaluator in our Financial QA application.</p>\n<h2>Learn More</h2>\n<p>To learn more about Jina, I recommend reading the following articles:</p>\n<ul>\n<li><a href=\"https://medium.com/jina-ai/what-is-jina-and-neural-search-7a9e166608ab\">What is Jina and Neural Search?</a></li>\n<li><a href=\"https://hanxiao.io/2020/10/19/A-Curated-List-of-Neural-Search-and-Jina-Framework-Designs/\">From Then to Now: a Curated List for Neural Search and Jina</a></li>\n</ul>\n<p>and checking out our <a href=\"https://github.com/jina-ai/jina\">Github</a> page!</p>\n<p>If you want to learn Jina by doing, I encourage you to start building your own examples and\nsharing them with the community to help us grow our open-source ecosystem! 🚀 For example, check out this\n<a href=\"https://github.com/ArturTan/transformers-for-lawyers\">community project - transformers-for-lawyers</a> built with Jina.</p>\n<p>We saw how versatile and extensible Jina is and we could create all kinds of search applications using our own\nlogic and models for NLP, Computer Vision, and other ML search applications.\n<strong><a href=\"https://github.com/jina-ai/jina-hub\">Jina Hub</a> is a great place to get started, where you can use\nthe available Executors to build other types of search engines (for images, videos, etc...) or create your own\nExecutors using the Jina Hub API!</strong> You can always come back to this tutorial and walk through the process again.</p>\n<p><strong>As an open-source company we would also love your help and contributions.️</strong> We have issues labelled as <a href=\"https://github.com/jina-ai/jina/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22\">good first issue</a>\nto get started! You can read more about our contributing guidelines <a href=\"https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md\">here</a>.</p>\n<p>If you want to know more about Jina's new features or ask any questions, welcome to join our <a href=\"https://slack.jina.ai/\">Slack Community</a>\nand our monthly public <a href=\"https://hanxiao.io/2020/08/06/Engineering-All-Hands-in-Public/\">Engineering All Hands</a> via\nZoom or Youtube live stream.</p>\n<p>If you are interested in joining us as a full-time AI / Backend / Frontend developer,\nplease submit your CV to our <a href=\"jobs.jina.ai\">job portal</a>. Let’s build the next open-source neural search ecosystem together!</p>\n<h2>Community</h2>\n<ul>\n<li><a href=\"slack.jina.ai\">Slack channel</a> - a communication platform for developers to discuss Jina</li>\n<li><a href=\"mailto:newsletter+subscribe@jina.ai\">Community newsletter</a> - subscribe to the latest update, release and event news of Jina</li>\n<li><a href=\"https://www.linkedin.com/company/jinaai/\">LinkedIn</a> - get to know Jina AI as a company and find job opportunities</li>\n<li><a href=\"https://twitter.com/JinaAI_\">Twitter</a> - follow us and interact with us using hashtag <code>#JinaSearch</code></li>\n<li><a href=\"https://jina.ai\">Company</a> - know more about our company, we are fully committed to open-source!</li>\n</ul>\n","coverImage":"/assets/images/blog/financial-qa.png"}},"__N_SSG":true}
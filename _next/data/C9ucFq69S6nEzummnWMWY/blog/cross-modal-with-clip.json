{"pageProps":{"post":{"title":"Using Jina to build a text-to-image search workflow based on OpenAI CLIP","date":"2021-03-25","slug":"cross-modal-with-clip","author":"David Buchaca, Bo Wang, Joan Fontanals","content":"<h2>Using Jina to build a text-to-image search workflow based on OpenAI CLIP</h2>\n<p>If you are not familiar with cross-modal search in Jina please read <a href=\"https://jina.ai/2020/10/02/Cross-modal-Search-with-Jina.html\">this wonderful blog post</a> which presents how you can use Jina and build a text-to-image search system based on VSE.</p>\n<p>To start, we need a dataset that we will use to retrieve items.\nFirst, you have to install <a href=\"https://pypi.org/project/kaggle/\">Kaggle</a> in your machine, because we will use the following  <a href=\"https://www.kaggle.com/adityajn105/flickr8k\">dataset</a>.\nAfter logging to Kaggle and setting your Kaggle Token in your system (as described <a href=\"https://www.kaggle.com/docs/api\">here</a>), run:</p>\n<pre><code class=\"language-sh\">kaggle datasets download adityajn105/flickr8k\nunzip flickr8k.zip \nrm flickr8k.zip\nmkdir data\nmkdir data/f8k\nmv Images data/f8k/images\nmv captions.txt data/f8k/captions.txt\n</code></pre>\n<p>make sure that your data folder has the following structure:</p>\n<pre><code class=\"language-sh\">data/f8k/images/*jpg\ndata/f8k/captions.txt\n</code></pre>\n<p>After the data is downloaded we can add  it in the root folder of  <a href=\"https://github.com/jina-ai/examples/blob/bd9d1b2b5ae2a27432add18a2905506a77a9295c/cross-modal-search/\">cross-modal-search</a> .\nAfter setting the working directory we are set to index the data using the CLIP model:</p>\n<pre><code class=\"language-sh\">python app.py -t index -n 16000 -d f8k -m clip\n</code></pre>\n<p>Once we've indexed the data we can perform a search by first running:</p>\n<pre><code class=\"language-sh\">python app.py -t query\n</code></pre>\n<p>Then, in a browser use <a href=\"https://jina.ai/jinabox.js/\">Jina Box</a> url, type anything in the search bar and wait to see the top-k results.</p>\n<h2>What is this magic? Introduction to  CLIP</h2>\n<p><a href=\"https://arxiv.org/pdf/2103.00020.pdf\">CLIP</a>  is a neural network built to jointly train an image encoder and a text encoder.\nThe learning task for CLIP consist of predicting correct pairings between text descriptions and images in a batch. The following image summarizes the training of the model:</p>\n<p><img src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png\" alt=\"flow\"></p>\n<p>After training, the model can be used to encode a piece of text or an image into a 512-dimensional vector.</p>\n<p>Since both text and images are embedded into the same fix-sized space the embedded text can be used to retrieve images (and vice-versa).</p>\n<p>Let <code>phi_t</code> and <code>phi_i</code> be the embeddings learned by CLIP to transform text and images respectively to vectors. In particular  <code>phi_t(x_text)</code> and <code>phi_t(x_image)</code> are 512 dimensional vectors.</p>\n<p>Given a text query <code>q</code> and  <code>N</code> images <code>(x_1, ..., x_N)</code>  we can compute the dot products between <code>q</code> and the <code>N</code> images and store them in the vector <code>s</code>. That is, <code>s = phi_t(q) * phi_i(x_1),  phi_t(q) * phi_i(x_2), ..., phi_t(q) * phi_i(x_N) )</code>.</p>\n<p>Then we can return the top <code>K</code>  elements  from  <code>x_1, ... , x_N</code> in a list <code>top_K</code>  using the similarity values in <code>s</code>, where the k'th value in <code>top_K</code> would correspond to the coordinate in  <code>s</code> with the k'th  highest score.</p>\n<h6>Quality CLIP vs VSE++</h6>\n<p>We can evaluate CLIP using  the mean reciprocal rank (MRR), which is a common metric to evaluate information retrieval systems.\nGiven a list of queries <code>query_list = (q_1,...,q_Q)</code> and a list (of lists) of retrieved items  <code>retrieved_list = ( r_1, ... ,  r_Q)</code>,  the mean reciprocal rank assigned to <code>(query_list, retrieved_list)</code> is defined as  the mean of the inverse of the ranks. That is:</p>\n<p><img src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/mean_reciprocal_rank.png\" alt=\"flow\"></p>\n<p>where <code>rank(q_i, r_i)</code> refers to the position of the first relevant document in <code>r_i</code>.</p>\n<p>The provided <a href=\"https://github.com/jina-ai/examples/tree/master/cross-modal-search\">evaluate.py</a> can be used to compute the MRR of CLIP and VSE++. The results are:</p>\n<pre><code>| Encoder | Modality      | Mean reciprocal Rank | Terminal command                                             |\n| ------- | ------------- | -------------------- | ------------------------------------------------------------ |\n| vse     | text to image | 0.3669               | python evaluate.py -e text2image -i 16000 -n 4000 -m vse -s 32 |\n| clip    | text to image | 0.4018               | python evaluate.py -e text2image -i 16000 -n 4000 -m clip -s 32 |\n| clip    | image to text | 0.4088               | python evaluate.py -e image2text -i 16000 -n 4000 -m clip -s 32 |\n| vse     | image to text | 0.3791               | python evaluate.py -e image2text -i 16000 -n 4000 -m vse -s 32 |\n</code></pre>\n<h2>Looking inside our <code>app.py</code></h2>\n<p>Now that we have seen how to use in our Jina application and the benefits of leveraging the CLIP encoder we can look at the code and the details of the implementation.</p>\n<p>A Jina application will usually start by setting several environment variables that are needed in order to properly set up the provided <code>.yml</code> files which might use different environment variables such as <strong><code>JINA_PARALLEL</code></strong> or <strong><code>JINA_SHARDS</code></strong>.\nThe setting of these variables is done in the <strong><code>config()</code></strong>  function.</p>\n<pre><code class=\"language-python\">def config(model_name):\n    os.environ['JINA_PARALLEL'] = os.environ.get('JINA_PARALLEL', '1')\n    os.environ['JINA_SHARDS'] = os.environ.get('JINA_SHARDS', '1')\n    os.environ['JINA_PORT'] = '45678'\n    os.environ['JINA_USE_REST_API'] = 'true'\n    if model_name == 'clip':\n        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.clipimageencoder:0.0.1-1.0.7')\n        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.cliptextencoder:0.0.1-1.0.7')\n        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/clip/text-encoder.yml'\n    elif model_name == 'vse':\n        os.environ['JINA_IMAGE_ENCODER'] = os.environ.get('JINA_IMAGE_ENCODER', 'docker://jinahub/pod.encoder.vseimageencoder:0.0.5-1.0.7')\n        os.environ['JINA_TEXT_ENCODER'] = os.environ.get('JINA_TEXT_ENCODER', 'docker://jinahub/pod.encoder.vsetextencoder:0.0.6-1.0.7')\n        os.environ['JINA_TEXT_ENCODER_INTERNAL'] = 'yaml/vse/text-encoder.yml'\n</code></pre>\n<p>This is paramount to start defining the <code>Flow</code>, because the <code>Flow</code> is created from\n<code>flow-index.yml</code>  which already expects some of this environment variables already set.\nFor example we can see <strong><code>shards: $JINA_PARALLEL</code></strong> and <strong><code>uses: $JINA_USES_VSE_IMAGE_ENCODER</code></strong>.</p>\n<pre><code class=\"language-bash\">!head -18 flow-index.yml\n</code></pre>\n<pre><code class=\"language-yaml\">!Flow\nversion: '1'\nwith:\n  prefetch: 10\npods:\n  - name: loader\n    uses: yaml/image-load.yml\n    shards: $JINA_PARALLEL\n    read_only: true\n  - name: normalizer\n    uses: yaml/image-normalize.yml\n    shards: $JINA_PARALLEL\n    read_only: true\n  - name: image_encoder\n    uses: $JINA_USES_VSE_IMAGE_ENCODER\n    shards: $JINA_PARALLEL\n    timeout_ready: 600000\n    read_only: true\n</code></pre>\n<p>Once <strong><code>config</code></strong> is called we can see that the application selects either <code>index</code> or <code>query</code> mode.</p>\n<p>Note that config sets some global variables with Docker, such as <code>'docker://clip_text_encoder'</code> .\nThis is needed becasue this encoder is not part of the core of Jina  and we need to specify how to find the docker for a specific part of the <code>Flow</code> that we will construct.</p>\n<p>Note that the <code>Pod</code> named <strong><code>loader</code></strong>, defined in the file <strong><code>image-load.yml</code></strong> does not need any docker information.\nThis file starts with <strong><code>!ImageReader</code></strong>  which is a crafter already provided in Jina's core.</p>\n<h3>Index mode</h3>\n<p>If <code>index</code> is selected a <code>Flow</code> is created with</p>\n<pre><code class=\"language-python\">f = Flow().load_config('flow-index.yml')\n</code></pre>\n<p>And the <code>Flow</code> can start indexing with <strong><code>f.index</code></strong></p>\n<pre><code class=\"language-python\">with f:\n    f.index(input_fn=input_index_data(num_docs, request_size, data_set), \n            request_size=request_size)\n</code></pre>\n<p>Here <strong><code>f.index</code></strong> receives a generator <strong><code>input_index_data</code></strong> that reads the input data and creates <strong><code>jina.Document</code></strong> objects with the images and the captions.</p>\n<p>We can plot a <code>Flow</code> using the <strong><code>f.plot(image_path)</code></strong> function. The output of</p>\n<pre><code class=\"language-python\">f.plot('flow_diagram_index.svg')\n</code></pre>\n<p>is the following plot:</p>\n<p><img src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_index.png\" alt=\"flow\"></p>\n<p>In the diagram, each blue node corresponds to one of <code>Pods</code> defined in <strong><code>flow-index.yml</code></strong>.\nThe string inside each node corresponds to the atrribute <strong><code>name</code></strong>  assigned to  each <code>Pod</code> in  <strong><code>flow-index.yml</code></strong>.</p>\n<p>Note that there are two main branches: the middle path transforms images to vectors while the bottom branch transforms text to vectors.\nBoth branches transform data to the same vector space.</p>\n<h3>Query mode</h3>\n<p>If  query mode is selected we can see that another <code>Flow</code> is created from <strong><code>flow_query.yml</code></strong></p>\n<pre><code class=\"language-python\">f = Flow().load_config('flow-query.yml')\nf.use_rest_gateway()\nwith f:\n    f.block()\n</code></pre>\n<p>as before, we can visually inspect the query <code>Flow</code> using <strong><code>f.plot('flow_diagram_index.svg')</code></strong> which will provide the following diagram:</p>\n<p><img src=\"/assets/images/blog/2021_03_04_cross_modal_with_clip/flow_query.png\" alt=\"flow\"></p>\n","coverImage":"/assets/images/blog/2021_03_04_cross_modal_with_clip/embedding.png","tags":["cross-modal","image&text search","CLIP"]}},"__N_SSG":true}
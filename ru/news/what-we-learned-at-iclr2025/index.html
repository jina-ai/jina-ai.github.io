<!DOCTYPE html><html translate="no" dir="ltr" lang="ru"><head><title>Что мы узнали на ICLR2025</title><meta charset="utf-8"><meta name="title" content="Что мы узнали на ICLR2025"><meta name="description" content="Мы собрали несколько наиболее интересных статей на ICLR 2025, посвященных TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba и т. д."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="og:title" content="Что мы узнали на ICLR2025"><meta property="og:description" content="Мы собрали несколько наиболее интересных статей на ICLR 2025, посвященных TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba и т. д."><meta property="og:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-we-learned-at-iclr2025"><meta property="twitter:title" content="Что мы узнали на ICLR2025"><meta property="twitter:description" content="Мы собрали несколько наиболее интересных статей на ICLR 2025, посвященных TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba и т. д."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-YcKF8TI2.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-JZ2jJDY_.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-C0Osewq_.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-BS_XIEMf.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-DvS87YWc.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-BWt7jZaZ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-B9p30eM3.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ru-CJwv998q.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BAOSQFVh.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-DabmIkP5.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-CW8ejzhT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-BGFZeNwH.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-C6j4vZxR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-SPOSAD8S.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-0jLRA8my.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-CcEU1oob.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-Buoz7n6t.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-CZ5ZUns-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-C_k600YI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-q2avoUHY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-BXn1CiUL.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-Ug6zhntO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-H6pj3DuM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-JhA-lJGT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DbiwX4j3.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-BPB3BEYN.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-E5LErBTI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-DwjjaqGU.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-CkMiLurS.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-Bm7roZuQ.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-C1lmt-F6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-D6yT2YJZ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-CUW7ambp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-CytXz6TE.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-DpHwTe8z.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-BJPm-iVk.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-0v_6KiP0.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Jina AI"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Jina AI"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="21 mins read"><meta property="article:published_time" content="2025-05-26T00:06:37.000+02:00"><meta property="article:modified_time" content="2025-05-26T00:06:37.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Что мы узнали на ICLR2025",
  "description": "Мы собрали несколько наиболее интересных статей на ICLR 2025, посвященных TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba и т. д.",
  "image": [
    "https://jina.ai/blog-banner/what-we-learned-at-iclr2025.webp"
  ],
  "datePublished": "2025-05-26T00:06:37.000+02:00",
  "dateModified": "2025-05-26T00:06:37.000+02:00",
  "author": [
    {
      "@type": "Organization",
      "name": "Jina AI",
      "url": "https://jina.ai"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Новости</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Модели</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_6c017e7a-2cba-4bc5-9b28-0917af262066" aria-label="Расширьте &quot;Продукты&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Продукты</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_6c017e7a-2cba-4bc5-9b28-0917af262066" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Продукты"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Читатель</div><div class="q-item__label q-item__label--caption text-caption">Читайте URL-адреса и ищите информацию в Интернете для получения более подходящей подготовки для получения степени магистра права.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Вложения</div><div class="q-item__label q-item__label--caption text-caption">Мультимодальные многоязычные вложения мирового класса.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Реранкер</div><div class="q-item__label q-item__label--caption text-caption">Нейронный ретривер мирового класса для максимального повышения релевантности поиска.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Глубокий поиск</div><div class="q-item__label q-item__label--caption text-caption">Ищите, читайте и рассуждайте, пока не найдете лучший ответ.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_526b491e-e79a-4123-b858-26629788b77e" aria-label="Расширьте &quot;Более&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Более</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_526b491e-e79a-4123-b858-26629788b77e" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Классификатор</div><div class="q-item__label q-item__label--caption text-caption">Классификация изображений и текста по нулевому и небольшому количеству кадров.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Сегментатор</div><div class="q-item__label q-item__label--caption text-caption">Разрежьте длинный текст на куски и выполните токенизацию.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">API-документы</div><div class="q-item__label q-item__label--caption text-caption">Автоматическая генерация кода для вашего второго пилота IDE или LLM</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_b1d58bc3-cd2f-46e2-8cb0-8508108ef311" aria-label="Расширьте &quot;Компания&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Компания</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_b1d58bc3-cd2f-46e2-8cb0-8508108ef311" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">О нас</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Связаться с отделом продаж</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Стажерская программа</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Присоединяйтесь к нам</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Скачать логотип</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Условия использования</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Авторизоваться"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Авторизоваться</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_cbdfcdf3-941f-43d1-91a5-f0500daaf3cb"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_cbdfcdf3-941f-43d1-91a5-f0500daaf3cb" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_cbdfcdf3-941f-43d1-91a5-f0500daaf3cb_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Mitigate the Gap: Improving Cross-Modal Alignment in CLIP</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">jina-clip-v2: Многоязычные мультимодальные векторные представления (Embeddings) для текста и изображений</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ReaderLM-V2: Малая языковая модель (SLM) для преобразования HTML в Markdown и JSON</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TIPS: Предварительное обучение текста и изображений с учетом пространственной осведомленности</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Cut Cross-Entropy: Эффективное вычисление потерь с точки зрения памяти для больших словарей</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">FlexPrefill: Разреженное внимание с учетом контекста для длинных последовательностей</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Эффективное сжатие векторных представлений (Embeddings) после обучения посредством контроля температуры</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Внимание в больших языковых моделях (LLM) обеспечивает эффективные 重排器 (Reranker) с нулевым выстрелом</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Установление связей и моделирование корреляций в парных данных для прямой оптимизации предпочтений</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">TAID: Временно-адаптивная интерполированная дистилляция для эффективной передачи знаний</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">SVD-LLM: Усечение-ориентированное сингулярное разложение для сжатия больших языковых моделей</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Видеть то, что тебе говорят: Поглотитель визуального внимания в больших мультимодальных моделях</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">К семантической эквивалентности токенизации в мультимодальных LLM</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Hymba: Гибридная архитектура голов для небольших языковых моделей</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Событие</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">май 25, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">Что мы узнали на ICLR2025</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Мы собрали несколько наиболее интересных статей на ICLR 2025, посвященных TIPS, FlexPrefill, Zero-Shot Rerankers, SVD-LLM, Hymba и т. д.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img"><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/ezgif-1ce788dea541e5.webp" style="object-fit: contain; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Jina AI"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Jina AI" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/08/Jjqb-JeY_400x400.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Jina AI • 21 минуты чтения</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>ICLR 2025 — одна из крупнейших и самых влиятельных конференций по машинному обучению в мире, наряду с NeurIPS и ICML, являющаяся тремя главными площадками для высокоэффективных исследований в области ИИ. Этот год ознаменовал собой историческую веху, поскольку ICLR впервые прошла в Азии, в сингапурском выставочном центре EXPO с 24 по 28 апреля. Время было выбрано как нельзя лучше — всего через несколько месяцев после "момента DeepSeek" в конце января 2025 года, который потряс Силиконовую долину и продемонстрировал быстрое развитие исследований в области ИИ в Китае. В сочетании с новым китайско-сингапурским соглашением о 30-дневном взаимном безвизовом режиме, вступившим в силу в феврале 2024 года, мы стали свидетелями беспрецедентного роста числа участников конференции из Китая.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png" class="kg-image" alt="" width="2000" height="1106" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-8.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-8.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-8.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-8.png 2126w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>В этом году наша команда с воодушевлением отправилась в Сингапур, где Седиге Эслами, Андреас Кукунас, Ван Фэн и генеральный директор Хан Сяо представили три исследовательские работы, демонстрирующие наши последние исследования по <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a> и <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a> для улучшения поиска. В то время как остальной мир ИИ, похоже, погряз в гонке вооружений за все более крупные модели, мы решили пойти против нормы, доказав, что меньшие, более умные модели могут быть намного эффективнее, если правильно спроектировать.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png" class="kg-image" alt="" width="2000" height="1391" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-9.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-9.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-9.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-9.png 2108w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Итак, берите кофе, устраивайтесь поудобнее, и давайте рассмотрим некоторые интересные исследования ICLR, начиная с нашего собственного взгляда на то, почему малый размер может быть мощным.</p><h2 id="mitigate-the-gap-improving-cross-modal-alignment-in-clip" style="position: relative;"><a href="#mitigate-the-gap-improving-cross-modal-alignment-in-clip" title="Mitigate the Gap: Improving Cross-Modal Alignment in CLIP" id="anchor-mitigate-the-gap-improving-cross-modal-alignment-in-clip"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mitigate the Gap: Improving Cross-Modal Alignment in CLIP</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.17639"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</div><div class="kg-bookmark-description">Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding space overly sparse and disconnected, with different modalities being densely distributed in distinct subregions of the hypersphere. In this work, we aim at answering three main questions: 1. Does sharing the parameter space between the multi-modal encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart the uni-modal embeddings via intra-modality separation? 3. How do these gap reduction approaches affect the downstream performance? We design AlignCLIP, in order to answer these questions and through extensive experiments, we show that AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the embeddings, and thereby, reduces the modality gap, while improving the performance across several zero-shot and fine-tuning downstream evaluations.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-21.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Sedigheh Eslami</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-17.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png" class="kg-image" alt="" width="2000" height="1279" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-10.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-10.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-10.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-10.png 2120w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Модели CLIP превосходно справляются с задачами обработки изображений и текста, но страдают от <a href="https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models">"модального разрыва" ("modality gap")</a> — векторные представления (embeddings) изображений и текста кластеризуются в отдельных регионах, что ограничивает производительность. Эта работа, проведенная нашей стажеркой Седиге Эслами во время ее работы над докторской диссертацией в Институте Хассо Платтнера, решает эту фундаментальную проблему.</p><p>Мы обнаружили, что простой перенос векторов разрушает структуру векторных представлений (embedding). Вместо этого, <strong>AlignCLIP</strong> использует общие параметры энкодера с семантически регулируемыми целями разделения. Этот двойной подход успешно уменьшает модальный разрыв, повышая производительность в задачах zero-shot и fine-tuning.</p><p><strong>Основные выводы:</strong></p><ul><li>Модальный разрыв является критическим узким местом в производительности CLIP</li><li>Совместное использование параметров + семантическое разделение эффективно устраняют модальные различия</li><li>Подход обеспечивает ощутимые улучшения при оценке downstream</li></ul><h2 id="jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" style="position: relative;"><a href="#jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images" title="jina-clip-v2: Многоязычные мультимодальные векторные представления (Embeddings) для текста и изображений" id="anchor-jina-clip-v2-multilingual-multimodal-embeddings-for-text-and-images"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>jina-clip-v2: Многоязычные мультимодальные векторные представления (Embeddings) для текста и изображений</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2412.08802"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images</div><div class="kg-bookmark-description">Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model’s performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-22.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Andreas Koukounas</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-18.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png" class="kg-image" alt="" width="2000" height="1115" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-11.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-11.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-11.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-11.png 2268w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Это статья, лежащая в основе <a class="dynamic-model-name" href="/?sui&amp;model=jina-clip-v2" target="_blank"><span class="dynamic-model-name-inner">jina-clip-v2</span></a>, многоязычной мультимодальной модели векторных представлений (embedding), которая поддерживает задачи, связанные только с текстом, и кросс-модальные задачи, используя многозадачный, многоэтапный подход контрастного обучения. Модель сочетает в себе текстовый энкодер (Jina XLM-RoBERTa, 561M параметров) и визуальный энкодер (EVA02-L14, 304M параметров), что в сумме составляет 865M параметров. Мы обучаем на многоязычных текстах из 29 неанглийских языков и визуально насыщенных документах, используя Matryoshka Representation Learning для гибкого определения размерности векторных представлений (embedding).</p><p><strong>Основные выводы:</strong></p><ul><li>Смешивание данных изображения-текста и текста-текста в отдельных пакетах с общими параметрами температуры работает хуже, чем раздельное обучение, из-за асимметрии информации о модальности.</li><li>Обучение кросс-модальному выравниванию по своей сути ставит под угрозу качество векторных представлений (embedding) только текста, демонстрируя фундаментальный компромисс.</li><li>Сокращение размерности векторных представлений (embedding) с 1024 до 256 измерений приводит к потере производительности менее чем на 1%, что свидетельствует о значительной неэффективности высокоразмерных представлений.</li></ul><h2 id="readerlm-v2-small-language-model-for-html-to-markdown-and-json" style="position: relative;"><a href="#readerlm-v2-small-language-model-for-html-to-markdown-and-json" title="ReaderLM-V2: Малая языковая модель (SLM) для преобразования HTML в Markdown и JSON" id="anchor-readerlm-v2-small-language-model-for-html-to-markdown-and-json"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ReaderLM-V2: Малая языковая модель (SLM) для преобразования HTML в Markdown и JSON</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.01151"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</div><div class="kg-bookmark-description">We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model’s effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-23.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Feng Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-19.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png" class="kg-image" alt="" width="1196" height="912" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-12.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-12.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-12.png 1196w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Это статья о <a class="dynamic-model-name" href="/models/ReaderLM-v2" target="_blank"><span class="dynamic-model-name-inner">ReaderLM-v2</span></a>, компактной языковой модели с 1,5 миллиардами параметров, разработанной для эффективного извлечения веб-контента. Модель обрабатывает документы размером до 512 тысяч 词元 (Tokens), преобразуя грязный HTML в чистые форматы Markdown или JSON. Наш подход сочетает в себе трехэтапный конвейер синтеза данных (DRAFT-REFINE-CRITIQUE), который генерирует высококачественные обучающие данные посредством итеративной доработки, с унифицированной структурой обучения, сочетающей непрерывное предварительное обучение, контролируемую тонкую настройку, прямую оптимизацию предпочтений и итеративную настройку самовоспроизведения. ReaderLM-v2 превосходит GPT-4o и другие более крупные модели на 15-20% по эталонным тестам, особенно преуспевая в работе с документами, превышающими 100 тысяч 词元 (Tokens), при этом сохраняя значительно более низкие вычислительные требования.</p><p><strong>Основные выводы:</strong></p><ul><li>Модель с 1,5 миллиардами параметров превосходит GPT-4o и модели с 32 миллиардами параметров на 15-20% при извлечении HTML, доказывая, что специфическая для задач тонкая настройка превосходит необработанный масштаб для экспертных знаний в предметной области.</li><li>Модель генерирует свои собственные обучающие данные на этапе 4 "самовоспроизведение", создавая наборы данных лучше, чем курируемые человеком, и постоянно улучшая производительность посредством рекурсивной обратной связи.</li><li>Модель страдала от катастрофического повторения 词元 (Tokens) во время обучения, но добавление контрастивных потерь для стимулирования дискриминативных представлений полностью устранило эту проблему дегенерации.</li></ul><h2 id="tips-text-image-pretraining-with-spatial-awareness" style="position: relative;"><a href="#tips-text-image-pretraining-with-spatial-awareness" title="TIPS: Предварительное обучение текста и изображений с учетом пространственной осведомленности" id="anchor-tips-text-image-pretraining-with-spatial-awareness"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TIPS: Предварительное обучение текста и изображений с учетом пространственной осведомленности</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.16512"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TIPS: Предварительное обучение текста и изображений с учетом пространственной осведомленности</div><div class="kg-bookmark-description">В то время как обучение представлению изображений и текста стало очень популярным в последние годы, существующим моделям, как правило, не хватает пространственной осведомленности, и они имеют ограниченную прямую применимость для задач плотного понимания. По этой причине самообучение только на изображениях по-прежнему является основным методом для многих задач плотного видения (например, оценка глубины, семантическая сегментация), несмотря на отсутствие явных контрольных сигналов. В этой статье мы устраняем этот разрыв между обучением изображений и текста и самообучением, предлагая новую универсальную модель изображений и текста, которая может быть эффективно использована "из коробки" для задач плотного и глобального видения. Наш метод, который мы называем предварительным обучением текста и изображений с пространственной осведомленностью (TIPS), использует два простых и эффективных подхода. Во-первых, в отношении текстового контроля: мы показываем, что замена шумных веб-подписей к изображениям синтетически сгенерированными текстовыми описаниями значительно повышает производительность плотного понимания из-за гораздо более богатого сигнала для обучения пространственно осведомленным представлениям. Мы предлагаем адаптированный метод обучения, который сочетает в себе шумные и синтетические подписи, что приводит к улучшениям как в задачах плотного, так и в задачах глобального понимания. Во-вторых, в отношении техники обучения: мы предлагаем объединить контрастное обучение изображений и текста с самообучающимся моделированием замаскированных изображений, чтобы стимулировать пространственную согласованность, открывая существенные улучшения для последующих приложений. Основываясь на этих двух идеях, мы масштабируем нашу модель с использованием архитектуры transformer, обученной на курируемом наборе общедоступных изображений. Наши эксперименты проводятся на 8 задачах, включающих в общей сложности 16 наборов данных, демонстрируя высокую производительность "из коробки" как в задачах плотного, так и в задачах глобального понимания для нескольких задач только с изображениями и изображениями и текстом. Код и модели выпущены по адресу https://github.com/google-deepmind/tips.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-24.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Kevis-Kokitsi Maninis</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-20.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png" class="kg-image" alt="" width="2000" height="1184" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-13.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-13.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-13.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-13.png 2210w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Модели видения и языка, обученные с использованием контрастного обучения, превосходно справляются с глобальным выравниванием изображений и текста, но терпят неудачу в задачах плотного пространственного понимания. TIPS сочетает в себе контрастное обучение с моделированием замаскированных изображений и использует синтетически сгенерированные подписи, кодирующие пространственные отношения, создавая 向量模型 (Embeddings), подходящие как для плотного, так и для глобального понимания без тонкой настройки для конкретной задачи. Подход демонстрирует, как пространственная осведомленность может быть включена в 向量模型 (embedding models) для лучшего понимания документов и приложений мультимодального поиска.</p><p><strong>Основные выводы:</strong></p><ul><li>Синтетические подписи с пространственными описаниями предоставляют более богатые сигналы обучения, чем шумные веб-подписи, для обучения пространственно-осведомленным представлениям</li><li>Сочетание контрастного обучения изображений и текста с самоконтролируемыми целями устраняет разрыв между глобальным и плотным пониманием</li><li>Производительность "из коробки" в различных задачах устраняет необходимость в специализированной тонкой настройке для различных приложений компьютерного зрения</li></ul><h2 id="cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" style="position: relative;"><a href="#cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies" title="Cut Cross-Entropy: Эффективное вычисление потерь с точки зрения памяти для больших словарей" id="anchor-cut-cross-entropy-memory-efficient-loss-computation-for-large-vocabularies"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Cut Cross-Entropy: Эффективное вычисление потерь с точки зрения памяти для больших словарей</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.09009"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Сократите свои потери в языковых моделях с большим словарем</div><div class="kg-bookmark-description">По мере того как языковые модели становятся все больше, увеличиваются и их словари. Это сместило объем памяти 大模型 (LLM) во время обучения непропорционально в один слой: перекрестную энтропию при вычислении потерь. Перекрестная энтропия создает матрицу логитов с записями для каждой пары входных 词元 (Tokens) и словарных элементов и для небольших моделей потребляет на порядок больше памяти, чем остальная часть 大模型 (LLM) вместе взятая. Мы предлагаем Cut Cross-Entropy (CCE), метод, который вычисляет потери перекрестной энтропии без материализации логитов для всех 词元 (Tokens) в глобальной памяти. Вместо этого CCE вычисляет логит только для правильного 词元 (Token) и оценивает log-sum-exp по всем логитам "на лету". Мы реализуем пользовательское ядро, которое выполняет матричные умножения и уменьшение log-sum-exp по словарю во флэш-памяти, делая потребление глобальной памяти для вычисления перекрестной энтропии незначительным. Это имеет драматический эффект. Взяв в качестве примера модель Gemma 2 (2B), CCE снижает объем памяти, занимаемой вычислением потерь, с 24 ГБ до 1 МБ, а общее потребление памяти классификатора во время обучения - с 28 ГБ до 1 ГБ. Чтобы повысить пропускную способность CCE, мы используем присущую softmax разреженность и предлагаем пропускать элементы вычисления градиента, которые вносят незначительный (т.е. ниже численной точности) вклад в градиент. Эксперименты показывают, что значительное сокращение потребления памяти достигается без ущерба для скорости обучения или сходимости.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-25.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Erik Wijmans</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-21.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png" class="kg-image" alt="" width="1904" height="1226" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-14.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-14.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-14.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-14.png 1904w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Вычисление перекрестной энтропии доминирует в использовании памяти в языковых моделях с большим словарем, требуя материализации матриц логитов, пропорциональных batch_size × vocabulary_size. CCE переформулирует вычисление, чтобы вычислять только необходимые компоненты "на лету" с использованием пользовательских ядер CUDA, уменьшая потребление памяти с гигабайт до мегабайт, сохраняя при этом идентичную динамику обучения. Это позволяет обучать 向量模型 (embedding) и 重排器 (reranking) с большими словарями на ограниченном оборудовании, что особенно полезно для многоязычных и специфических для предметной области приложений.</p><p><strong>Основные выводы:</strong></p><ul><li>Вычисление потерь перекрестной энтропии может потреблять 90% памяти при обучении моделей с большим словарем, становясь основным узким местом</li><li>Вычисление log-sum-exp терминов "на лету" устраняет необходимость материализации полных матриц логитов без математических приближений</li><li>Реализация пользовательского ядра обеспечивает значительное снижение объема памяти при сохранении точных свойств сходимости</li></ul><h2 id="flexprefill-context-aware-sparse-attention-for-long-sequences" style="position: relative;"><a href="#flexprefill-context-aware-sparse-attention-for-long-sequences" title="FlexPrefill: Разреженное внимание с учетом контекста для длинных последовательностей" id="anchor-flexprefill-context-aware-sparse-attention-for-long-sequences"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>FlexPrefill: Разреженное внимание с учетом контекста для длинных последовательностей</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2502.20766"><div class="kg-bookmark-content"><div class="kg-bookmark-title">FlexPrefill: Механизм разреженного внимания с учетом контекста для эффективного вывода длинных последовательностей</div><div class="kg-bookmark-description">Большие языковые модели (LLM) сталкиваются с вычислительными трудностями во время логического вывода длинных последовательностей, особенно на этапе предварительного заполнения внимания, где сложность растет квадратично с длиной запроса (Prompt). Предыдущие усилия по смягчению этих проблем опирались на фиксированные разреженные шаблоны внимания или выявление разреженных шаблонов внимания на основе ограниченных случаев. Однако этим методам не хватало гибкости для эффективной адаптации к различным требованиям ввода. В этой статье мы представляем FlexPrefill, гибкий механизм разреженного предварительного заполнения, который динамически регулирует разреженные шаблоны внимания и вычислительный бюджет в режиме реального времени, чтобы соответствовать конкретным требованиям каждого ввода и каждой головы внимания. Гибкость нашего метода демонстрируется двумя ключевыми нововведениями: 1) Определение разреженного шаблона с учетом запроса: измеряя дивергенцию Дженсена-Шеннона, этот компонент адаптивно переключается между разнообразными шаблонами внимания, специфичными для запроса, и предопределенными шаблонами внимания. 2) Выбор индекса на основе кумулятивного внимания: этот компонент динамически выбирает индексы «запрос-ключ» для вычисления на основе различных шаблонов внимания, гарантируя, что сумма оценок внимания соответствует предопределенному порогу. FlexPrefill адаптивно оптимизирует разреженный шаблон и коэффициент разреженности каждой головы внимания на основе запроса, повышая эффективность задач логического вывода длинных последовательностей. Экспериментальные результаты показывают значительные улучшения как в скорости, так и в точности по сравнению с предыдущими методами, предоставляя более гибкое и эффективное решение для логического вывода LLM.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-26.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xunhao Lai</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-22.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png" class="kg-image" alt="" width="1882" height="1254" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-15.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-15.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2025/05/image-15.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-15.png 1882w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Логический вывод трансформера длинной последовательности страдает от квадратичной сложности внимания. FlexPrefill динамически определяет разреженные шаблоны внимания для каждой головы, используя дивергенцию Дженсена-Шеннона, и адаптивно распределяет вычислительный бюджет на основе кумулятивных оценок внимания, достигая значительного ускорения с минимальной потерей точности для различных типов контента. Этот метод обеспечивает эффективную обработку длинных документов для систем поиска и извлечения, позволяя небольшим языковым моделям (SLM) обрабатывать расширенные контексты для лучшего понимания документов.</p><p><strong>Выводы:</strong></p><ul><li>Динамические разреженные шаблоны внимания, адаптированные к типу контента, превосходят стратегии фиксированной разреженности для различных характеристик ввода.</li><li>Адаптивное распределение бюджета для каждой головы на основе накопления оценок внимания оптимизирует распределение вычислений в режиме реального времени.</li><li>Учет контекста достигает 13,7-кратного ускорения с потерей точности 0,1%, не требуя переобучения модели.</li></ul><h2 id="effective-post-training-embedding-compression-via-temperature-control" style="position: relative;"><a href="#effective-post-training-embedding-compression-via-temperature-control" title="Эффективное сжатие векторных представлений (Embeddings) после обучения посредством контроля температуры" id="anchor-effective-post-training-embedding-compression-via-temperature-control"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Эффективное сжатие векторных представлений (Embeddings) после обучения посредством контроля температуры</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://openreview.net/forum?id=szRmEM8Kx5"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Effective post-training embedding compression via temperature...</div><div class="kg-bookmark-description">Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates…</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/favicon-37.ico" alt="" style="cursor: help;"><span class="kg-bookmark-author">OpenReview.net</span><span class="kg-bookmark-publisher">Georgiana Dinu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/pdf_icon_blue.svg" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png" class="kg-image" alt="" width="1230" height="906" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2025/05/image-16.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2025/05/image-16.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/2025/05/image-16.png 1230w" sizes="(min-width: 720px) 720px" style="cursor: help;"></figure><p>Масштабирование температуры в контрастивном обучении значительно влияет на внутреннюю размерность изученных векторных представлений (Embeddings), при этом более низкие температуры производят более сжимаемые представления. В статье демонстрируется, что методы агрегации температуры могут уменьшить размерность векторного представления (Embedding) на порядок, сохраняя при этом производительность поиска, выявляя компромисс между эффективностью кластеризации и точностью поиска. Это обеспечивает эффективное развертывание систем плотного поиска, где ограничения памяти имеют решающее значение для производственных приложений.</p><p><strong>Выводы:</strong></p><ul><li>Более низкие значения температуры в контрастивной тренировке производят векторные представления (Embeddings) с более низкой внутренней размерностью, которые более эффективно сжимаются.</li><li>Методы агрегации температуры достигают 10-кратного коэффициента сжатия с минимальным ухудшением качества в задачах поиска.</li><li>Систематический контроль температуры во время обучения предоставляет прямой механизм для оптимизации компромисса между сжатием и производительностью.</li></ul><h2 id="attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" style="position: relative;"><a href="#attention-in-large-language-models-yields-efficient-zero-shot-re-rankers" title="Внимание в больших языковых моделях (LLM) обеспечивает эффективные 重排器 (Reranker) с нулевым выстрелом" id="anchor-attention-in-large-language-models-yields-efficient-zero-shot-re-rankers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Внимание в больших языковых моделях (LLM) обеспечивает эффективные 重排器 (Reranker) с нулевым выстрелом</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2410.02642"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers</div><div class="kg-bookmark-description">Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more directly leverage such signals, we propose in-context re-ranking (ICR), a novel method that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>) forward passes to re-rank <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> documents, making it substantially more efficient than generative re-ranking methods that require at least <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mclose">)</span></span></span></span></span> forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR’s performance is specially strong on tasks that require more complex re-ranking signals. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-27.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shijie Chen</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-23.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd_nbgJG03yk0oOma7ozLQ6zutKGy3ngkVLKUwmp3ie6UPp2RR6qjwsWzwwtP0QzyAneCTD24nrPXpA085rkjtS_HmlrkHbrksxSjsaknx1lb9OtgtlACmwoOZkoRK9oPb4Haf?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Внутриконтекстное 重排 (Reranking) (ICR) использует изменения в структуре внимания в LLM для выполнения 重排 (Reranking) документов без генерации текста, снижая вычислительную сложность с O(N log N) до O(1). Этот метод агрегирует веса внимания по слоям и головам для вычисления оценок релевантности, с калибровкой запросов без контента для смягчения смещений LLM. Этот подход позволяет эффективно 重排 (Reranking) с моделями с открытым весом, устраняя необходимость в специализированной тонкой настройке или дорогостоящих процессах генерации.</p><p><strong>Выводы:</strong> </p><ul><li>Структуры внимания в LLM содержат достаточно сигналов для эффективного 重排 (Reranking) документов без необходимости генерации текста.</li><li>Калибровка запросов без контента успешно смягчает внутренние смещения в механизмах оценки на основе внимания.</li><li>ICR достигает превосходной производительности и эффективности по сравнению с генеративными методами, особенно в сложных задачах поиска с несколькими переходами.</li></ul><h2 id="bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" style="position: relative;"><a href="#bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization" title="Установление связей и моделирование корреляций в парных данных для прямой оптимизации предпочтений" id="anchor-bridging-and-modeling-correlations-in-pairwise-data-for-direct-preference-optimization"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Установление связей и моделирование корреляций в парных данных для прямой оптимизации предпочтений</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2408.07471"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization</div><div class="kg-bookmark-description">Прямая оптимизация предпочтений (DPO), широко используемый алгоритм оптимизации офлайн-предпочтений, направлена на согласование больших языковых моделей (LLM) с желаемым поведением человека с использованием парных данных о предпочтениях. Однако генерация выигрышного и проигрышного ответов в парных данных обычно изолирована, что приводит к слабой корреляции между ними, а также к субоптимальной производительности согласования. Для решения этой проблемы мы предлагаем эффективную структуру для преодоления и моделирования корреляций в парных данных, под названием BMC. Во-первых, мы повышаем согласованность и информативность парных сигналов предпочтений посредством целенаправленных модификаций, синтезируя псевдо-выигрышный ответ путем улучшения проигрышного ответа с выигрышным ответом в качестве эталона. Во-вторых, мы определили, что одного DPO недостаточно для моделирования этих корреляций и захвата нюансированных вариаций. Поэтому мы предлагаем изучать корреляции на уровне 词元 (Tokens) путем динамического использования уверенности модели политики во время обучения. Всесторонние эксперименты по задачам QA, математике и следованию инструкциям демонстрируют эффективность нашего подхода, значительно превосходя конкурентные базовые показатели, включая DPO. Кроме того, наш углубленный количественный анализ выявляет причины превосходной производительности нашего метода по сравнению с DPO и демонстрирует его универсальность по отношению к другим вариантам DPO. Мы публикуем наш репозиторий по адресу https://github.com/YJiangcm/BMC.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-28.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Yuxin Jiang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-24.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfd94pEqUtljUS8gF3KonDFqs9umcVCfXEASHlAeCjl07YMviucHiIj1doZIe5_VHSVxthzhgA_ta0E90vQVcunSRj0UnHsubFzD75ow-EfNICcDadQvdtUx-WOZGt9v9rFB_4E?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Традиционный DPO страдает от слабой корреляции между выбранными и отклоненными ответами в парах предпочтений, что ограничивает эффективность согласования модели. BMC решает эту проблему путем синтеза псевдо-предпочтительных ответов, которые интерполируются между выигрышными и проигрышными ответами, а затем применяет моделирование корреляции на уровне 词元 (Tokens) с использованием уверенности модели политики. Двухфазный подход сначала преодолевает пары предпочтений посредством целенаправленных модификаций, а затем моделирует детализированные корреляции во время обучения для улучшения качества сигнала обучения.</p><p><strong>Основные выводы:</strong></p><ul><li>Слабая корреляция между выбранными и отклоненными ответами в данных о предпочтениях значительно ограничивает эффективность DPO для согласования модели</li><li>Синтез псевдо-предпочтительных ответов в качестве интерполяций между парами предпочтений обеспечивает более богатые сигналы обучения для оптимизации</li><li>Моделирование корреляции на уровне 词元 (Tokens) с использованием уверенности политики динамически взвешивает сигналы обучения для захвата нюансированных вариаций в данных о предпочтениях</li></ul><h2 id="taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" style="position: relative;"><a href="#taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer" title="TAID: Временно-адаптивная интерполированная дистилляция для эффективной передачи знаний" id="anchor-taid-temporally-adaptive-interpolated-distillation-for-efficient-knowledge-transfer"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>TAID: Временно-адаптивная интерполированная дистилляция для эффективной передачи знаний</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2501.16937"><div class="kg-bookmark-content"><div class="kg-bookmark-title">TAID: Временно-адаптивная интерполированная дистилляция для эффективной передачи знаний в языковых моделях</div><div class="kg-bookmark-description">Каузальные языковые модели продемонстрировали замечательные возможности, но их размер создает значительные проблемы для развертывания в средах с ограниченными ресурсами. Дистилляция знаний, широко используемый метод передачи знаний от большой обучающей модели к маленькой студенческой модели, представляет собой перспективный подход к сжатию модели. Значительная оставшаяся проблема заключается в основных различиях между обучающей и студенческой моделями, а именно в существенном разрыве в возможностях, усреднении режимов и коллапсе режимов, которые создают барьеры во время дистилляции. Для решения этих проблем мы представляем <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="italic">Временно-адаптивную</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">интерполированную</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">дистилляцию</mtext><mtext>&nbsp;</mtext><mtext mathvariant="italic">(TAID)</mtext></mrow><annotation encoding="application/x-tex">\textit{Временно-адаптивную интерполированную дистилляцию (TAID)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord textit cyrillic_fallback">Временно</span><span class="mord textit">-</span><span class="mord textit cyrillic_fallback">адаптивную</span><span class="mord textit">&nbsp;</span><span class="mord textit cyrillic_fallback">интерполированную</span><span class="mord textit">&nbsp;</span><span class="mord textit cyrillic_fallback">дистилляцию</span><span class="mord textit">&nbsp;(TAID)</span></span></span></span></span></span>, новый подход к дистилляции знаний, который динамически интерполирует распределения студента и учителя через адаптивное промежуточное распределение, постепенно переходя от начального распределения студента к распределению учителя. Мы предоставляем теоретический анализ, демонстрирующий способность TAID предотвращать коллапс режимов, и эмпирически показываем его эффективность в решении проблемы разрыва в возможностях при одновременной балансировке усреднения режимов и коллапса режимов. Наши всесторонние эксперименты демонстрируют превосходную производительность TAID для моделей различных размеров и архитектур как в настройке инструкций, так и в сценариях предварительного обучения. Кроме того, мы демонстрируем практическое влияние TAID, разрабатывая две современные компактные базовые модели: <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-LLM-1.5B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-LLM-1.5B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-LLM-1.5B</span></span></span></span></span></span> для языковых задач и <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">TAID-VLM-2B</mtext></mrow><annotation encoding="application/x-tex">\texttt{TAID-VLM-2B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6111em;"></span><span class="mord text"><span class="mord texttt">TAID-VLM-2B</span></span></span></span></span></span> для задач обработки визуальной информации. Эти результаты демонстрируют эффективность TAID в создании высокопроизводительных и эффективных моделей, продвигая разработку более доступных технологий искусственного интеллекта.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-29.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Makoto Shing</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-25.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe9EWtq20jDVieU8M2BPDP5kENd3oSJKwIKNnKq_9bB4mb9vtNvjK-RMx8ZB29EhnyjIST90b2HRNek6bSkPXFlOxzTPhUAjf86d6iBCphJtgjfcxrCdY__HcDW9ADgVla1mVWBpQ?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="312" style="cursor: help;"></figure><p>Дистилляция знаний сталкивается с проблемами из-за разрывов в возможностях, усреднения режимов и коллапса режимов при передаче знаний между большими и маленькими моделями. TAID представляет динамического промежуточного учителя, который интерполирует распределения студента и учителя, постепенно адаптируя целевое распределение на основе прогресса обучения. Этот подход предотвращает коллапс режимов посредством теоретических гарантий и достигает превосходной производительности для моделей различных размеров, позволяя разрабатывать компактные, но способные языковые модели.</p><p><strong>Основные выводы: </strong></p><ul><li>Динамические промежуточные учителя, которые адаптируются во время обучения, обеспечивают более плавные траектории обучения по сравнению с дистилляцией фиксированного учителя</li><li>TAID предотвращает коллапс режимов посредством адаптивной интерполяции, одновременно балансируя передачу знаний при различных разрывах в возможностях</li><li>Метод позволяет обучать современные компактные модели, не требуя специализированных архитектур или обширной настройки гиперпараметров</li></ul><h2 id="svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" style="position: relative;"><a href="#svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression" title="SVD-LLM: Усечение-ориентированное сингулярное разложение для сжатия больших языковых моделей" id="anchor-svd-llm-truncation-aware-singular-value-decomposition-for-large-language-model-compression"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>SVD-LLM: Усечение-ориентированное сингулярное разложение для сжатия больших языковых моделей</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2403.07378"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SVD-LLM: Усечение-ориентированное сингулярное разложение для сжатия больших языковых моделей</div><div class="kg-bookmark-description">Развитие больших языковых моделей (LLM) сдерживается их значительными размерами, что требует методов сжатия LLM для практического развертывания. Сингулярное разложение (SVD) предлагает многообещающее решение для сжатия LLM. Однако современные методы сжатия LLM на основе SVD имеют два ключевых ограничения: усечение меньших сингулярных чисел может привести к более высоким потерям сжатия, а также отсутствие обновления сжатых весов после усечения SVD. В этой работе мы предлагаем SVD-LLM, метод сжатия LLM после обучения на основе SVD, который решает ограничения существующих методов. SVD-LLM включает в себя технику усечения-ориентированного отбеливания данных, чтобы обеспечить прямое сопоставление между сингулярными числами и потерями сжатия. Кроме того, SVD-LLM использует обновление параметров с последовательным приближением низкого ранга, чтобы компенсировать ухудшение точности после сжатия SVD. Мы оцениваем SVD-LLM на 10 наборах данных и семи моделях из трех различных семейств LLM в трех различных масштабах. Наши результаты демонстрируют превосходство SVD-LLM над современными методами, особенно при высоких коэффициентах сжатия модели. Наш код доступен по адресу https://github.com/AIoT-MLSys-Lab/SVD-LLM</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-30.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Wang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-26.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXenhj46Ar7NKFevDTmA3FK2dnjd7nQxdhULJ1H3Je-2OKoQN6_Ov8km-AvIldpEriENz2Q465hq2yoOZ1lLAle7ijbMgSK0ME9UxNeIN3yqyRFtRO_3FFXEyXdI04wndPS17a-3?key=HM7UHIZt2c2Fh4qitXjYcQ" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Существующие методы сжатия на основе SVD не учитывают входные активации во время приближения и не имеют тонкой настройки после усечения. SVD-LLM включает в себя усечение-ориентированное отбеливание данных, которое учитывает распределения активаций, и применяет тонкую настройку на основе LoRA после сжатия. Метод устанавливает теоретические связи между сингулярными числами и потерями сжатия, обеспечивая более принципиальные решения по сжатию, которые превосходят структурированную обрезку и подходы к квантованию.</p><p><strong>Основные выводы:</strong> </p><ul><li>Усечение-ориентированное отбеливание данных, которое учитывает входные активации, значительно повышает эффективность сжатия SVD по сравнению с методами, не зависящими от активации</li><li>Тонкая настройка LoRA после сжатия компенсирует ухудшение точности, сохраняя при этом преимущества факторизации низкого ранга</li><li>Теоретический анализ, связывающий сингулярные числа с потерями сжатия, обеспечивает принципиальные решения по усечению, которые превосходят эвристические подходы</li></ul><h2 id="see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" style="position: relative;"><a href="#see-what-you-are-told-visual-attention-sink-in-large-multimodal-models" title="Видеть то, что тебе говорят: Поглотитель визуального внимания в больших мультимодальных моделях" id="anchor-see-what-you-are-told-visual-attention-sink-in-large-multimodal-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Видеть то, что тебе говорят: Поглотитель визуального внимания в больших мультимодальных моделях</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2503.03321"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Видеть то, что тебе говорят: Поглотитель визуального внимания в больших мультимодальных моделях</div><div class="kg-bookmark-description">Большие мультимодальные модели (LMM) "видят" изображения, используя механизм внимания между текстом и визуальными токенами в декодере-трансформере. В идеале, эти модели должны фокусироваться на ключевой визуальной информации, относящейся к текстовому токену. Однако недавние исследования показывают, что LMM обладают необычайной тенденцией последовательно выделять высокие веса внимания определенным визуальным токенам, даже когда эти токены не имеют отношения к соответствующему тексту. В этом исследовании мы изучаем свойство, лежащее в основе появления этих нерелевантных визуальных токенов, и изучаем их характеристики. Наши результаты показывают, что такое поведение возникает из-за массивной активации определенных измерений скрытого состояния, что напоминает поглотитель внимания, обнаруженный в языковых моделях. Следовательно, мы называем это явление визуальным поглотителем внимания. В частности, наш анализ показывает, что удаление нерелевантных визуальных токенов-поглотителей не влияет на производительность модели, несмотря на получение высоких весов внимания. Следовательно, мы перерабатываем внимание к этим токенам как избыточные ресурсы, перераспределяя бюджет внимания для усиления фокуса на изображении. Для достижения этой цели мы представляем Visual Attention Redistribution (VAR), метод, который перераспределяет внимание в ориентированных на изображение головах, которые мы определяем как изначально ориентированные на визуальную информацию. VAR можно легко применять в различных LMM для повышения производительности в широком спектре задач, включая общие задачи обработки языка и изображений, задачи визуальных галлюцинаций и задачи, ориентированные на зрение, и все это без необходимости дополнительного обучения, моделей или этапов вывода. Экспериментальные результаты показывают, что VAR позволяет LMM более эффективно обрабатывать визуальную информацию, корректируя свои внутренние механизмы внимания, предлагая новое направление для улучшения мультимодальных возможностей LMM.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-31.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Seil Kang</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-27.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdrHjLu7qX5TOjtDH10svBjs-6rihxNRpgS3Bq9r8qtY9UvOC4LqyBo-NDWeESuRrv-vj6btANt6doA4IneaENN1712o3kzHhQwx20PR62b8JKDA5jIjCNgKAhXoCp9bEcbadyfPA?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Большие мультимодальные модели демонстрируют феномен, называемый "визуальным поглотителем внимания", когда они последовательно выделяют высокие веса внимания определенным визуальным токенам, которые не имеют отношения к соответствующим текстовым токенам. Эти нерелевантные визуальные токены возникают из-за массивной активации в определенных измерениях скрытого состояния, аналогично поглотителям внимания в языковых моделях. Метод Visual Attention Redistribution (VAR) определяет ориентированные на изображение головки внимания и перераспределяет бюджет внимания от токенов-поглотителей к значимому визуальному контенту, улучшая производительность в задачах обработки языка и изображений без необходимости дополнительного обучения.</p><p><strong>Основные выводы:</strong> </p><ul><li>Визуальные токены-поглотители можно идентифицировать по экстремальным значениям активации в фиксированных измерениях, унаследованных от базовых языковых моделей</li><li>Удаление визуальных токенов-поглотителей не влияет на производительность модели, несмотря на получение высоких весов внимания, что указывает на нерациональное использование вычислительных ресурсов</li><li>VAR перераспределяет внимание от токенов-поглотителей к значимому визуальному контенту, улучшая производительность в общих задачах обработки языка и изображений, снижая галлюцинации и улучшая задачи, ориентированные на зрение</li></ul><h2 id="towards-semantic-equivalence-of-tokenization-in-multimodal-llm" style="position: relative;"><a href="#towards-semantic-equivalence-of-tokenization-in-multimodal-llm" title="К семантической эквивалентности токенизации в мультимодальных LLM" id="anchor-towards-semantic-equivalence-of-tokenization-in-multimodal-llm"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>К семантической эквивалентности токенизации в мультимодальных LLM</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2406.05127"><div class="kg-bookmark-content"><div class="kg-bookmark-title">К семантической эквивалентности токенизации в мультимодальных LLM</div><div class="kg-bookmark-description">Мультимодальные большие языковые модели (MLLM) продемонстрировали исключительные возможности в обработке задач обработки языка и изображений. Одним из важнейших аспектов MLLM является визуальная токенизация, которая включает в себя эффективное преобразование входных визуальных сигналов в представления признаков, которые наиболее полезны для LLM. Однако существующие визуальные токенизаторы, необходимые для семантического выравнивания между зрением и языком, остаются проблематичными. Существующие методы агрессивно фрагментируют визуальный ввод, нарушая визуальную семантическую целостность. Чтобы решить эту проблему, в этой статье предлагается новый динамический Semantic-Equivalent Vision Tokenizer (SeTok), который группирует визуальные признаки в семантические единицы с помощью динамического алгоритма кластеризации, гибко определяя количество токенов на основе сложности изображения. Полученные визуальные токены эффективно сохраняют семантическую целостность и захватывают как низкочастотные, так и высокочастотные визуальные признаки. Предлагаемая MLLM (Setokim), оснащенная SeTok, значительно демонстрирует превосходную производительность в различных задачах, о чем свидетельствуют наши экспериментальные результаты. Страница проекта находится по адресу https://chocowu.github.io/SeTok-web/.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-32.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Shengqiong Wu</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-28.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdXtLgOhJPcWaPKTtGm7D3lK6tc7EhPQGXleGMHMqYG_KFVTxCSBGOd8z6xovad6UMgDjTWPBFfKqD4J2gSD6L6YXpSaTlGNNrLWiViAlfPkKinc9jNjsD2Ulnrh0tZQ74RR62tvQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="624" height="468" style="cursor: help;"></figure><p>Традиционные методы визуальной токенизации в мультимодальных LLM фрагментируют визуальный ввод с использованием фиксированных патчей, нарушая семантическую целостность и приводя к плохому выравниванию языка и изображений. SeTok (Semantic-Equivalent Vision Tokenizer) решает эту проблему с помощью динамической кластеризации, которая группирует визуальные признаки в согласованные семантические единицы, при этом количество токенов адаптируется к сложности изображения. Система использует двойные цели обучения: контрастное обучение для семантического выравнивания с языком и реконструктивное обучение для сохранения деталей на уровне пикселей для реконструкции изображения.</p><p><strong>Основные выводы:</strong> </p><ul><li>Токенизация с фиксированным патчем нарушает визуальную семантическую целостность, фрагментируя объекты по произвольным границам патчей</li><li>Алгоритмы динамической кластеризации могут адаптивно определять оптимальное количество токенов на основе семантической сложности изображения, а не фиксированных структур сетки</li><li>Обучение с двойной целью уравновешивает семантическое выравнивание с языком, сохраняя при этом достаточную визуальную детализацию для задач реконструкции</li></ul><h2 id="hymba-a-hybrid-head-architecture-for-small-language-models" style="position: relative;"><a href="#hymba-a-hybrid-head-architecture-for-small-language-models" title="Hymba: Гибридная архитектура голов для небольших языковых моделей" id="anchor-hymba-a-hybrid-head-architecture-for-small-language-models"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Hymba: Гибридная архитектура голов для небольших языковых моделей</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2411.13676"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Hymba: Гибридная архитектура голов для небольших языковых моделей</div><div class="kg-bookmark-description">Мы предлагаем Hymba, семейство небольших языковых моделей, характеризующихся гибридной параллельной архитектурой голов, которая объединяет механизмы внимания трансформера с моделями пространства состояний (SSM) для повышения эффективности. Головки внимания обеспечивают высокое разрешение припоминания, в то время как головки SSM обеспечивают эффективное обобщение контекста. Кроме того, мы представляем обучаемые мета-токены, которые добавляются к подсказкам, храня критическую информацию и облегчая бремя «принудительного внимания», связанное с механизмами внимания. Эта модель дополнительно оптимизирована за счет включения кросс-слойного совместного использования ключей-значений (KV) и частичного скользящего окна внимания, что приводит к компактному размеру кеша. В ходе разработки мы провели контролируемое исследование, сравнивающее различные архитектуры в идентичных условиях, и наблюдали значительные преимущества предложенной нами архитектуры. Примечательно, что Hymba достигает самых современных результатов для небольших LM: Наша модель Hymba-1.5B-Base превосходит все общедоступные модели до 2B по производительности и даже превосходит Llama-3.2-3B со средней точностью на 1,32% выше, уменьшением размера кеша в 11,67 раза и пропускной способностью в 3,49 раза.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina-ai-gmbh.ghost.io/content/images/icon/apple-touch-icon-33.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Xin Dong</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/arxiv-logo-fb-29.png" alt="" onerror="this.style.display = 'none'" style="cursor: help;"></div></a></figure><figure class="kg-card kg-image-card"><img loading="lazy" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdqTXEnWvnqhnbdUKsRw-mZ8hzuqwIbX_dnFwiY4BHa8DF4ViWiekeIRVlRBtQkJF8a2EPv5U_H5kvqxFQfCg0jGplWefzce1RHzHBd17D93k6DpE3vNurR0Ufg7kMEJ_C4IeDBZQ?key=jcOajfpjEtGsUeEc3rEhlw" class="kg-image" alt="" width="552" height="573" style="cursor: help;"></figure><p>Hymba представляет гибридную архитектуру голов, которая объединяет механизмы внимания трансформера с моделями пространства состояний (SSM) параллельно внутри каждого слоя, обеспечивая одновременное припоминание с высоким разрешением и эффективное обобщение контекста. Архитектура включает в себя обучаемые мета-токены, кросс-слойное совместное использование ключей-значений и частичное скользящее окно внимания для достижения компактных размеров кеша. Hymba-1.5B превосходит все модели до 2B и превосходит Llama-3.2-3B, обеспечивая при этом уменьшение кеша в 11,67 раза и улучшение пропускной способности в 3,49 раза.</p><p><strong>Основные выводы:</strong> </p><ul><li>Параллельная гибридная архитектура голов превосходит последовательное стекирование компонентов внимания и SSM, позволяя одновременно обрабатывать дополнительные механизмы</li><li>Обучаемые мета-токены действуют как сжатые мировые знания и облегчают бремя «принудительного внимания» механизмов внимания softmax</li><li>Оптимизации кросс-слойного совместного использования ключей-значений и скользящего окна внимания позволяют значительно уменьшить размер кеша без ущерба для производительности</li></ul></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Категории:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Событие</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fru%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fru%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fru%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fru%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fru%2Fnews%2Fwhat-we-learned-at-iclr2025%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Офисы</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Саннивейл, Калифорния</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Саннивейл, Калифорния 94085, США</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Берлин, Германия (штаб-квартира)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Берлин, Германия</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Пекин, Китай</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Уровень 5, здание 6, ул. Хайдянь Вест, д. 48, Пекин, Китай</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Шэньчжэнь, Китай</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">402, этаж 4, здание Fu'an Technology, Шэньчжэнь, Китай</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Поиск Фонда</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Читатель</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Вложения</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Реранкер</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Глубокий поиск</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Классификатор</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Сегментатор</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">API-документация</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Получить API-ключ Jina</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Ограничение скорости</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Статус API</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Компания</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">О нас</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Связаться с отделом продаж</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">отдел новостей</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Стажерская программа</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Присоединяйтесь к нам</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Скачать логотип</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Условия</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Безопасность</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Условия использования</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Конфиденциальность</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Управление файлами cookie</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
<!DOCTYPE html><html translate="no" dir="ltr" lang="en-US"><head><title>What Should We Learn From ModernBERT?</title><meta charset="utf-8"><meta name="title" content="What Should We Learn From ModernBERT?"><meta name="description" content="Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="og:title" content="What Should We Learn From ModernBERT?"><meta property="og:description" content="Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models."><meta property="og:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-should-we-learn-from-modernbert"><meta property="twitter:title" content="What Should We Learn From ModernBERT?"><meta property="twitter:description" content="Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-CEkaW1pY.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-Coy5UueA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-DRg-ruCs.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-DpdKaelR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-CHZcOyep.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-D8a-mgKP.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-CasnIi-x.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-BycRk3RX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-DFn8uV5o.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-dGFNDIZq.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-whYbMluJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-D1L3QaC7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-DcmygGzz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-WyzsUU9g.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-EHgmgr09.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-m3FazEu5.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-CPT0FSkR.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-C00zkw9p.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-B-5EgDX3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-XOYo5Q2-.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-DpYKigSF.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-DFyh3kl_.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-053V-Mi_.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-F_ldUg_q.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-DEdo3Pfu.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-CuCHcjuE.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-C93aLNEz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-BimLjFM3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-VSz9qRrO.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DO9vo9XY.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-BB5fsfNn.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-BmH_I2hz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-CKQdPIji.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-BpcHjdtp.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-D7_DPGhU.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-BE6GOLey.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-vSZoRHSY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Nan Wang, Alex C-G"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Nan Wang, Alex C-G"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="10 mins read"><meta property="article:published_time" content="2025-01-22T08:31:26.000+01:00"><meta property="article:modified_time" content="2025-01-22T19:12:10.000+01:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "What Should We Learn From ModernBERT?",
  "description": "Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.",
  "image": [
    "https://jina.ai/blog-banner/what-should-we-learn-from-modernbert.webp"
  ],
  "datePublished": "2025-01-22T08:31:26.000+01:00",
  "dateModified": "2025-01-22T19:12:10.000+01:00",
  "author": [
    {
      "@type": "Person",
      "name": "Nan Wang",
      "url": "https://jina-ai-gmbh.ghost.io/author/nan/"
    },
    {
      "@type": "Person",
      "name": "Alex C-G",
      "url": "https://jina-ai-gmbh.ghost.io/author/alexcg/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-85e17eff="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-85e17eff="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-85e17eff="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-85e17eff="" class="q-space"></div><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-85e17eff="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-85e17eff="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-85e17eff="" class="q-list q-list--dark" role="list"><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">News</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Models</div></a><div data-v-85e17eff="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_f5f35cb5-94f2-4144-ae77-96f8e587038c" aria-label="Expand &quot;Products&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Products</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_f5f35cb5-94f2-4144-ae77-96f8e587038c" style="display: none;"><div data-v-85e17eff="" class="q-list q-list--dark" role="list" label="Products"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">DeepSearch</div><div class="q-item__label q-item__label--caption text-caption">Search, read and reason until best answer found.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reader</div><div class="q-item__label q-item__label--caption text-caption">Convert any URL to Markdown for better grounding LLMs.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Embeddings</div><div class="q-item__label q-item__label--caption text-caption">World-class multimodal multilingual embeddings.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reranker</div><div class="q-item__label q-item__label--caption text-caption">World-class neural retriever for maximizing search relevancy.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_8bcfe4b3-d2d3-4309-a607-a63c3e4759c9" aria-label="Expand &quot;More&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">More</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_8bcfe4b3-d2d3-4309-a607-a63c3e4759c9" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classifier</div><div class="q-item__label q-item__label--caption text-caption">Zero-shot and few-shot classification for image and text.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmenter</div><div class="q-item__label q-item__label--caption text-caption">Cut long text into chunks and do tokenization.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">API Docs</div><div class="q-item__label q-item__label--caption text-caption">Auto codegen for your copilot IDE or LLM</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-85e17eff="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_3010dc85-b229-496f-aabd-200f146aa70b" aria-label="Expand &quot;Company&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Company</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_3010dc85-b229-496f-aabd-200f146aa70b" style="display: none;"><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">About us</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Join us</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Download logo</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Log in"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Log in</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-85e17eff="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_ed46471c-e8e4-495f-bff8-ccbc312ad364"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_ed46471c-e8e4-495f-bff8-ccbc312ad364" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_ed46471c-e8e4-495f-bff8-ccbc312ad364_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-85e17eff="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ModernBERT's Parameter-Efficiency</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ModernBERT's Code Modeling</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">ModernBERT's Long-Context Handling</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">The Bitter Lesson?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Conclusion</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech blog</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">January 22, 2025</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">What Should We Learn From ModernBERT?</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img" aria-label="Futuristic illustration with a central white circle surrounded by white dots on a dotted background."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Futuristic illustration with a central white circle surrounded by white dots on a dotted background." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/modernbert-banner.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Nan Wang"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Nan Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/avartar_2024.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Nan Wang, Alex C-G • 10 minutes read</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><p>Back in 2018, Google dropped BERT and it was a game-changer for NLP, way before the current LLM wave. Even now, tons of Small Language Models are built on BERT. In Dec. 2024, <a href="https://huggingface.co/blog/modernbert" rel="noreferrer">ModernBERT</a> takes what we've learned from recent LLM developments and applies it back to these smaller models. The key moves? Better parameter-efficiency, code understanding and long-context handling.</p><p>In this post, we'll break down how ModernBERT stacks up against two models we know inside and out: <code>jina-XLM-RoBERTa</code> (the multilingual backbone behind <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>) and <code>RobERTa-large</code>. Let's look at each model:</p><ul><li><strong>ModernBERT </strong>(Dec. 2024) is a recently released SLM, developed collaboratively by Answer.AI, LightOn, and HuggingFace. It leverages modern optimizations like RoPE for an 8,192-token context window and <a href="https://arxiv.org/abs/2002.05202">GeGLU layers</a>, boosting performance while maintaining efficiency.</li><li><a href="https://huggingface.co/jinaai/xlm-roberta-flash-implementation"><strong><code>jina-XLM-RoBERTa</code></strong></a><strong> </strong>(Sept. 2024) is a multilingual text embedding model based on Meta's <a href="https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta"><code>XLM-RoBERTa</code></a>. While the original <code>XLM-RoBERTa</code> enhances <code>RoBERTa</code> using the XLM large multilingual dataset, <code>jina-XLM-RoBERTa</code> takes it further with extended context training, <a href="https://arxiv.org/abs/2104.09864">RoPE</a> implementation, and <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a> support. This model serves as the backbone for <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a>.</li><li><a href="https://huggingface.co/FacebookAI/roberta-large"><strong><code>RoBERTa-large</code></strong></a> (July 2019) developed by Meta, is an enhanced version of BERT with 355 million parameters. Through extended training, larger datasets, and innovations like dynamic masking, it has achieved impressive results on key benchmarks including <a href="https://gluebenchmark.com/">GLUE</a>, <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a>, and <a href="https://arxiv.org/abs/1704.04683">RACE</a>. This makes it well-suited for various NLP tasks from text classification to question answering.</li></ul><p>By comparing these models across three core aspects, we aim to highlight ModernBERT's effective design choices for fellow model developers and identify key development insights for future BERT-like models. We'll also share our learnings from developing <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a> and discuss planned improvements for <code>jina-embeddings-v4</code> and <code>jina-reranker-v3</code>.</p><h2 id="modernberts-parameter-efficiency" style="position: relative;"><a href="#modernberts-parameter-efficiency" title="ModernBERT's Parameter-Efficiency" id="anchor-modernberts-parameter-efficiency"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ModernBERT's Parameter-Efficiency</h2><p>Let's first look at ModernBERT's approach to parameter-efficiency - it's bringing in several key insights from recent LLM developments. ModernBERT leverages three core strategies: a deeper but thinner architecture, controlled vocabulary size, and progressive model upscaling from smaller models.</p><h3 id="deep-and-thin-architecture" style="position: relative;"><a href="#deep-and-thin-architecture" title="Deep-And-Thin Architecture" id="anchor-deep-and-thin-architecture"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Deep-And-Thin Architecture</h3><p>ModernBERT-large goes deeper with 28 layers, while <code>jina-XLM-RoBERTa</code> and <code>RoBERTa-large</code> run at 24. But here's the interesting part - it matches <code>RoBERTa-large</code> in parameter count despite those extra layers. <code>jina-XLM-RoBERTa</code> needs more parameters since it's handling 89 languages, while the other two focus just on English.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark-architecture-outlines-1.svg" class="kg-image" alt="Comparative chart with &quot;Deep-and-thin&quot; vs &quot;Shallow-and-fat&quot; in red and yellow bars against a black background." width="1389" height="547" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">The depth (number of layers) is more important than width (number of hidden units) for small LLMs. A deep-and-thin model structure excels in capturing abstract concepts, resulting in superior final performance.</span></figcaption></figure><p>Most of a transformer's parameters comes from attention and fully-connected layers. ModernBERT stays competitive size-wise by going "thinner" - they're running 2,624 hidden units across 28 layers, compared to RoBERTa-large's 4,096 units across 24 layers. This "deeper" but thinner setup is letting them hit their performance targets without bloating the model. </p>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameters</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Hidden states</td>
<td>1,024</td>
<td>1,024</td>
<td>1,024</td>
</tr>
<tr>
<td>Intermediate dims</td>
<td>2,624</td>
<td>4,096</td>
<td>4,096</td>
</tr>
<tr>
<td>Attention heads</td>
<td>16</td>
<td>16</td>
<td>16</td>
</tr>
<tr>
<td>Layers</td>
<td>28</td>
<td>24</td>
<td>24</td>
</tr>
<tr>
<td>Vocabulary size</td>
<td>50,368</td>
<td>250,002</td>
<td>50,265</td>
</tr>
</tbody>
</table>

<p>This approach lines up with Meta's <a href="https://openreview.net/pdf?id=EIGbXbxcUQ">MobileLLM</a> research, which found that for smaller models, depth matters more than width when it comes to capturing complex patterns and driving performance. Essentially, the ability to process information through more transformer layers proves more valuable than having wider layers for parallel processing.</p><p>Let's look at the data on how this deep-and-thin architecture performs.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/performance_comparison_general.v3.svg" class="kg-image" alt="Bar chart evaluating NLP model performances, including ModernBERT and ROBERTa, with a scale from 0 to 100." width="872" height="371" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Put it up against comparable models using the traditional shallow-fat architecture, and ModernBERT is delivering better results on key tasks like retrieval and STS - all while keeping the parameter count similar.</span></figcaption></figure>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>STS12</td>
<td>72.6</td>
<td><strong>72.7</strong></td>
<td>68.9</td>
</tr>
<tr>
<td>STS13</td>
<td><strong>84.9</strong></td>
<td>83.9</td>
<td>81.0</td>
</tr>
<tr>
<td>STS14</td>
<td>77.5</td>
<td><strong>77.7</strong></td>
<td>74.8</td>
</tr>
<tr>
<td>STS15</td>
<td>84.8</td>
<td><strong>85.8</strong></td>
<td>84.1</td>
</tr>
<tr>
<td>STS16</td>
<td>79.4</td>
<td><strong>79.6</strong></td>
<td>78.6</td>
</tr>
<tr>
<td>STS17</td>
<td><strong>87.5</strong></td>
<td>87.2</td>
<td>87.2</td>
</tr>
<tr>
<td>TRECCOVID</td>
<td><strong>61.1</strong></td>
<td>59.6</td>
<td>49.3</td>
</tr>
<tr>
<td>FiQA</td>
<td><strong>44.4</strong></td>
<td>40.0</td>
<td>40.7</td>
</tr>
<tr>
<td>NFCorpus</td>
<td><strong>32.6</strong></td>
<td>30.6</td>
<td>27.9</td>
</tr>
<tr>
<td>SciFact</td>
<td><strong>68.6</strong></td>
<td>65.5</td>
<td>63.1</td>
</tr>
<tr>
<td>Average</td>
<td><strong>69.3</strong></td>
<td>68.2</td>
<td>65.6</td>
</tr>
</tbody>
</table>

<p>Take <code>jina-XLM-RoBERTa</code> - it builds on <code>RoBERTa-large</code>'s shallow-fat architecture but pumps up the vocabulary from 50K to 250K tokens and trains on more data. Yet ModernBERT still edges it out, suggesting the architectural shift is making a real difference in efficiency.</p><h3 id="vocabulary-size-matters" style="position: relative;"><a href="#vocabulary-size-matters" title="Vocabulary Size Matters" id="anchor-vocabulary-size-matters"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Vocabulary Size Matters</h3><p>First, let's understand how vocabulary parameters is counted in transformers. For any transformer, <code>vocabulary parameters = number of distinct tokens × hidden size</code>. Take <code>jina-XLM-RoBERTa</code>: with 250K tokens and 1,024 dimensions, it needs 256M parameters just for vocabulary encoding - before handling any actual language tasks!</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/tokenizer-dark-outline.svg" class="kg-image" alt="Diagram illustrating a transformer model process: tokenization of text &quot;hello, jina&quot;, assigning of vocabulary weights, and la" width="3757" height="715" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">In transformers, the first layer maps tokens to hidden states using a weight matrix, namely the vocabulary weights. Consider using all UTF-8 code points (1,112,064) with 1,024 hidden dimensions - you'd need a massive </span><code spellcheck="false" style="white-space: pre-wrap;"><span>1,112,064 × 1,024 = 1 B</span></code><span style="white-space: pre-wrap;"> parameters just for token conversion. While larger LLMs (100B+ parameters) can handle this overhead, it's a serious constraint for smaller models. This is exactly why we use tokenizers like BPE, which efficiently merge common UTF-8 code points into single tokens.</span></figcaption></figure><p>But here is the thing: <strong>vocabulary weights don't contribute to attention mechanisms - they're just lookup tables.</strong> For SLMs working with fixed parameter budgets, a larger vocabulary means fewer parameters available for attention layers, which do the actual language processing. This explains why English-only ModernBERT-large outperforms multilingual <code>jina-XLM-RoBERTa</code> despite being smaller - <code>jina-XLM-RoBERTa</code> allocates more parameters (47%!) to support multiple languages. ModernBERT's focused vocabulary not only improves performance but also speeds up inference, making it particularly effective for resource-constrained applications.</p><p>So now if we look at <em>just</em> the core model parameters (excluding vocabulary weights), ModernBERT actually packs more computational power than its peers: ModernBERT dedicates 19% more parameters to <em>actual</em> language modeling than <code>jina-XLM-RoBERTa</code> and 15% more than <code>RoBERTa-large</code>!</p>

<table>
<thead>
<tr>
<th>Model Specs</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Language Support</td>
<td>English Only</td>
<td>89 Languages</td>
<td>English Only</td>
</tr>
<tr>
<td>Vocab Size</td>
<td>50.4K</td>
<td>250K</td>
<td>50.3K</td>
</tr>
<tr>
<td>Total Params</td>
<td>400M</td>
<td>550M</td>
<td>355M</td>
</tr>
<tr>
<td>Vocab Params</td>
<td>51M</td>
<td>256M</td>
<td>51M</td>
</tr>
<tr>
<td>Vocab Param Ratio</td>
<td>13%</td>
<td>47%</td>
<td>14%</td>
</tr>
<tr>
<td>Core Model Params</td>
<td><b>349M</b></td>
<td>294M</td>
<td>304M</td>
</tr>
</tbody>
</table>

<h3 id="model-upscaling-by-weight-tiling" style="position: relative;"><a href="#model-upscaling-by-weight-tiling" title="Model Upscaling by &quot;Weight Tiling&quot;" id="anchor-model-upscaling-by-weight-tiling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Model Upscaling by "Weight Tiling"</h3><p>In building <a href="https://huggingface.co/jinaai/jina-bert-implementation"><code>jina-BERT-v2</code></a> backbone, we found training SLMs from scratch was resource-intensive and complex. ModernBERT tackles this with a smart initialization approach called <strong>weight tiling</strong> - essentially bootstrapping ModernBERT-large from the weights of its smaller base version.</p><p>This technique isn't entirely new - it builds on DeepMind's work with <a href="https://gpt3demo.com/apps/deepmind-gopher">Gopher</a> and shows up in Microsoft's <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2 models</a> too. But its application here is particularly effective for addressing the SLM training bottleneck.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/deep_and_thin-dark.svg" class="kg-image" alt="Diagram comparing weight initialization in small vs. large models with labeled sections and steps for layer initialization." width="1877" height="1308" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT scales from 22 to 28 layers using the Gopher team's depth initialization strategy. For those extra layers (23-28), they initialize each one using weights from the original 22 layers of ModernBERT-base. For each layer's weight matrices, they use Phi-2's center tiling approach. It works like this: they take the ModernBERT-base weights and drop them right in the middle of ModernBERT-large's matrices. For the edges that are still empty? They wrap the original weights around cyclically to fill them in.</span></figcaption></figure><p>This initialization strategy gives ModernBERT-large a significant advantage - instead of starting cold from scratch, it leverages pre-learned patterns from its smaller counterpart. It's proven particularly <a href="https://arxiv.org/pdf/2112.11446">effective for scaling up language models in this size range</a>.</p><blockquote>We find that a warm started model rapidly recovers from a high initial loss (due to the added parameters) to a loss quite close to that of the base model. We are able to expand 417M parameters by over 3× in size and maintain performance greater than an equivalent fresh model trained from scratch to convergence, implying that the gains were not limited to the start of training. However, at larger sizes, the relative gains achieved at convergence diminish, especially with expansions in width.</blockquote><p>The cyclical weight wrapping isn't just a convenience - it aligns well with how attention matrices naturally exhibit periodic patterns. Gopher's research shows this approach really shines for SLMs (sub-9B parameters), though the benefits start tapering off as you move into larger model territories.</p><h2 id="modernberts-code-modeling" style="position: relative;"><a href="#modernberts-code-modeling" title="ModernBERT's Code Modeling" id="anchor-modernberts-code-modeling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ModernBERT's Code Modeling</h2><p>ModernBERT optimizes for code understanding with its code-friendly tokenizer and training data. This pays off specifically in retrieval tasks. We ran a benchmark on <a href="https://github.com/github/CodeSearchNet">CodeSearchNet</a> - matching text descriptions to code snippets. ModernBERT outperformed both alternatives across the board.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_search_net.v3.svg" class="kg-image" alt="Vertical bar chart comparing performance metrics of AI models like ModernBERT-large on CodeSearchNet, with rising bars for sc" width="787" height="489" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">The gap makes sense - neither </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> nor </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;"> saw programming languages during training. Meanwhile, ModernBERT-large trained on two trillion tokens, including a substantial amount of code. This exposure to programming syntax and patterns gives it a clear advantage in code-related tasks. </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-XLM-RoBERTa</span></code><span style="white-space: pre-wrap;"> edges out </span><code spellcheck="false" style="white-space: pre-wrap;"><span>RoBERTa-large</span></code><span style="white-space: pre-wrap;">, likely due to its larger multilingual training data - same architecture, more exposure. Nonetheless, both trail ModernBERT-large significantly.</span></figcaption></figure>

<table>
<thead>
<tr>
<th>Task</th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
<th><code>RoBERTa-large</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>AdvRetrieval</td>
<td>0.342</td>
<td><strong>0.363</strong></td>
<td>0.331</td>
</tr>
<tr>
<td>QueryRetrieval.python</td>
<td>0.521</td>
<td><strong>0.530</strong></td>
<td>0.525</td>
</tr>
<tr>
<td>QueryRetrieval java</td>
<td><strong>0.679</strong></td>
<td>0.633</td>
<td>0.644</td>
</tr>
<tr>
<td>QueryRetrieval.javascript</td>
<td>0.755</td>
<td><strong>0.768</strong></td>
<td>0.732</td>
</tr>
<tr>
<td>QueryRetrieval.php</td>
<td><strong>0.815</strong></td>
<td>0.781</td>
<td>0.755</td>
</tr>
<tr>
<td>QueryRetrieval.ruby</td>
<td>0.729</td>
<td><strong>0.744</strong></td>
<td>0.722</td>
</tr>
<tr>
<td>QueryRetrieval.go</td>
<td><strong>0.833</strong></td>
<td>0.809</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.go</td>
<td><strong>0.778</strong></td>
<td>0.750</td>
<td>0.759</td>
</tr>
<tr>
<td>Retrieval.java</td>
<td><strong>0.840</strong></td>
<td>0.792</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.javascript</td>
<td><strong>0.817</strong></td>
<td>0.792</td>
<td>0.757</td>
</tr>
<tr>
<td>Retrieval.php</td>
<td><strong>0.852</strong></td>
<td>0.805</td>
<td>0.796</td>
</tr>
<tr>
<td>Retrieval.python</td>
<td><strong>0.849</strong></td>
<td>0.816</td>
<td>0.787</td>
</tr>
<tr>
<td>Retrieval.ruby</td>
<td><strong>0.849</strong></td>
<td>0.796</td>
<td>0.803</td>
</tr>
<tr>
<td>Avg.</td>
<td><strong>0.743</strong></td>
<td>0.721</td>
<td>0.708</td>
</tr>
</tbody>
</table>

<h3 id="the-tokenizer-edge" style="position: relative;"><a href="#the-tokenizer-edge" title="The Tokenizer Edge" id="anchor-the-tokenizer-edge"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>The Tokenizer Edge</h3><p>Let's dig into why ModernBERT handles code so well - it's using <a href="https://huggingface.co/docs/transformers/en/model_doc/olmo" rel="noreferrer">OLMo tokenizer</a>, which was specifically trained on code, rather than standard BERT/RoBERTa tokenizers.</p><p>A tokenizer breaks UTF-8 text into tokens that get mapped to vectors - these are what the model actually processes. During training, it learns to combine frequently occurring character sequences into single tokens. The difference? A standard tokenizer might break <code>init</code> into <code>in</code> + <code>it</code>, missing the programming context. But ModernBERT's code-aware tokenizer gets it without breaking it.</p><p>Here's where it gets interesting with space handling: ModernBERT preserves Python's leading spaces as single tokens and differentiates between 4 vs 8 spaces - crucial for code structure. Meanwhile, <strong><code>jina-XLM-RoBERTa</code> collapses all continuous spaces into a single <code>_</code>, and RoBERTa-large treats each space as its own token.</strong> This means ModernBERT's encoder gets cleaner, more meaningful input when processing code, while the others are working with fractured, less coherent tokens.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/code_tokens-cheat-2.svg" class="kg-image" alt="Hierarchical flowchart with tokenizers for text processing including ModernBERT-large, XLM-ROBERTa, and a large ROBERTa." width="3156" height="1247" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">ModernBERT preserves Python's leading spaces as single tokens and differentiates between 4 vs 8 spaces - crucial for code structure; while the others are working with fractured, less coherent tokens.</span></figcaption></figure><h2 id="modernberts-long-context-handling" style="position: relative;"><a href="#modernberts-long-context-handling" title="ModernBERT's Long-Context Handling" id="anchor-modernberts-long-context-handling"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>ModernBERT's Long-Context Handling</h2><p>ModernBERT has made significant strides in processing long text, thanks to its extensive training corpus (300B tokens with 8,192-token samples) and advanced techniques like combined global and local attention.</p><p>To evaluate long-document handling capabilities, we used the <a href="https://huggingface.co/datasets/Shitao/MLDR">MLDR dataset</a> - a comprehensive long-text benchmark spanning 13 languages. Since ModernBERT currently supports only English, we focused on MLDR's English subset to benchmark ModernBERT against <code>jina-XLM-RoBERTa</code>. While both these models can handle 8K-token inputs, <code>RoBERTa-large</code> was excluded from this benchmark due to its 512-token limit, which is insufficient for long-text analysis.</p>

<table>
<thead>
<tr>
<th></th>
<th>ModernBERT-large</th>
<th><code>jina-XLM-RoBERTa</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>MLDR-en</td>
<td><strong>0.351</strong></td>
<td>0.290</td>
</tr>
</tbody>
</table>

<p>ModernBERT's superior performance isn't just due to its extensive long-text training - it's largely thanks to its innovative combination of global and local attention mechanisms. Unlike <code>jina-XLM-RoBERTa</code>, which applies computationally expensive global attention to every layer, ModernBERT takes a more efficient approach. It alternates between global attention (used every third layer with a <code>theta</code> of 160,000) and local attention (using a 128-token sliding window with a <code>theta</code> of 100,000). This hybrid strategy maintains high performance while dramatically reducing training time.</p><blockquote>In ModernBERT, every third layer employs global attention with a RoPE theta of 160,000 and the remaining layers use a 128 token, local sliding window attention with a RoPE theta of 10,000. —— <a href="https://arxiv.org/pdf/2412.13663">ModernBERT</a></blockquote><h2 id="the-bitter-lesson" style="position: relative;"><a href="#the-bitter-lesson" title="The Bitter Lesson?" id="anchor-the-bitter-lesson"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>The Bitter Lesson?</h2><p>The scaling law and <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">the bitter lesson</a> suggest that major performance improvements come primarily from increasing parameter counts and training data. This principle guided our approach of expanding the corpus and using LoRA for task-specific adaptations.</p><p>However, ModernBERT's success has revealed that we undervalued the power of architectural optimization. It demonstrates that SLMs can achieve exceptional results through better data-model efficiency, without necessarily scaling up parameters. A recent<a href="https://arxiv.org/pdf/2408.11868"> Stella Embeddings technical report</a> reinforces this finding, indicating that current embedding model training methods can be improved without increasing corpus or model size.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/09/plot--4-.svg" class="kg-image" alt="Graph showing Scaling Law of Embedding Models with 'Parameter Size' on the x-axis and 'MTEB Performance' on the y-axis, featu" width="949" height="949" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Scaling law of embedding models. The average MTEB performance on English tasks is plotted against the number of model parameters. Each dot represents an embedding model. The trendline, representing all models, is highlighted, with multilingual models emphasized in cyan. One can see that </span><a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v3" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v3</span></a><span style="white-space: pre-wrap;"> demonstrates superior performance compared to models of similar size, also showing a superlinear improvement over its predecessor, </span><code spellcheck="false" style="white-space: pre-wrap;"><span>jina-embeddings-v2</span></code><span style="white-space: pre-wrap;">. This graph was created by selecting top-100 embedding models from the MTEB leaderboard , excluding those without size information, typically closed-source or proprietary models. Submissions identified as obvious trolling were also filtered out. </span></figcaption></figure><p>Moving forward, we anticipate lower computational costs and smaller model sizes as we gain deeper insights into data utilization and implement ModernBERT's techniques. In the short term, we can implement straightforward improvements outlined in the ModernBERT paper - specifically integrating more code-related data and adopting a code-friendly tokenizer. More complex changes, like switching to a deep-and-thin architecture or bootstrapping large models from smaller ones, will require building backbone models from scratch - a more mid-term initiative.</p><p>While ModernBERT's efficiency is remarkable, its text-only limitation points to future challenges. As multimodal embedding models gain popularity, our next challenge is developing smarter, faster, and more capable search foundation models that can handle inputs for multimodal applications. These applications demand even longer context windows - an efficiency challenge that remains to be solved.</p><h2 id="conclusion" style="position: relative;"><a href="#conclusion" title="Conclusion" id="anchor-conclusion"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Conclusion</h2><p>Throughout this post, we've explored how ModernBERT advances BERT-family models through three key innovations: its deep-and-thin architecture, optimized tokenizer, and efficient scaling using weight tiling. These improvements enable ModernBERT to deliver outstanding performance in a relatively compact size, surpassing both <code>RoBERTa-large</code> and <code>jina-XLM-RoBERTa</code> across various tasks. ModernBERT demonstrates that architectural improvements can matter more than parameter size, opening doors for more efficient models. Its successful use of weight tiling shows how progressive scaling can reduce training costs while preserving or even boosting performance. Additionally, its compact vocabulary and targeted optimizations suggest growing opportunities for specialized SLMs in resource-limited settings.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Categories:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech blog</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fwhat-should-we-learn-from-modernbert%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-c36e4d4e="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-c36e4d4e="" class="text-h5 q-my-xl">Read more</div><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/snippet-selection-and-url-ranking-in-deepsearch-deepresearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">March 12, 2025 • 11 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Snippet Selection and URL Ranking in DeepSearch/DeepResearch</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Logo with words &quot;THINK SEARCH THINK&quot; in black dot and arrow patterns on an orange background, accompanied by horizontal lines"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Logo with words &quot;THINK SEARCH THINK&quot; in black dot and arrow patterns on an orange background, accompanied by horizontal lines" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/03/Heading--89-.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/long-context-embedding-models-are-blind-beyond-4k-tokens"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">March 07, 2025 • 14 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Long-Context Embedding Models are Blind Beyond 4K Tokens</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Saahil Ognawala"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Saahil Ognawala" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2023/03/profile-option-2.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Vertical repetition of the word 'HAYSTACK' with a solitary 'NEEDLE' on a yellowish background."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Vertical repetition of the word 'HAYSTACK' with a solitary 'NEEDLE' on a yellowish background." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/03/haystack.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/llm-as-serp-search-engine-result-pages-from-large-language-models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">February 27, 2025 • 5 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">LLM-as-SERP: Search Engine Result Pages from Large Language Models</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Screenshot of a search engine displaying results for 'Jina AI' featuring links to GitHub, Wikipedia, and other resources on a"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Screenshot of a search engine displaying results for 'Jina AI' featuring links to GitHub, Wikipedia, and other resources on a" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/02/llmserp-banner.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div data-v-85e17eff="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-85e17eff="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-85e17eff="" class="col-sm-12 col-md"><div data-v-85e17eff="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Offices</div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Sunnyvale, CA</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Berlin, Germany (HQ)</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Germany</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Beijing, China</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">Level 5, Building 6, No.48 Haidian West St. Beijing, China</div></div></div><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-85e17eff="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center"><div data-v-85e17eff="" class="q-item__label">Shenzhen, China</div><div data-v-85e17eff="" class="q-item__label q-item__label--caption text-caption text-dim">402 Floor 4, Fu'an Technology Building, Shenzhen, China</div></div></div></div></div><div data-v-85e17eff="" class="col-sm-12 col-md row"><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Search Foundation</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">DeepSearch</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Reader</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Embeddings</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Reranker</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Classifier</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Segmenter</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">API Documentation</div></a><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Get Jina API key</div></div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Rate Limit</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-85e17eff="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">API Status</div></a></div><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Company</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">About us</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Newsroom</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Join us</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Download logo</div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-85e17eff="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-85e17eff="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Terms</div><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Security</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Privacy</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--main justify-center">Manage Cookies</div></a><a data-v-85e17eff="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-85e17eff="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-85e17eff="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-85e17eff="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-85e17eff="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-85e17eff="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-85e17eff="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
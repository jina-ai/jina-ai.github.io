<!DOCTYPE html><html translate="no" dir="ltr" lang="en-US"><head><title>A Deep Dive into Tokenization</title><meta charset="utf-8"><meta name="title" content="A Deep Dive into Tokenization"><meta name="description" content="Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="og:title" content="A Deep Dive into Tokenization"><meta property="og:description" content="Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?"><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="twitter:title" content="A Deep Dive into Tokenization"><meta property="twitter:description" content="Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?"><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-CNxRRvi6.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-CRvJtbiE.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-CGhGwfvw.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-Eo0kYxJG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-CYHCFlwV.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-DhotFS0T.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-ChLcirF9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-DMKghpXJ.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-CkNZIIWP.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-CVzXx_xf.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-C6nB_MsM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-Dj149qtI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/PurchaseSuccessDialog-DDgOfsKA.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-C25DziD8.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-BKrks-BF.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-CLZgXYWd.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-BSdNc5SB.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-DNkcIkB0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-DrltYmzX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-BhePeLtz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-C-i1SBsM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-ByCgy1Is.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-DZ18pgNg.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-DEBJMlxe.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-CTktk7iy.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-Cva-Yr8q.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-vfk0IlQM.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-CaVlSg2N.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-BW8my4G4.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-B_CyKxBv.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-DCosUcCG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-C-w_BjVp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-DZoP6Cf6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-DEYsjdez.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-CXa-WjDj.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-C_TQJU5F.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-CbLKrWCf.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Scott Martens"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-01-31T16:10:14.000+01:00"><meta property="article:modified_time" content="2024-08-14T11:38:01.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "A Deep Dive into Tokenization",
  "description": "Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?",
  "image": [
    "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"
  ],
  "datePublished": "2024-01-31T16:10:14.000+01:00",
  "dateModified": "2024-08-14T11:38:01.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div class="q-space"></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">News</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Models</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_928c68d3-5f25-4fff-ab13-6f608afdf4b0" aria-label="Expand &quot;Products&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Products</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_928c68d3-5f25-4fff-ab13-6f608afdf4b0" style="display: none;"><div class="q-list q-list--dark" role="list" label="Products"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reader</div><div class="q-item__label q-item__label--caption text-caption">Convert any URL to Markdown for better grounding LLMs.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Embeddings</div><div class="q-item__label q-item__label--caption text-caption">World-class multimodal multilingual embeddings.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reranker</div><div class="q-item__label q-item__label--caption text-caption">World-class neural retriever for maximizing search relevancy.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classifier</div><div class="q-item__label q-item__label--caption text-caption">Zero-shot and few-shot classification for image and text.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmenter</div><div class="q-item__label q-item__label--caption text-caption">Cut long text into chunks and do tokenization.</div></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_eb8f124d-6394-4c02-ae4c-1e8fbb1af8c0" aria-label="Expand &quot;Company&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Company</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_eb8f124d-6394-4c02-ae4c-1e8fbb1af8c0" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">About us</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Join us</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Download logo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Log in"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Log in</div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_8a2d00ad-a3b6-4bb4-851f-6d7e4f9943bf"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_8a2d00ad-a3b6-4bb4-851f-6d7e4f9943bf" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_8a2d00ad-a3b6-4bb4-851f-6d7e4f9943bf_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 56px;"><main data-v-d0fd5afa="" class="q-page" style="min-height: 100vh;"><div data-v-d0fd5afa="" class="row full-width relative-position justify-end"><div data-v-d0fd5afa="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-d0fd5afa="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">tl;dr</div></div></div><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">Words, Tokens, Numbers</div></div></div><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">Mapping Language to Numbers</div></div></div><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">Why Do We Tokenize? And Why This Way?</div></div></div><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">Empirical Estimates of Token Output Sizes</div></div></div><div data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-d0fd5afa="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-d0fd5afa="" class="q-item__label">Taking Tokens Seriously</div></div></div></div></div><div data-v-d0fd5afa="" class="col-12 col-md-10 col-lg-12"><div data-v-d0fd5afa="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech blog</div></div></div></div><div data-v-d0fd5afa="" class="row justify-center"><div data-v-d0fd5afa="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-d0fd5afa="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">January 31, 2024</div><h1 data-v-d0fd5afa="" class="text-weight-medium text-center q-px-md my-title">A Deep Dive into Tokenization</h1><div data-v-d0fd5afa="" class="col row justify-center"><div data-v-d0fd5afa="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Tokenization, in LLMs, means chopping input texts up into smaller parts for processing. So why are embeddings billed by the token?</div></div><div data-v-d0fd5afa="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-d0fd5afa="" class="q-img q-img--menu" role="img" aria-label="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-d0fd5afa="" class="row justify-center"><div data-v-d0fd5afa="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-d0fd5afa="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-d0fd5afa="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-d0fd5afa="" class="q-item__label">Scott Martens • 16 minutes read</div></div></div></div><div data-v-d0fd5afa="" class="row justify-center"><div data-v-d0fd5afa="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-d0fd5afa="" class="article"><section data-v-d0fd5afa="" class="gh-content"><p>There are a lot of barriers to understanding AI models, some of them pretty big barriers, and they can stand in the way of implementing AI processes. But the first one many people encounter is understanding what we mean when talking about <strong>tokens</strong>. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/tokenizer"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Tokenizer API</div><div class="kg-bookmark-description">Free API to tokenize texts, count and get first/last-N tokens.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-tokenize-api.png" alt="" style="cursor: help;"></div></a></figure><p>One of the most important practical parameters in choosing an AI language model is the size of its context window — the maximum input text size — which is given in tokens, not words or characters or any other automatically recognizable unit.</p><p>Furthermore, embedding services are typically figured “per token,” meaning tokens are important to understanding your bill.</p><p>This can be very confusing if you aren’t clear about what a token is. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="Jina Embeddings current price sheet (as of February 2024)." width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Jina Embeddings current price sheet (as of February 2024). Note that prices are stated per “1M tokens”.</span></figcaption></figure><p>But of all the confusing aspects of modern AI, tokens are probably the least complicated. This article will try to clarify what tokenization is, what it does, and why we do it that way.</p><h2 id="tldr" style="position: relative;"><a href="#tldr" title="tl;dr" id="anchor-tldr"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>tl;dr</h2><p>For those who want or need a quick answer to figure out how many tokens to buy from Jina Embeddings or an estimate of how many they need to expect to buy, the following statistics are what you're looking for.</p><h3 id="tokens-per-english-word" style="position: relative;"><a href="#tokens-per-english-word" title="Tokens per English Word" id="anchor-tokens-per-english-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens per English Word</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">A call to the Jina Embeddings v2 API for English models will use <b><strong style="white-space: pre-wrap;">approximately</strong></b> <b><strong style="white-space: pre-wrap;">10% more</strong></b> tokens than the number of words in your text, <b><strong style="white-space: pre-wrap;">plus two tokens per embedding</strong></b>.</div></div><p>During empirical testing, described further down in this article, a variety of English texts converted into tokens at a rate of about 10% more tokens than words, using Jina Embeddings English-only models. This result was pretty robust. </p><p>Jina Embeddings v2 models have a context window of 8192 tokens. This means that if you pass a Jina model an English text longer than 7,400 words, there is a good chance it will be truncated.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">The maximum size for input to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">7,400 words</strong></b>.</div></div><h3 id="tokens-per-chinese-character" style="position: relative;"><a href="#tokens-per-chinese-character" title="Tokens per Chinese Character" id="anchor-tokens-per-chinese-character"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens per Chinese Character</h3><p>For Chinese, results are more variable. Depending on the text type, ratios varied from 0.6 to 0.75 tokens per Chinese character (汉字). English texts given to Jina Embeddings v2 for Chinese produce approximately the same number of tokens as Jina Embeddings v2 for English: roughly 10% more than the number of words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">The maximum size for input in Chinese to <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 for Chinese and English</strong></b> is approximately <b><strong style="white-space: pre-wrap;">10,500 characters</strong></b> (<b><strong style="white-space: pre-wrap;">字数</strong></b>), or <b><strong style="white-space: pre-wrap;">0.6 to 0.75 tokens per Chinese character, plus two per embedding.</strong></b></div></div><h3 id="tokens-per-german-word" style="position: relative;"><a href="#tokens-per-german-word" title="Tokens per German Word" id="anchor-tokens-per-german-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens per German Word</h3><p>German word-to-token ratios are more variable than English but less than Chinese. Depending on the genre of the text, I got 20% to 30% more tokens than words on average. Giving English texts to Jina Embeddings v2 for German and English uses a few more tokens than the English-only and Chinese/English models: 12% to 15% more tokens than words.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Jina Embeddings v2 for German and English will count <b><strong style="white-space: pre-wrap;">20% to 30% more tokens than words, plus two per embedding</strong></b>. The maximum size of the input context is approximately <b><strong style="white-space: pre-wrap;">6,300 German words</strong></b>.</div></div><h3 id="caution" style="position: relative;"><a href="#caution" title="Caution!" id="anchor-caution"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Caution!</h3><p>These are simple calculations, but they should be approximately right for most natural language texts and most users. Ultimately, we can only promise that the number of tokens will always be no more than the number of characters in your text, plus two. It will practically always be much less than that, but we cannot promise any specific count in advance.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Your Mileage May Vary! </strong></b><br><br>These are estimates based on statistically naive calculations. We do not guarantee how many tokens any particular request will take.</div></div><p>If all you need is advice on how many tokens to buy for Jina Embeddings, you can stop here. Other embedding models, from companies other than Jina AI, may not have the same token-to-word and token-to-Chinese-character ratios Jina models have, but they will not generally be very different overall.</p><p>If you want to understand why, the rest of this article is a deeper dive into tokenization for language models.</p><h2 id="words-tokens-numbers" style="position: relative;"><a href="#words-tokens-numbers" title="Words, Tokens, Numbers" id="anchor-words-tokens-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Words, Tokens, Numbers</h2><p>Tokenization has been a part of natural language processing for longer than modern AI models have existed.</p><p>It’s a bit cliché to say that everything in a computer is just a number, but it’s also mostly true. Language, however, is not naturally just a bunch of numbers. It might be speech, made of sound waves, or writing, made of marks on paper, or even an image of a printed text or a video of someone using sign language. But most of the time, when we talk about using computers to process natural language, we mean texts composed of sequences of characters: letters (a, b, c, etc.), numerals (0, 1, 2…), punctuation, and spaces, in different languages and textual encodings.</p><p>Computer engineers call these “strings”.</p><p>AI language models take sequences of numbers as input. So, you might write the sentence:</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>But, after tokenization, the AI model gets as input:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Tokenization is the process of converting an input string into a specific sequence of numbers that your AI model can understand.</p><p>When you use an AI model via a web API that charges users per token, each request is converted into a sequence of numbers like the one above. The number of tokens in the request is the length of that sequence of numbers. So, asking Jina Embeddings v2 for English to give you an embedding for “<em>What is today's weather in Berlin?</em>” will cost you 11 tokens because it converted that sentence into a sequence of 11 numbers before passing it to the AI model.</p><p>AI models based on the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer architecture</a> have a fixed-size <strong>context window</strong> whose size is measured in tokens. Sometimes this is called an “input window,” “context size,” or “sequence length” (especially on the <a href="https://huggingface.co/spaces/mteb/leaderboard">Hugging Face MTEB leaderboard</a>). It means the maximum text size that the model can see at one time.</p><p>So, if you want to use an embedding model, this is the maximum input size allowed.</p><p>Jina Embeddings v2 models all have a context window of 8,192 tokens. Other models will have different (typically smaller) context windows. This means that however much text you put into it, the tokenizer associated with that Jina Embeddings model must convert it into no more than 8,192 tokens.</p><h2 id="mapping-language-to-numbers" style="position: relative;"><a href="#mapping-language-to-numbers" title="Mapping Language to Numbers" id="anchor-mapping-language-to-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mapping Language to Numbers</h2><p>The simplest way to explain the logic of tokens is this:</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">A token is a number that stands in for a part of a string.</div></div><p>For natural language models, the part of a string that a token stands for is a word, a part of a word, or a piece of punctuation. Spaces are not generally given any explicit representation in tokenizer output.</p><p>Tokenization is part of a group of techniques in natural language processing called <a href="https://en.wikipedia.org/wiki/Text_segmentation"><em>text segmentation</em></a>, and the module that performs tokenization is called, very logically, a <strong>tokenizer</strong>.</p><p>To show how tokenization works, we’re going to tokenize some sentences using the smallest Jina Embeddings v2 for English model: <code>jina-embeddings-v2-small-en</code>. Jina Embeddings’ other English-only model — <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> — uses the same tokenizer, so there’s no point in downloading extra megabytes of AI model that we won’t use in this article.</p><p>First, install the <code>transformers</code> module in your Python environment or notebook. Use the <code>-U</code> flag to make sure you upgrade to the latest version because this model will not work with some older versions:</p><pre class="hljs-copy-wrapper"><code class="language-bash hljs">pip install -U transformers
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Then, download <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> using <code>AutoModel.from_pretrained</code>:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-small-en'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>To tokenize a string, use the <code>encode</code> method of the <code>tokenizer</code> member object of the model:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">model.tokenizer.encode(<span class="hljs-string">"What is today's weather in Berlin?"</span>)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The result is a list of numbers:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>To convert these numbers back to string forms, use the <code>convert_ids_to_tokens</code> method of the <code>tokenizer</code> object:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">model.tokenizer.convert_ids_to_tokens([<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>])
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The result is a list of strings:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'what'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">"'"</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'in'</span>,
 <span class="hljs-string">'berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Note that the model’s tokenizer has:</p><ol><li>Added <code>[CLS]</code>at the beginning and <code>[SEP]</code> at the end. This is necessary for technical reasons and means that <strong>every request for an embedding will cost two extra tokens</strong>, above however many tokens the text takes.</li><li>Split punctuation from words, turning “<em>Berlin?</em>” into: <code>berlin</code> and <code>?</code>, and “<em>today’s</em>” into <code>today</code>, <code>'</code>, and <code>s</code>.</li><li>Put everything in lowercase. Not all models do this, but this can help with training when using English. It may be less helpful in languages where capitalization has a different meaning.</li></ol><p>Different word-counting algorithms in different programs might count the words in this sentence differently. OpenOffice counts this as six words. The Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) counts seven words. Other software may come to other numbers, depending on how they handle punctuation and clitics like “’s.”</p><p>The tokenizer for this model produces nine tokens for those six or seven words, plus the two extra tokens needed with every request.</p><p>Now, let’s try with a less common place-name than Berlin:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"I live in Kinshasa."</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The result:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'kin'</span>, <span class="hljs-string">'##sha'</span>, <span class="hljs-string">'##sa'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The name “Kinshasa” is broken up into three tokens: <code>kin</code>, <code>##sha</code>, and <code>##sa</code>. The <code>##</code> indicates that this token is not the beginning of a word.</p><p>If we give the tokenizer something completely alien, the number of tokens over the number of words increases even more:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"Klaatu barada nikto"</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)

[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'##la'</span>, <span class="hljs-string">'##at'</span>, <span class="hljs-string">'##u'</span>, <span class="hljs-string">'bar'</span>, <span class="hljs-string">'##ada'</span>, <span class="hljs-string">'nik'</span>, <span class="hljs-string">'##to'</span>, <span class="hljs-string">'[SEP]'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Three words become eight tokens, plus the <code>[CLS]</code> and <code>[SEP]</code> tokens.</p><p>Tokenization in German is similar. With the <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/" rel="noreferrer">Jina Embeddings v2 for German</a> model, we can tokenize a translation of "What is today's weather in Berlin?" the same way as with the English model.</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">german_model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-base-de'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
token_ids = german_model.tokenizer.encode(<span class="hljs-string">"Wie wird das Wetter heute in Berlin?"</span>)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The result:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Wie'</span>, <span class="hljs-string">'wird'</span>, <span class="hljs-string">'das'</span>, <span class="hljs-string">'Wetter'</span>, <span class="hljs-string">'heute'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'Berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>This tokenizer is a little bit different from the English one in that <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> replace <code>[CLS]</code> and <code>[SEP]</code> but serve the same function. Also, the text is not case-normalized — upper and lower cases remain as written — because capitalization is meaningful in German differently from English.</p><p>(To simplify this presentation, I removed a special character indicating a word's beginning.)</p><p>Now, let’s try a more complex sentence <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html">from a newspaper text</a>:</p><blockquote>Ein Großteil der milliardenschweren Bauern-Subventionen bleibt liegen – zu genervt sind die Landwirte von bürokratischen Gängelungen und Regelwahn.</blockquote><pre class="hljs-copy-wrapper"><code class="hljs language-python">sentence = <span class="hljs-string">"""
Ein Großteil der milliardenschweren Bauern-Subventionen
bleibt liegen – zu genervt sind die Landwirte von 
bürokratischen Gängelungen und Regelwahn.
"""</span>
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The tokenized result:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Ein'</span>, <span class="hljs-string">'Großteil'</span>, <span class="hljs-string">'der'</span>, <span class="hljs-string">'mill'</span>, <span class="hljs-string">'iarden'</span>, <span class="hljs-string">'schwer'</span>, 
 <span class="hljs-string">'en'</span>, <span class="hljs-string">'Bauern'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'Sub'</span>, <span class="hljs-string">'ventionen'</span>, <span class="hljs-string">'bleibt'</span>, <span class="hljs-string">'liegen'</span>, 
 <span class="hljs-string">'–'</span>, <span class="hljs-string">'zu'</span>, <span class="hljs-string">'gen'</span>, <span class="hljs-string">'ervt'</span>, <span class="hljs-string">'sind'</span>, <span class="hljs-string">'die'</span>, <span class="hljs-string">'Landwirte'</span>, <span class="hljs-string">'von'</span>, 
 <span class="hljs-string">'büro'</span>, <span class="hljs-string">'krat'</span>, <span class="hljs-string">'ischen'</span>, <span class="hljs-string">'Gän'</span>, <span class="hljs-string">'gel'</span>, <span class="hljs-string">'ungen'</span>, <span class="hljs-string">'und'</span>, <span class="hljs-string">'Regel'</span>, 
 <span class="hljs-string">'wahn'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>Here, you see that many German words were broken up into smaller pieces and not necessarily along the lines licensed by German grammar. The result is that a long German word that would count as just one word to a word counter might be any number of tokens to Jina’s AI model.</p><p>Let’s do the same in Chinese, translating ”What is today's weather in Berlin?” as:</p><blockquote>柏林今天的天气怎么样？</blockquote><pre class="hljs-copy-wrapper"><code class="hljs language-makefile">chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(<span class="hljs-string">"柏林今天的天气怎么样？"</span>)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The tokenized result:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'柏林'</span>, <span class="hljs-string">'今天的'</span>, <span class="hljs-string">'天气'</span>, <span class="hljs-string">'怎么样'</span>, <span class="hljs-string">'？'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>In Chinese, there are usually no word breaks in written text, but the Jina Embeddings tokenizer frequently joins multiple Chinese characters together:</p>

<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏林</td>
<td>Bólín</td>
<td>Berlin</td>
</tr>
<tr>
<td>今天的</td>
<td>jīntiān de</td>
<td>today’s</td>
</tr>
<tr>
<td>天气</td>
<td>tiānqì</td>
<td>weather</td>
</tr>
<tr>
<td>怎么样</td>
<td>zěnmeyàng</td>
<td>how</td>
</tr>
</tbody>
</table>

<p>Let’s use a more complex sentence <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7">from a Hong Kong-based newspaper</a>:</p><pre class="hljs-copy-wrapper"><code class="language-Python hljs">sentence = <span class="hljs-string">"""
新規定執行首日，記者在下班高峰前的下午5時來到廣州地鐵3號線，
從繁忙的珠江新城站啟程，向機場北方向出發。
"""</span>
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>(Translation: <em>“On the first day that the new regulations were in force, this reporter arrived at Guangzhou Metro Line 3 at 5 p.m., during rush hour, having departed the Zhujiang New Town Station heading north towards the airport.”</em>)</p><p>The result:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'新'</span>, <span class="hljs-string">'規定'</span>, <span class="hljs-string">'執行'</span>, <span class="hljs-string">'首'</span>, <span class="hljs-string">'日'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'記者'</span>, <span class="hljs-string">'在下'</span>, <span class="hljs-string">'班'</span>, 
 <span class="hljs-string">'高峰'</span>, <span class="hljs-string">'前的'</span>, <span class="hljs-string">'下午'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'時'</span>, <span class="hljs-string">'來到'</span>, <span class="hljs-string">'廣州'</span>, <span class="hljs-string">'地'</span>, <span class="hljs-string">'鐵'</span>, <span class="hljs-string">'3'</span>, 
 <span class="hljs-string">'號'</span>, <span class="hljs-string">'線'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'從'</span>, <span class="hljs-string">'繁忙'</span>, <span class="hljs-string">'的'</span>, <span class="hljs-string">'珠江'</span>, <span class="hljs-string">'新城'</span>, <span class="hljs-string">'站'</span>, <span class="hljs-string">'啟'</span>, 
 <span class="hljs-string">'程'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'向'</span>, <span class="hljs-string">'機場'</span>, <span class="hljs-string">'北'</span>, <span class="hljs-string">'方向'</span>, <span class="hljs-string">'出發'</span>, <span class="hljs-string">'。'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>These tokens do not map to any specific dictionary of Chinese words (词典). For example, “啟程” - <em>qǐchéng</em> (depart, set out) would typically be categorized as a single word but is here split into its two constituent characters. Similarly, “在下班” would usually be recognized as two words, but with the split between “在” - <em>zài</em> (in, during) and “下班” - <em>xiàbān</em> (the end of the workday, rush hour), not between “在下” and “班” as the tokenizer has done here.</p><p>In all three languages, the places where the tokenizer breaks the text up are not directly related to the logical places where a human reader would break them.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">The tokenizer algorithm does not use a conventional, language-aware dictionary, so its behavior does not match how humans count words.</div></div><p>This is not a specific feature of Jina Embeddings models. This approach to tokenization is almost universal in AI model development. Although two different AI models may not have identical tokenizers, in the current state of development, they will practically all use tokenizers with this kind of behavior.</p><p>The next section will discuss the specific algorithm used in tokenization and the logic behind it.</p><h2 id="why-do-we-tokenize-and-why-this-way" style="position: relative;"><a href="#why-do-we-tokenize-and-why-this-way" title="Why Do We Tokenize? And Why This Way?" id="anchor-why-do-we-tokenize-and-why-this-way"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Why Do We Tokenize? And Why This Way?</h2><p>AI language models take as input sequences of numbers that stand in for text sequences, but a bit more happens before running the underlying neural network and creating an embedding. When presented with a list of numbers representing small text sequences, the model looks each number up in an internal dictionary that stores a unique vector for each number. It then combines them, and that becomes the input to the neural network.</p><p>This means that the tokenizer <strong>must</strong> be able to convert <strong><em>any</em></strong> input text we give it into tokens that appear in the model’s dictionary of token vectors. If we took our tokens from a conventional dictionary, the first time we encountered a misspelling or a rare proper noun or foreign word, the whole model would stop. It could not process that input.</p><p>In natural language processing, this is called the out-of-vocabulary (OOV) problem, and it’s pervasive in all text types and all languages. There are a few strategies for addressing the OOV problem:</p><ol><li>Ignore it. Replace everything not in the dictionary with an “unknown” token.</li><li>Bypass it. Instead of using a dictionary that maps text sequences to vectors, use one that maps <em>individual characters</em> to vectors. English only uses 26 letters most of the time, so this must be smaller and more robust against OOV problems than any dictionary.</li><li>Find frequent subsequences in the text, put them in the dictionary, and use characters (single-letter tokens) for whatever is left.</li></ol><p>The first strategy means that a lot of important information is lost. The model can’t even learn about the data it’s seen if it takes the form of something not in the dictionary. A lot of things in ordinary text are just not present in even the largest dictionaries.</p><p>The second strategy is possible, and researchers have investigated it. However, it means that the model has to accept a lot more input and has to learn a lot more. This means a much bigger model and much more training data for a result that has never proven to be any better than the third strategy.</p><p>AI language models pretty much all implement the third strategy in some form. Most use some variant of the <a href="https://huggingface.co/learn/nlp-course/chapter6/6">Wordpiece algorithm</a> <a href="https://ieeexplore.ieee.org/document/6289079">[Schuster and Nakajima 2012]</a> or a similar technique called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/">Senrich et al. 2016</a>] These algorithms are <em>language-agnostic</em>. That means they work the same for all written languages without any knowledge beyond a comprehensive list of possible characters. They were designed for multilingual models like Google’s BERT that take just any input from scraping the Internet — hundreds of languages and texts other than human language like computer programs — so that they could be trained without doing complicated linguistics.</p><p>Some research shows significant improvements using more language-specific and language-aware tokenizers. [<a href="https://aclanthology.org/2021.acl-long.243/">Rust et al. 2021</a>] But building tokenizers that way takes time, money, and expertise. Implementing a universal strategy like BPE or Wordpiece is much cheaper and easier.</p><p>However, as a consequence, there is no way to know how many tokens a specific text represents other than to run it through a tokenizer and then count the number of tokens that come out of it. Because the smallest possible subsequence of a text is one letter, you can be sure the number of tokens won’t be larger than the number of characters (minus spaces) plus two.</p><p>To get a good estimate, we need to throw a lot of text at our tokenizer and calculate empirically how many tokens we get on average, compared to how many words or characters we input. In the next section, we’ll do some not-very-systematic empirical measurements for all Jina Embeddings v2 models currently available.</p><h2 id="empirical-estimates-of-token-output-sizes" style="position: relative;"><a href="#empirical-estimates-of-token-output-sizes" title="Empirical Estimates of Token Output Sizes" id="anchor-empirical-estimates-of-token-output-sizes"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Empirical Estimates of Token Output Sizes</h2><p>For English and German, I used the Unicode text segmentation algorithm (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) to get word counts for texts. This algorithm is widely used to select text snippets when you double-click on something. It is the closest thing available to a universal objective word counter.</p><p>I installed the <a href="https://pypi.org/project/polyglot/">polyglot library</a> in Python, which implements this text segmenter:</p><pre class="hljs-copy-wrapper"><code class="language-bash hljs">pip install -U polyglot
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>To get the word count of a text, you can use code like this snippet:</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> polyglot.text <span class="hljs-keyword">import</span> Text

txt = <span class="hljs-string">"What is today's weather in Berlin?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(Text(txt).words))
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copy</button></div></pre><p>The result should be <code>7</code>.</p><p>To get a token count, segments of the text were passed to the tokenizers of various Jina Embeddings models, as described below, and each time, I subtracted two from the number of tokens returned.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text">The token counts listed here <b><strong style="white-space: pre-wrap;">do not include</strong></b> the extra two tokens at the beginning and end of each tokenized text.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" style="position: relative;"><a href="#english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" title="English
(jina-embeddings-v2-small-en and jina-embeddings-v2-base-en)" id="anchor-english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>English<br>(<code>jina-embeddings-v2-small-en</code> and <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>)</h3><p>To calculate averages, I downloaded two English text corpora from <a href="https://wortschatz.uni-leipzig.de/en" rel="noreferrer">Wortschatz Leipzig</a>, a collection of freely downloadable corpora in a number of languages and configurations hosted by Leipzig University:</p><ul><li>A one-million-sentence corpus of news data in English from 2020 (<code>eng_news_2020_1M</code>)</li><li>A one-million-sentence corpus of <a href="https://en.wikipedia.org/">English Wikipedia</a> data from 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Both can be found on <a href="https://wortschatz.uni-leipzig.de/en/download/English">their English downloads page</a>.</p><p>For diversity, I also downloaded the <a href="https://www.gutenberg.org/ebooks/135">Hapgood translation of Victor Hugo’s <em>Les Misérables</em></a> from Project Gutenberg, and a copy of the King James Version of the Bible, translated to English in 1611.</p><p>For each all four texts, I counted the words using the Unicode segmenter implemented in <code>polyglot</code>, then counted the tokens made by <code>jina-embeddings-v2-small-en</code>, subtracting two tokens for each tokenization request. The results are as follows:</p>

<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Text</th><th id="|<X;" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for English)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class=""><code>eng_news_2020_1M</code></td><td id="|<X;" class="">22,825,712</td><td id="GHal" class="">25,270,581</td><td id="h]mu" class="">1.107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class=""><code>eng_wikipedia_2016_1M</code></td><td id="|<X;" class="">24,243,607</td><td id="GHal" class="">26,813,877</td><td id="h]mu" class="">1.106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class=""><code>les_miserables_en</code></td><td id="|<X;" class="">688,911</td><td id="GHal" class="">764,121</td><td id="h]mu" class="">1.109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class=""><code>kjv_bible</code></td><td id="|<X;" class="">1,007,651</td><td id="GHal" class="">1,099,335</td><td id="h]mu" class="">1.091</td></tr></tbody></table>

<p>The use of precise numbers does not mean this is a precise result. That documents of such different genres would all have between 9% and 11% more tokens than words indicates that you can probably expect somewhere around 10% more tokens than words, as measured by the Unicode segmenter. Word processors often do not count punctuation, while the Unicode Segmenter does, so you can’t expect the word counts from office software to necessarily match this.</p><h3 id="german-jina-embeddings-v2-base-de" style="position: relative;"><a href="#german-jina-embeddings-v2-base-de" title="German
(jina-embeddings-v2-base-de)" id="anchor-german-jina-embeddings-v2-base-de"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>German<br>(<a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-de" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-de</span></a>)</h3><p>For German, I downloaded three corpora from <a href="https://wortschatz.uni-leipzig.de/en/download/German">Wortschatz Leipzig’s German page</a>:</p><ul><li><code>deu_mixed-typical_2011_1M</code> — One million sentences from a balanced mixture of texts in different genres, dating to 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> — One million sentences of news text from 2019.</li><li><code>deu_wikipedia_2021_1M</code> — One million sentences extracted from the German Wikipedia in 2021.</li></ul><p>And for diversity, I also downloaded all <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata">three volumes of Karl Marx’s <em>Kapital</em></a> from the <a href="https://www.deutschestextarchiv.de/" rel="noreferrer">Deutsches Textarchiv</a>.</p><p>I then followed the same procedure as for English:</p>

<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Text</th><th id="|qln" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class="">7,924,024</td><td id="YXZX" class="">9,772,652</td><td id="oEoQ" class="">1.234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class="">17,949,120</td><td id="YXZX" class="">21,711,555</td><td id="oEoQ" class="">1.210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class="">17,999,482</td><td id="YXZX" class="">22,654,901</td><td id="oEoQ" class="">1.259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class="" style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class="">784,336</td><td id="YXZX" class="">1,011,377</td><td id="oEoQ" class="">1.289</td></tr></tbody></table>

<p>These results have a larger spread than the English-only model but still suggest that German text will yield, on average, 20% to 30% more tokens than words.</p><p>English texts yield more tokens with the German-English tokenizer than the English-only one:</p>

<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc<l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Text</th><th id="R@A^" class="simple-table-header-color simple-table-header">Word count<br>(Unicode Segmenter)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for German and English)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class="">24243607</td><td id="UUfl" class="">27758535</td><td id="iTZS" class="">1.145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class="">22825712</td><td id="UUfl" class="">25566921</td><td id="iTZS" class="">1.120</td></tr></tbody></table>

<p>You should expect to need 12% to 15% more tokens than words to embed English texts with the bilingual German/English than with the English-only one.</p><h3 id="chinese-jina-embeddings-v2-base-zh" style="position: relative;"><a href="#chinese-jina-embeddings-v2-base-zh" title="Chinese
(jina-embeddings-v2-base-zh)" id="anchor-chinese-jina-embeddings-v2-base-zh"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Chinese<br>(<a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-zh" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-zh</span></a>)</h3><p>Chinese is typically written without spaces and had no traditional notion of “words” before the 20th century. Consequently, the size of a Chinese text is typically measured in characters (<strong>字数</strong>). So, instead of using the Unicode Segmenter, I measured the length of Chinese texts by removing all the spaces and then just getting the character length.</p><p>I downloaded three corpora from the <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese">Chinese corpus page at Wortschatz Leipzig</a>:</p><ul><li><code>zho_wikipedia_2018_1M</code> — One million sentences from the Chinese language Wikipedia, extracted in 2018.</li><li><code>zho_news_2007-2009_1M</code> — One million sentences from Chinese news sources, collected from 2007 to 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> — One million sentences from news sources that use exclusively traditional Chinese characters (繁體字).</li></ul><p>In addition, for some diversity, I also used <em>The True Story of Ah Q</em> (阿Q正傳), a novella by Lu Xun (魯迅) written in the early 1920s. I downloaded the <a href="https://www.gutenberg.org/ebooks/25332">traditional character version from Project Gutenberg</a>.</p>

<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Text</th><th id="CaUc" class="simple-table-header-color simple-table-header">Character count<br>(字数)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 <br>for Chinese and English)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio of tokens to characters<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class="">45,116,182</td><td id="CQ{d" class="">29,193,028</td><td id="_};C" class="">0.647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class="">44,295,314</td><td id="CQ{d" class="">28,108,090</td><td id="_};C" class="">0.635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class="" style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class="">54,585,819</td><td id="CQ{d" class="">40,290,982</td><td id="_};C" class="">0.738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class="" style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class="">41,268</td><td id="CQ{d" class="">25,346</td><td id="_};C" class="">0.614</td></tr></tbody></table>

<p>This spread in token-to-character ratios is unexpected, and especially the outlier for the traditional character corpus merits further investigation. Nonetheless, we can conclude that for Chinese, you should expect to need <em>fewer</em> tokens than there are characters in your text. Depending on your content, you can expect to need 25% to 40% less.</p><p>English texts in Jina Embeddings v2 for Chinese and English yielded roughly the same number of tokens as they do in the English-only model:</p>

<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="<rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class="" style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class="" style="width:165.3125px">24,243,607</td><td id="=]?F" class="">26,890,176</td><td id="<rlw" class="">1.109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class="" style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class="" style="width:165.3125px">22,825,712</td><td id="=]?F" class="">25,060,352</td><td id="<rlw" class="">1.097</td></tr></tbody></table>

<h2 id="taking-tokens-seriously" style="position: relative;"><a href="#taking-tokens-seriously" title="Taking Tokens Seriously" id="anchor-taking-tokens-seriously"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Taking Tokens Seriously</h2><p>Tokens are an important scaffolding for AI language models, and research is ongoing in this area.</p><p>One of the places where AI models have proven revolutionary is the discovery that they are very robust against noisy data. Even if a particular model does not use the optimal tokenization strategy, if the network is large enough, given enough data, and adequately trained, it can learn to do the right thing from imperfect input.</p><p>Consequently, much less effort is spent on improving tokenization than in other areas, but this may change.</p><p>As a user of embeddings, who buys them via an <a href="https://jina.ai/embeddings/">API like Jina Embeddings</a>, you can’t know precisely how many tokens you’ll need for a specific task and may have to do some testing of your own to get solid numbers. But the estimates provided here — circa 110% of the word count for English, circa 125% of the word count for German, and circa 70% of the character count for Chinese — should be enough for basic budgeting.</p></section></article><div data-v-d0fd5afa="" class="row justify-between items-center q-py-md"><div data-v-d0fd5afa=""><span data-v-d0fd5afa="" class="text-weight-bold">Categories:</span><span data-v-d0fd5afa="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Tech blog</div></div></div></span></div><div data-v-d0fd5afa=""><div data-v-d0fd5afa="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-d0fd5afa="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-d0fd5afa="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-d0fd5afa="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-d0fd5afa="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-d0fd5afa="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Fen-US%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-d0fd5afa="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-d0fd5afa="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-d0fd5afa="" class="text-h5 q-my-xl">Read more</div><a data-v-aa70018b="" data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/text-image-global-contrastive-alignment-and-token-patch-local-alignment"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">January 07, 2025 • 6 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Text-Image Global Contrastive Alignment and Token-Patch Local Alignment</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="3D rendered scene with a black-screened laptop on a geometrical pedestal and patterned spheres, against a blue backdrop."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="3D rendered scene with a black-screened laptop on a geometrical pedestal and patterned spheres, against a blue backdrop." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2025/01/banner--16-.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/text-embeddings-fail-to-capture-word-order-and-how-to-fix-it"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">December 17, 2024 • 12 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Text Embeddings Fail to Capture Word Order and How to Fix It</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Bo Wang"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Bo Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Three abstract figures in white, gray, and pink on matching cubes placed on a colorful checkered surface against a green back"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Three abstract figures in white, gray, and pink on matching cubes placed on a colorful checkered surface against a green back" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/banner-order.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa70018b="" data-v-d0fd5afa="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/scaling-test-time-compute-for-embedding-models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa70018b="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa70018b="" class="q-focus-helper"></span><div data-v-aa70018b="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa70018b="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__label q-item__label--caption text-caption">December 12, 2024 • 12 minutes read</div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa70018b="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa70018b="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Scaling Test-Time Compute For Embedding Models</div></div></div><div data-v-aa70018b="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa70018b="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa70018b="" class="col-4 overflow-hidden"><div data-v-aa70018b="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="David Hockney artwork of a hand holding a rod with three colored spheres on a blue-toned background."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="David Hockney artwork of a hand holding a rod with three colored spheres on a blue-toned background." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/12/test-time-compute.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div class="col-sm-12 col-md"><div class="q-list q-list--dark small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Offices</div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Sunnyvale, CA</div><div class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlin, Germany (HQ)</div><div class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Germany</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Beijing, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">Level 5, Building 6, No.48 Haidian West St. Beijing, China</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, China</div><div class="q-item__label q-item__label--caption text-caption text-dim">402 Floor 4, Fu'an Technology Building, Shenzhen, China</div></div></div></div></div><div class="col-sm-12 col-md row"><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Search Foundation</div><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Reader</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Embeddings</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Reranker</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Classifier</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Segmenter</div></a><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Get Jina AI API key</div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Rate Limit</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-pa-none"><svg class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div class="q-item__section column q-item__section--main justify-center">API Status</div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Company</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">About us</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contact sales</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Newsroom</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Intern program</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Join us</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Download logo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Terms</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Security</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Terms &amp; Conditions</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Privacy</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Manage Cookies</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>
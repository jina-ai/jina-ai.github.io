<!DOCTYPE html><html translate="no" dir="ltr" lang="fr"><head><title>Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?</title><meta charset="utf-8"><meta name="title" content="Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?"><meta name="description" content="Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche."><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"><meta property="og:title" content="Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?"><meta property="og:description" content="Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche."><meta property="og:image" content="https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"><meta property="twitter:title" content="Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?"><meta property="twitter:description" content="Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche."><meta property="twitter:image" content="https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-DkBpQVdD.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-rzO9Riiq.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-jnAggiaj.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/dynamic-import-helper-BheWnx7M.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-Cm5_2pz9.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-CsZSpDM6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-26k8wCgp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-DRdCuJId.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-FUo4egug.js"><script src="https://www.googletagmanager.com/gtag/js?l=dataLayer&amp;id=G-4GEXCSE3MV" async=""></script><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/fr-DiR1v30g.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-CnUiC5SO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-dialog-plugin-component-ChxM5luF.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/_setToArray-BELsSfoH.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBadge-CvTpNiiu.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/UserAvatarComponent-Tn8sGzFf.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-1cGoe3Mv.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QChip-BUF_3DeJ.js"><link rel="stylesheet" crossorigin="" href="/assets/UserAvatarComponent-HOhEbA2Z.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-BARxGti7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-BGKpP_Tp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QList-Bfdb17YG.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-Cqqq-YAs.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLayout-66NssidD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-DksAwkjz.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QScrollObserver-BfS3AE0f.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/TouchPan-C5Y6unZy.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/touch-BjYP5sR0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-6KmVA0PL.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpinnerRings-BVVpAkWw.js"><link rel="stylesheet" crossorigin="" href="/assets/QSpinnerRings-0UdsL2AK.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-BRZqkcGJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-fMyul494.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-C8y_-vy0.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/VideoDialog-zEv7gdp6.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useRoute-BtFYFIbg.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-DihvZdVB.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-D0v6Ofia.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-Dm7PFtEq.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-Cv0lzJ4E.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-CGi3FQN5.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-CQ5WKB68.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-Dppj5U4D.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/useModels-Dv-vQph0.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-C1vDmaBY.css"><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><meta name="author" content="Han Xiao"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Han Xiao"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-02-20T02:19:04.000+01:00"><meta property="article:modified_time" content="2024-08-30T23:11:22.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?",
  "description": "Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche.",
  "image": [
    "https://jina.ai/blog-banner/what-is-colbert-and-late-interaction-and-why-they-matter-in-search.webp"
  ],
  "datePublished": "2024-02-20T02:19:04.000+01:00",
  "dateModified": "2024-08-30T23:11:22.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Han Xiao",
      "url": "https://jina-ai-gmbh.ghost.io/author/han/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script charset="utf-8" src="https://platform.twitter.com/js/tweet.d7aeb21a88e025d2ea5f5431a103f586.js"></script><script prerender-ignore id=usercentrics-cmp src=https://web.cmp.usercentrics.eu/ui/loader.js data-settings-id=w5v6v2pJsC3wdR async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div data-v-ce90450d="" class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header data-v-ce90450d="" class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div data-v-ce90450d="" class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div data-v-ce90450d="" class="q-space"></div><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div data-v-ce90450d="" class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div data-v-ce90450d="" class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div data-v-ce90450d="" class="q-list q-list--dark" role="list"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--active q-router-link--active q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Nouvelles</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/models"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Modèles</div></a><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_16ce5a10-eaf4-43ac-ad2d-af50ad28cc68" aria-label="Développer &quot;Des produits&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Des produits</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_16ce5a10-eaf4-43ac-ad2d-af50ad28cc68" style="display: none;"><div data-v-ce90450d="" class="q-list q-list--dark" role="list" label="Des produits"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M123.395%20131.064L162.935%20102.948L154.175%2087.776L123.395%20131.064ZM146.664%2074.7669L121.428%20129.927L129.479%2045.0007L146.664%2074.7669ZM117.189%20137.27L36%20195H76.1387L117.189%20137.27ZM93.2635%20195L119.156%20138.405L113.791%20195H93.2635ZM177.409%20128.018L124.531%20133.031L168.649%20112.846L177.409%20128.018ZM38.4785%20170.794L116.053%20135.302L55.6643%20141.027L38.4785%20170.794ZM184.92%20141.027L202.105%20170.793L124.531%20135.302L184.92%20141.027ZM116.053%20133.031L63.1751%20128.018L71.9347%20112.846L116.053%20133.031ZM123.395%20137.269L204.584%20195H164.446L123.395%20137.269ZM77.6493%20102.948L117.189%20131.063L86.4089%2087.7758L77.6493%20102.948ZM121.428%20138.406L126.793%20195H147.321L121.428%20138.406ZM119.156%20129.927L93.9197%2074.7667L111.105%2045L119.156%20129.927Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Recherche profonde</div><div class="q-item__label q-item__label--caption text-caption">Recherchez, lisez et raisonnez jusqu'à trouver la meilleure réponse.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lecteur</div><div class="q-item__label q-item__label--caption text-caption">Lisez les URL et effectuez des recherches sur le Web pour de meilleurs LLM de base.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Intégrations</div><div class="q-item__label q-item__label--caption text-caption">Intégrations multimodales et multilingues de classe mondiale.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reclasseur</div><div class="q-item__label q-item__label--caption text-caption">Récupérateur neuronal de classe mondiale pour maximiser la pertinence de la recherche.</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard text-dim"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_d5d62d59-33fe-4457-89c6-c232c9b472ad" aria-label="Développer &quot;Plus&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Plus</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_d5d62d59-33fe-4457-89c6-c232c9b472ad" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/classifier" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classificateur</div><div class="q-item__label q-item__label--caption text-caption">Classification à zéro plan et à quelques plans pour l'image et le texte.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/segmenter" target=""><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmenteur</div><div class="q-item__label q-item__label--caption text-caption">Coupez un long texte en morceaux et effectuez la tokenisation.</div></div></a></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Documentation de l'API</div><div class="q-item__label q-item__label--caption text-caption">Génération automatique de code pour votre IDE ou LLM copilote</div></div><div class="q-item__section column q-item__section--side justify-center"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div data-v-ce90450d="" class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_3143c719-aaa0-4104-885a-a76736944142" aria-label="Développer &quot;Entreprise&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Entreprise</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_3143c719-aaa0-4104-885a-a76736944142" style="display: none;"><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">À propos de nous</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contacter le service commercial</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programme de stage</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Rejoignez-nous</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Télécharger le logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">termes et conditions</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/api-dashboard?login=true" label="Se connecter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Se connecter</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">login</i></div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><label data-v-ce90450d="" class="q-field row no-wrap items-start q-field--borderless q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dark full-width" for="f_af247e24-24aa-4e77-ad44-a63565136de8"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_af247e24-24aa-4e77-ad44-a63565136de8" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_af247e24-24aa-4e77-ad44-a63565136de8_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">language</i></div></div></div></label></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div data-v-ce90450d="" class="q-page-container squeeze-top" style="padding-top: 56px;"><main data-v-c36e4d4e="" class="q-page" style="min-height: 100vh;"><div data-v-c36e4d4e="" class="row full-width relative-position justify-end"><div data-v-c36e4d4e="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-c36e4d4e="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Qu'est-ce que ColBERT ?</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Comprendre la conception de ColBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Encodeurs de requêtes et de documents dans ColBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Trouver les K meilleurs documents avec ColBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">La stratégie d'indexation de ColBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Efficacité et performance de ColBERT</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Utilisation de jina-colbert-v1-en : un modèle ColBERTv2 avec une longueur de 8192</div></div></div><div data-v-c36e4d4e="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-c36e4d4e="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-c36e4d4e="" class="q-item__label">Conclusion</div></div></div></div></div><div data-v-c36e4d4e="" class="col-12 col-md-10 col-lg-12"><div data-v-c36e4d4e="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><i class="q-icon notranslate material-symbols material-symbols-sharp q-chip__icon q-chip__icon--left" aria-hidden="true" role="presentation">star</i><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Mis en exergue</div></div></div><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog technique</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">février 20, 2024</div><h1 data-v-c36e4d4e="" class="text-weight-medium text-center q-px-md my-title">Qu'est-ce que ColBERT et l'interaction tardive (Late Interaction) et pourquoi sont-ils importants pour la recherche ?</h1><div data-v-c36e4d4e="" class="col row justify-center"><div data-v-c36e4d4e="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">Le ColBERT de Jina AI sur Hugging Face fait sensation sur Twitter, apportant une nouvelle perspective à la recherche avec sa capacité de traitement de 8192 tokens. Cet article explore les nuances de ColBERT et ColBERTv2, mettant en lumière leurs conceptions innovantes et expliquant pourquoi leur fonctionnalité d'interaction tardive révolutionne la recherche.</div></div><div data-v-c36e4d4e="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-c36e4d4e="" class="q-img q-img--menu" role="img" aria-label="Neon theater or concert hall marquee letters lit up at night with city lights and faint &quot;Adobe Sto&quot; visible."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Neon theater or concert hall marquee letters lit up at night with city lights and faint &quot;Adobe Sto&quot; visible." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/Untitled-design--28-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-c36e4d4e="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-c36e4d4e="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-c36e4d4e="" class="q-item__label">Han Xiao • 16 minutes lues</div></div></div></div><div data-v-c36e4d4e="" class="row justify-center"><div data-v-c36e4d4e="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-c36e4d4e="" class="article"><section data-v-c36e4d4e="" class="gh-content"><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://jina.ai/news/jina-colbert-v2-multilingual-late-interaction-retriever-for-embedding-and-reranking"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Jina ColBERT v2 : Récupérateur d'interaction tardive multilingue pour l'intégration et le reclassement</div><div class="kg-bookmark-description">Jina ColBERT v2 prend en charge 89 langues avec des performances de récupération supérieures, des dimensions de sortie contrôlées par l'utilisateur, et une longueur de token de 8192.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/08/colbert-banner.jpg" alt="" style="cursor: help;"></div></a><figcaption><p dir="ltr"><span style="white-space: pre-wrap;">Mise à jour : Le 31 août 2024, nous avons publié la 2e version de Jina-ColBERT, avec des performances améliorées, un support multilingue pour 89 langues et des dimensions de sortie flexibles. Consultez l'article de publication pour plus de détails.</span></p></figcaption></figure><p>Vendredi dernier, la sortie du <a href="https://huggingface.co/jinaai/jina-colbert-v1-en">modèle ColBERT par Jina AI sur Hugging Face</a> a suscité un enthousiasme significatif dans la communauté IA, particulièrement sur Twitter/X. Alors que beaucoup connaissent le modèle révolutionnaire BERT, l'effervescence autour de ColBERT en a laissé certains perplexes : Qu'est-ce qui fait que ColBERT se démarque dans le domaine très concurrentiel des technologies de recherche d'information ? Pourquoi la communauté IA est-elle enthousiasmée par le ColBERT de longueur 8192 ? Cet article explore les subtilités de ColBERT et ColBERTv2, soulignant leur conception, leurs améliorations et l'efficacité surprenante de l'interaction tardive de ColBERT.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/reranker"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Reranker API</div><div class="kg-bookmark-description">Maximisez facilement la pertinence de recherche et la précision RAG</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-reranker-api.png" alt="" style="cursor: help;"></div></a></figure><figure class="kg-card kg-embed-card"><div class="twitter-tweet twitter-tweet-rendered" style="display: flex; max-width: 550px; width: 100%; margin-top: 10px; margin-bottom: 10px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="" style="position: static; visibility: visible; width: 550px; height: 671px; display: block; flex-grow: 1;" title="X Post" src="https://platform.twitter.com/embed/Tweet.html?creatorScreenName=JinaAI_&amp;dnt=false&amp;embedId=twitter-widget-0&amp;features=eyJ0ZndfdGltZWxpbmVfbGlzdCI6eyJidWNrZXQiOltdLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2ZvbGxvd2VyX2NvdW50X3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9iYWNrZW5kIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19yZWZzcmNfc2Vzc2lvbiI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfZm9zbnJfc29mdF9pbnRlcnZlbnRpb25zX2VuYWJsZWQiOnsiYnVja2V0Ijoib24iLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X21peGVkX21lZGlhXzE1ODk3Ijp7ImJ1Y2tldCI6InRyZWF0bWVudCIsInZlcnNpb24iOm51bGx9LCJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3Nob3dfYmlyZHdhdGNoX3Bpdm90c19lbmFibGVkIjp7ImJ1Y2tldCI6Im9uIiwidmVyc2lvbiI6bnVsbH0sInRmd19kdXBsaWNhdGVfc2NyaWJlc190b19zZXR0aW5ncyI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdXNlX3Byb2ZpbGVfaW1hZ2Vfc2hhcGVfZW5hYmxlZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9LCJ0ZndfdmlkZW9faGxzX2R5bmFtaWNfbWFuaWZlc3RzXzE1MDgyIjp7ImJ1Y2tldCI6InRydWVfYml0cmF0ZSIsInZlcnNpb24iOm51bGx9LCJ0ZndfbGVnYWN5X3RpbWVsaW5lX3N1bnNldCI6eyJidWNrZXQiOnRydWUsInZlcnNpb24iOm51bGx9LCJ0ZndfdHdlZXRfZWRpdF9mcm9udGVuZCI6eyJidWNrZXQiOiJvbiIsInZlcnNpb24iOm51bGx9fQ%3D%3D&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1758503072999907825&amp;lang=fr&amp;origin=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F&amp;sessionId=784aa1d0edc8aad42597a2a4c7d4b75028f98717&amp;siteScreenName=JinaAI_&amp;theme=light&amp;widgetsVersion=2615f7e52b7e0%3A1702314776716&amp;width=550px" data-tweet-id="1758503072999907825"></iframe></div>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></figure><h2 id="what-is-colbert" style="position: relative;"><a href="#what-is-colbert" title="Qu'est-ce que ColBERT ?" id="anchor-what-is-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Qu'est-ce que ColBERT ?</h2><p>Le nom "ColBERT" signifie <strong>Co</strong>ntextualized <strong>L</strong>ate Interaction over <strong>BERT</strong>, un modèle issu de l'Université Stanford, qui exploite la compréhension profonde du langage de BERT tout en introduisant un nouveau mécanisme d'interaction. Ce mécanisme, connu sous le nom d'<strong>interaction tardive</strong>, permet une récupération efficace et précise en traitant les requêtes et les documents séparément jusqu'aux étapes finales du processus de récupération. Plus précisément, il existe deux versions du modèle :</p><ul><li><strong>ColBERT</strong> : Le modèle initial était l'œuvre d'<a href="https://x.com/lateinteraction?s=20"><strong>Omar Khattab</strong></a><strong> et Matei Zaharia</strong>, présentant une nouvelle approche de la recherche d'information à travers l'article "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT". Leur travail a été publié à SIGIR 2020.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2004.12832"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</div><div class="kg-bookmark-description">Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Omar Khattab</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="" style="cursor: help;"></div></a><figcaption><p><span style="white-space: pre-wrap;">L'article original de ColBERT qui introduit l'"interaction tardive".</span></p></figcaption></figure><ul><li><strong>ColBERTv2</strong> : S'appuyant sur le travail fondamental, <strong>Omar Khattab</strong> a poursuivi ses recherches, collaborant avec <strong>Barlas Oguz, Matei Zaharia, et Michael S. Bernstein</strong> pour introduire "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", présenté à SIGIR 2021. Cette nouvelle itération de ColBERT a abordé les limitations précédentes et introduit des améliorations clés, telles que la <strong>supervision débruitée</strong> et la <strong>compression résiduelle</strong>, améliorant à la fois l'efficacité de récupération du modèle et son efficacité de stockage.</li></ul><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.01488"><div class="kg-bookmark-content"><div class="kg-bookmark-title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</div><div class="kg-bookmark-description">Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">×</span></span></span></span></span>.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png" alt="" style="cursor: help;"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">Keshav Santhanam</span></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="" style="cursor: help;"></div></a><figcaption><p><span style="white-space: pre-wrap;">ColBERTv2 ajoutant la supervision débruitée et la compression résiduelle pour améliorer la qualité des données d'entraînement et réduire l'empreinte spatiale.</span></p></figcaption></figure><h2 id="understand-colberts-design" style="position: relative;"><a href="#understand-colberts-design" title="Comprendre la conception de ColBERT" id="anchor-understand-colberts-design"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Comprendre la conception de ColBERT</h2><p>Étant donné que l'architecture de ColBERTv2 reste très similaire à celle du ColBERT original, avec ses innovations clés tournant autour des techniques d'entraînement et des mécanismes de compression, nous allons d'abord examiner les aspects fondamentaux du ColBERT original.</p><h3 id="what-is-late-interaction-in-colbert" style="position: relative;"><a href="#what-is-late-interaction-in-colbert" title="Qu'est-ce que l'interaction tardive dans ColBERT ?" id="anchor-what-is-late-interaction-in-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Qu'est-ce que l'interaction tardive dans ColBERT ?</h3><p>"Interaction" fait référence au processus d'évaluation de la pertinence entre une requête et un document en comparant leurs représentations.</p><p>L'"<em>interaction tardive</em>" est l'essence de ColBERT. Le terme provient de l'architecture du modèle et de sa stratégie de traitement, où l'interaction entre les représentations de la requête et du document se produit tard dans le processus, après que les deux aient été encodés indépendamment. Cela contraste avec les modèles d'"<em>interaction précoce</em>", où les embeddings de requête et de document interagissent à des stades plus précoces, généralement avant ou pendant leur encodage par le modèle.</p>

<table>
<thead>
<tr>
<th>Interaction Type</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Early Interaction</td>
<td>BERT, ANCE, DPR, Sentence-BERT, DRMM, KNRM, Conv-KNRM, etc.</td>
</tr>
<tr>
<td>Late Interaction</td>
<td>ColBERT, ColBERTv2</td>
</tr>
</tbody>
</table>

<p>L'interaction précoce peut augmenter la complexité computationnelle car elle nécessite de considérer toutes les paires requête-document possibles, la rendant moins efficace pour les applications à grande échelle.</p><p>Les modèles d'interaction tardive comme ColBERT optimisent l'efficacité et la scalabilité en permettant le pré-calcul des représentations des documents et en employant une étape d'interaction plus légère à la fin, qui se concentre sur les représentations déjà encodées. Ce choix de conception permet des temps de récupération plus rapides et des demandes computationnelles réduites, le rendant plus adapté au traitement de grandes collections de documents.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/colbert-blog-interaction.svg" class="kg-image" alt="Diagram illustrating query-document similarity with models for no, partial, and late interaction, including language mode rep" width="300" height="143" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Diagrammes schématiques illustrant les paradigmes d'interaction requête-document dans la RI neuronale, avec l'interaction tardive de ColBERT à gauche.</span></figcaption></figure><h3 id="no-interaction-cosine-similarity-of-document-and-query-embeddings" style="position: relative;"><a href="#no-interaction-cosine-similarity-of-document-and-query-embeddings" title="Pas d'interaction : similarité cosinus des embeddings de documents et de requêtes" id="anchor-no-interaction-cosine-similarity-of-document-and-query-embeddings"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Pas d'interaction : similarité cosinus des embeddings de documents et de requêtes</h3><p>De nombreuses bases de données vectorielles et solutions de recherche neuronale pratiques reposent sur la correspondance rapide de similarité cosinus entre les embeddings de documents et de requêtes. Bien qu'attrayante pour sa simplicité et son efficacité computationnelle, cette méthode, souvent appelée « <em>sans interaction</em> » ou « <em>non basée sur l'interaction</em> », s'est révélée moins performante par rapport aux modèles qui intègrent une forme d'interaction entre les requêtes et les documents.</p><p>La limitation principale de l'approche « sans interaction » réside dans son incapacité à capturer les nuances complexes et les relations entre les termes des requêtes et des documents. La recherche d'information, dans son essence, consiste à comprendre et à faire correspondre l'intention derrière une requête avec le contenu d'un document. Ce processus nécessite souvent une compréhension contextuelle profonde des termes impliqués, ce que les embeddings uniques et agrégés pour les documents et les requêtes peinent à fournir.</p><h2 id="query-and-document-encoders-in-colbert" style="position: relative;"><a href="#query-and-document-encoders-in-colbert" title="Encodeurs de requêtes et de documents dans ColBERT" id="anchor-query-and-document-encoders-in-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Encodeurs de requêtes et de documents dans ColBERT</h2><p>La stratégie d'encodage de ColBERT est basée sur le modèle BERT, connu pour sa compréhension contextuelle profonde du langage. Le modèle génère des représentations vectorielles denses pour chaque token dans une requête ou un document, <strong>créant respectivement un sac d'embeddings contextualisés pour une requête et un sac pour un document.</strong> Cela facilite une comparaison nuancée de leurs embeddings durant la phase d'interaction tardive.</p><h3 id="query-encoder-of-colbert" style="position: relative;"><a href="#query-encoder-of-colbert" title="Encodeur de requêtes de ColBERT" id="anchor-query-encoder-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Encodeur de requêtes de ColBERT</h3><p>Pour une requête <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span> avec les tokens <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>q</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">{q_1, q_2, ..., q_l}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span>, le processus commence par tokeniser <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span> en tokens WordPiece basés sur BERT et en préfixant un token spécial <code>[Q]</code>. Ce token <code>[Q]</code>, positionné juste après le token <code>[CLS]</code> de BERT, signale le début d'une requête.</p><p>Si la requête est plus courte qu'un nombre prédéfini de tokens <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>, elle est complétée avec des tokens <code>[mask]</code> jusqu'à <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> ; sinon, elle est tronquée aux premiers tokens <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">N_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.109em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>. La séquence complétée est ensuite passée à travers BERT, suivie d'un CNN (Réseau de Neurones Convolutif) et d'une normalisation. Le résultat est un ensemble de vecteurs d'embedding notés <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9722em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> ci-dessous :<br><span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>q</mi></msub><mo>:</mo><mo>=</mo><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">z</mi><mi mathvariant="normal">e</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">Q</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><msub><mi>q</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>q</mi><mi>l</mi></msub><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">a</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">k</mi><mo stretchy="false">]</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{E}_q := \mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[Q]},q_0,q_1,\ldots,q_l\mathtt{[mask]},\mathtt{[mask]},\ldots,\mathtt{[mask]}\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9722em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathrm">Normalize</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">BERT</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">Q</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">mask</span><span class="mclose">]</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span></span></p><h3 id="document-encoder-of-colbert" style="position: relative;"><a href="#document-encoder-of-colbert" title="Encodeur de documents de ColBERT" id="anchor-document-encoder-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Encodeur de documents de ColBERT</h3><p>De même, pour un document <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span></span></span></span></span> avec les tokens <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">{d_1, d_2, ..., d_n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span>, un token <code>[D]</code> est préfixé pour indiquer le début d'un document. Cette séquence, sans besoin de complétion, subit le même processus, donnant un ensemble de vecteurs d'embedding notés <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8361em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> ci-dessous :<br><span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">E</mi><mi>d</mi></msub><mo>:</mo><mo>=</mo><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">z</mi><mi mathvariant="normal">e</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo stretchy="false">[</mo><mi mathvariant="monospace">D</mi><mo stretchy="false">]</mo></mrow><mo separator="true">,</mo><msub><mi>d</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{E}_d := \mathrm{Filter}\left(\mathrm{Normalize}\left(\mathrm{BERT}\left(\mathtt{[D]},d_0,d_1,...,d_n\right)\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8361em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathrm">Filter</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">Normalize</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathrm">BERT</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mopen">[</span><span class="mord mathtt">D</span><span class="mclose">]</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span></span></span></p><p>L'utilisation de tokens <code>[mask]</code> pour compléter les requêtes (appelée <strong>augmentation de requête</strong> dans l'article) assure une longueur uniforme pour toutes les requêtes, facilitant le traitement par lots. Les tokens <code>[Q]</code> et <code>[D]</code> marquent explicitement le début des requêtes et des documents, respectivement, aidant le modèle à distinguer les deux types d'entrées.</p><h3 id="comparing-colbert-to-cross-encoders" style="position: relative;"><a href="#comparing-colbert-to-cross-encoders" title="Comparaison de ColBERT aux cross-encoders" id="anchor-comparing-colbert-to-cross-encoders"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Comparaison de ColBERT aux cross-encoders</h3><p>Les cross-encoders traitent les paires de requêtes et de documents ensemble, les rendant très précis mais moins efficaces pour les tâches à grande échelle en raison du coût computationnel d'évaluation de chaque paire possible. Ils excellent dans des scénarios spécifiques où le scoring précis des paires de phrases est nécessaire, comme dans les tâches de similarité sémantique ou la comparaison détaillée de contenu. Cependant, cette conception limite leur applicabilité dans les situations nécessitant une récupération rapide depuis de grands ensembles de données, où les embeddings pré-calculés et les calculs de similarité efficaces sont primordiaux.</p><figure class="kg-card kg-image-card"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/02/ce-vs-colbert.svg" class="kg-image" alt="Diagrams comparing &quot;Cross Encoder: Early all-to-all interaction&quot; and &quot;ColBERT: Late interaction&quot; with labeled Query and Docum" width="210" height="150" style="cursor: help;"></figure><p>En revanche, le modèle d'interaction tardive de ColBERT permet le pré-calcul des embeddings de documents, accélérant significativement le processus de récupération sans compromettre la profondeur de l'analyse sémantique. Cette méthode, bien que contre-intuitive comparée à l'approche directe des cross-encoders, offre une solution évolutive pour les tâches de recherche d'information en temps réel et à grande échelle. Elle représente un compromis stratégique entre l'efficacité computationnelle et la qualité de la modélisation des interactions.</p><h2 id="finding-the-top-k-documents-using-colbert" style="position: relative;"><a href="#finding-the-top-k-documents-using-colbert" title="Trouver les K meilleurs documents avec ColBERT" id="anchor-finding-the-top-k-documents-using-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Trouver les K meilleurs documents avec ColBERT</h2><p>Une fois que nous avons les embeddings pour la requête et les documents, trouver les K documents les plus pertinents devient simple (mais pas aussi simple que le calcul du cosinus de deux vecteurs).</p><p>Les opérations clés incluent un produit scalaire par lots pour calculer les similarités terme à terme, un max-pooling sur les termes du document pour trouver la plus haute similarité par terme de requête, et une somme sur les termes de requête pour dériver le score total du document, suivi du tri des documents basé sur ces scores. Le pseudo-code PyTorch est décrit ci-dessous :</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">import</span> torch

<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_relevance_scores</span>(<span class="hljs-params">query_embeddings, document_embeddings, k</span>):
    <span class="hljs-string">"""
    Compute relevance scores for top-k documents given a query.
    
    :param query_embeddings: Tensor representing the query embeddings, shape: [num_query_terms, embedding_dim]
    :param document_embeddings: Tensor representing embeddings for k documents, shape: [k, max_doc_length, embedding_dim]
    :param k: Number of top documents to re-rank
    :return: Sorted document indices based on their relevance scores
    """</span>
    
    <span class="hljs-comment"># Ensure document_embeddings is a 3D tensor: [k, max_doc_length, embedding_dim]</span>
    <span class="hljs-comment"># Pad the k documents to their maximum length for batch operations</span>
    <span class="hljs-comment"># Note: Assuming document_embeddings is already padded and moved to GPU</span>
    
    <span class="hljs-comment"># Compute batch dot-product of Eq (query embeddings) and D (document embeddings)</span>
    <span class="hljs-comment"># Resulting shape: [k, num_query_terms, max_doc_length]</span>
    scores = torch.matmul(query_embeddings.unsqueeze(<span class="hljs-number">0</span>), document_embeddings.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))
    
    <span class="hljs-comment"># Apply max-pooling across document terms (dim=2) to find the max similarity per query term</span>
    <span class="hljs-comment"># Shape after max-pool: [k, num_query_terms]</span>
    max_scores_per_query_term = scores.<span class="hljs-built_in">max</span>(dim=<span class="hljs-number">2</span>).values
    
    <span class="hljs-comment"># Sum the scores across query terms to get the total score for each document</span>
    <span class="hljs-comment"># Shape after sum: [k]</span>
    total_scores = max_scores_per_query_term.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)
    
    <span class="hljs-comment"># Sort the documents based on their total scores</span>
    sorted_indices = total_scores.argsort(descending=<span class="hljs-literal">True</span>)
    
    <span class="hljs-keyword">return</span> sorted_indices
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copier</button></div></pre><p>Notez que cette procédure est utilisée à la fois pendant l'entraînement et le reclassement lors de l'inférence. Le modèle ColBERT est entraîné en utilisant une perte de classement par paires, où les données d'entraînement consistent en triplets <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>+</mo></msup><mo separator="true">,</mo><msup><mi>d</mi><mo>−</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d^+, d^-)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.0213em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, où <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span> représente une requête, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">d^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span> est un document pertinent (positif) pour la requête, et <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">d^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span></span> est un document non pertinent (négatif). Le modèle vise à apprendre des représentations telles que le score de similarité entre <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span> et <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">d^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span> soit plus élevé que le score entre <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span> et <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">d^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7713em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7713em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span></span>.</p><p>L'objectif d'entraînement peut être mathématiquement représenté comme la minimisation de la fonction de perte suivante : <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">L</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi></mrow><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>−</mo><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>+</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msup><mi>d</mi><mo>−</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{Loss} = \max(0, 1 - S(q, d^+) + S(q, d^-))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord"><span class="mord mathrm">Loss</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0713em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8213em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0713em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8213em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></span></p><p>, où <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S(q, d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span> dénote le score de similarité calculé par ColBERT entre une requête <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span></span></span></span></span> et un document <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></span>. Ce score est obtenu en agrégeant les scores de similarité maximale des embeddings les mieux appariés entre la requête et le document, suivant le motif d'interaction tardive décrit dans l'architecture du modèle. Cette approche assure que le modèle est entraîné à distinguer entre les documents pertinents et non pertinents pour une requête donnée, en encourageant une plus grande marge dans les scores de similarité pour les paires de documents positifs et négatifs.</p><h3 id="denoised-supervision-in-colbertv2" style="position: relative;"><a href="#denoised-supervision-in-colbertv2" title="Supervision débruitée dans ColBERTv2" id="anchor-denoised-supervision-in-colbertv2"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Supervision débruitée dans ColBERTv2</h3><p>La supervision débruitée dans ColBERTv2 affine le processus d'entraînement original en sélectionnant des négatifs difficiles et en exploitant un cross-encoder pour la distillation. Cette méthode sophistiquée d'amélioration de la qualité des données d'entraînement implique plusieurs étapes :</p><ol><li><strong>Entraînement Initial</strong> : Utilisation des triplets officiels du jeu de données MS MARCO, comprenant une requête, un document pertinent et un document non pertinent.</li><li><strong>Indexation et Récupération</strong> : Emploi de la compression de ColBERTv2 pour indexer les passages d'entraînement, suivi de la récupération des k meilleurs passages pour chaque requête.</li><li><strong>Reclassement par Cross-Encoder</strong> : Amélioration de la sélection des passages par reclassement via un cross-encoder MiniLM, distillant ses scores dans ColBERTv2.</li><li><strong>Formation des Tuples d'Entraînement</strong> : Génération de tuples à w voies pour l'entraînement, incorporant à la fois des passages bien classés et moins bien classés pour créer des exemples difficiles.</li><li><strong>Raffinement Itératif</strong> : Répétition du processus pour améliorer continuellement la sélection des négatifs difficiles, améliorant ainsi les performances du modèle.</li></ol><p>Notez que ce processus représente une amélioration sophistiquée du régime d'entraînement de ColBERT plutôt qu'un changement fondamental de son architecture.</p><h3 id="hyperparameters-of-colbert" style="position: relative;"><a href="#hyperparameters-of-colbert" title="Hyperparamètres de ColBERT" id="anchor-hyperparameters-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Hyperparamètres de ColBERT</h3><p>Les hyperparamètres de ColBERT sont résumés ci-dessous :</p>

<table>
<thead>
<tr>
<th>Hyperparamètre</th>
<th>Meilleur choix</th>
<th>Raison</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>3 x 10^{-6}</td>
<td>Sélectionné pour le fine-tuning afin d'assurer des mises à jour stables et efficaces du modèle.</td>
</tr>
<tr>
<td>Batch Size</td>
<td>32</td>
<td>Équilibre entre l'efficacité computationnelle et la capacité à capturer suffisamment d'informations par mise à jour.</td>
</tr>
<tr>
<td>Number of Embeddings per Query (Nq)</td>
<td>32</td>
<td>Fixé pour assurer une taille de représentation cohérente entre les requêtes, facilitant un traitement efficace.</td>
</tr>
<tr>
<td>Embedding Dimension (m)</td>
<td>128</td>
<td>Démontre un bon équilibre entre la puissance de représentation et l'efficacité computationnelle.</td>
</tr>
<tr>
<td>Training Iterations</td>
<td>200k (MS MARCO), 125k (TREC CAR)</td>
<td>Choisi pour assurer un apprentissage approfondi tout en évitant le surapprentissage, avec des ajustements basés sur les caractéristiques du dataset.</td>
</tr>
<tr>
<td>Bytes per Dimension in Embeddings</td>
<td>4 (re-ranking), 2 (end-to-end ranking)</td>
<td>Compromis entre précision et efficacité spatiale, en tenant compte du contexte d'application (re-ranking vs. end-to-end).</td>
</tr>
<tr>
<td>Vector-Similarity Function</td>
<td>Cosine (re-ranking), (Squared) L2 (end-to-end)</td>
<td>Sélectionné selon la performance et l'efficacité dans les contextes de recherche respectifs.</td>
</tr>
<tr>
<td>FAISS Index Partitions (P)</td>
<td>2000</td>
<td>Détermine la granularité du partitionnement de l'espace de recherche, impactant l'efficacité de la recherche.</td>
</tr>
<tr>
<td>Nearest Partitions Searched (p)</td>
<td>10</td>
<td>Équilibre entre l'étendue de la recherche et l'efficacité computationnelle.</td>
</tr>
<tr>
<td>Sub-vectors per Embedding (s)</td>
<td>16</td>
<td>Affecte la granularité de la quantification, influençant la vitesse de recherche et l'utilisation de la mémoire.</td>
</tr>
<tr>
<td>Index Representation per Dimension</td>
<td>16-bit values</td>
<td>Choisi pour la deuxième étape de la recherche end-to-end pour gérer le compromis entre précision et espace.</td>
</tr>
<tr>
<td>Number of Layers in Encoders</td>
<td>12-layer BERT</td>
<td>Équilibre optimal entre la profondeur de la compréhension contextuelle et l'efficacité computationnelle.</td>
</tr>
  <tr>
  <td>Max Query Length</td>
<td>128</td>
<td>Le nombre maximum de tokens traités par l'encodeur de requêtes. <b>Ceci est étendu dans le modèle Jina-ColBERT.</b></td>
</tr>
    <tr>
  <td>Max Document Length</td>
<td>512</td>
<td>Le nombre maximum de tokens traités par l'encodeur de documents. <b>Ceci est étendu à 8192 dans le modèle Jina-ColBERT.</b></td>
</tr>
</tbody>
</table>

<h2 id="the-indexing-strategy-of-colbert" style="position: relative;"><a href="#the-indexing-strategy-of-colbert" title="La stratégie d'indexation de ColBERT" id="anchor-the-indexing-strategy-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>La stratégie d'indexation de ColBERT</h2>
<p>Contrairement aux approches basées sur la représentation qui encodent chaque document en un seul vecteur d'embedding, <strong>ColBERT encode les documents (et les requêtes) en ensembles d'embeddings, chaque token dans un document ayant son propre embedding.</strong> Cette approche signifie intrinsèquement que pour les documents plus longs, plus d'embeddings seront stockés, <strong>ce qui est un point faible du ColBERT original, et qui a été résolu plus tard par ColBERTv2.</strong></p>
<p>La clé pour gérer cela efficacement réside dans l'utilisation par ColBERT d'une base de données vectorielle (par exemple <a href="https://github.com/facebookresearch/faiss">FAISS</a>) pour l'indexation et la recherche, et son processus d'indexation détaillé qui est conçu pour gérer efficacement de grands volumes de données. L'article original de ColBERT mentionne plusieurs stratégies pour améliorer l'efficacité de l'indexation et de la recherche, notamment :</p>
<ul>
<li><strong>Indexation hors ligne</strong> : Les représentations des documents sont calculées hors ligne, permettant le pré-calcul et le stockage des embeddings de documents. Ce processus utilise le traitement par lots et l'accélération GPU pour gérer efficacement de grandes collections de documents.</li>
<li><strong>Stockage des embeddings</strong> : Les embeddings de documents peuvent être stockés en utilisant des valeurs 32-bit ou 16-bit pour chaque dimension, offrant un compromis entre précision et besoins en stockage. Cette flexibilité permet à ColBERT de maintenir un équilibre entre efficacité (en termes de performance de recherche) et efficience (en termes de coûts de stockage et de calcul).</li>
</ul>
<p>L'introduction de la <strong>compression résiduelle</strong> dans ColBERTv2, qui est une approche novatrice non présente dans le ColBERT original, joue un rôle clé dans la réduction de l'empreinte spatiale du modèle de 6 à 10 fois tout en préservant la qualité. Cette technique compresse davantage les embeddings en capturant et stockant efficacement uniquement les différences par rapport à un ensemble de centroïdes de référence fixes.</p>
<h2 id="effectiveness-and-efficiency-of-colbert" style="position: relative;"><a href="#effectiveness-and-efficiency-of-colbert" title="Efficacité et performance de ColBERT" id="anchor-effectiveness-and-efficiency-of-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Efficacité et performance de ColBERT</h2>
<p>On pourrait initialement supposer que l'incorporation de la compréhension contextuelle profonde de BERT dans la recherche nécessiterait intrinsèquement des ressources computationnelles importantes, rendant une telle approche moins viable pour les applications en temps réel en raison de la latence et des coûts computationnels élevés. Cependant, ColBERT remet en question et renverse cette supposition grâce à son utilisation innovante du mécanisme d'interaction tardive. Voici quelques points notables :</p>
<ol>
<li><strong>Gains d'efficacité significatifs</strong> : ColBERT réalise une réduction des coûts computationnels (FLOPs) et de la latence de plusieurs ordres de grandeur par rapport aux modèles de classement traditionnels basés sur BERT. Plus précisément, pour une taille de modèle donnée (par exemple, encodeur transformer "base" à 12 couches), ColBERT non seulement égale mais dans certains cas dépasse l'efficacité des modèles basés sur BERT avec des demandes computationnelles nettement inférieures. Par exemple, pour une profondeur de reclassement de <em>k</em>=10, BERT nécessite près de 180× plus de FLOPs que ColBERT ; cet écart s'élargit à mesure que <em>k</em> augmente, atteignant 13900× à <em>k</em>=1000 et même 23000× à <em>k</em>=2000.</li>
<li><strong>Amélioration du Recall et MRR@10 dans la recherche de bout en bout</strong> : Contrairement à l'intuition initiale selon laquelle une interaction plus profonde entre les représentations des requêtes et des documents (comme observé dans les modèles d'interaction précoce) serait nécessaire pour une performance de recherche élevée, la configuration de recherche de bout en bout de ColBERT démontre une efficacité supérieure. Par exemple, son Recall@50 dépasse le Recall@1000 du BM25 officiel et presque tous les Recall@200 des autres modèles, soulignant la capacité remarquable du modèle à retrouver des documents pertinents dans une vaste collection sans comparaison directe de chaque paire requête-document.</li>
<li><strong>Praticité pour les applications du monde réel</strong> : Les résultats expérimentaux soulignent l'applicabilité pratique de ColBERT pour les scénarios du monde réel. Son débit d'indexation et son efficacité mémoire le rendent adapté à l'indexation de grandes collections de documents comme MS MARCO en quelques heures, maintenant une haute efficacité avec une empreinte spatiale gérable. Ces qualités soulignent l'adéquation de ColBERT pour le déploiement dans des environnements de production où la performance et l'efficacité computationnelle sont primordiales.</li>
<li><strong>Scalabilité avec la taille de la collection de documents</strong> : La conclusion peut-être la plus surprenante est la scalabilité et l'efficacité de ColBERT dans la gestion de collections de documents à grande échelle. L'architecture permet le pré-calcul des embeddings de documents et exploite un traitement par lots efficace pour l'interaction requête-document, permettant au système de s'adapter efficacement à la taille de la collection de documents. Cette scalabilité est contre-intuitive lorsqu'on considère la complexité et la profondeur de compréhension requises pour une recherche de documents efficace, démontrant l'approche innovante de ColBERT pour équilibrer l'efficacité computationnelle et l'efficacité de la recherche.</li>
</ol>
<h2 id="using-jina-colbert-v1-en-a-8192-length-colbertv2-model" style="position: relative;"><a href="#using-jina-colbert-v1-en-a-8192-length-colbertv2-model" title="Utilisation de jina-colbert-v1-en : un modèle ColBERTv2 avec une longueur de 8192" id="anchor-using-jina-colbert-v1-en-a-8192-length-colbertv2-model"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Utilisation de <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> : un modèle ColBERTv2 avec une longueur de 8192</h2>
<p>Jina-ColBERT est conçu pour une recherche à la fois rapide et précise, supportant <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/">des longueurs de contexte jusqu'à 8192, tirant parti des avancées de JinaBERT</a>, qui permet le traitement de séquences plus longues grâce à ses améliorations architecturales.</p>
<div class="kg-card kg-callout-card kg-callout-card-blue">
<div class="kg-callout-emoji">💡</div>
<div class="kg-callout-text">Strictement parlant, Jina-ColBERT supporte une longueur de 8190 tokens. Rappelons que dans l'encodeur de documents ColBERT, chaque document est complété avec <code spellcheck="false" style="white-space: pre-wrap;">[D],[CLS]</code> au début.</div>
</div>
<figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://huggingface.co/jinaai/jina-colbert-v1-en"><div class="kg-bookmark-content"><div class="kg-bookmark-title">jinaai/jina-colbert-v1-en · Hugging Face</div><div class="kg-bookmark-description">We're on a journey to advance and democratize artificial intelligence through open source and open science.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://huggingface.co/favicon.ico" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-colbert-v1-en.png" alt="" style="cursor: help;"></div></a></figure>
<h3 id="jinas-improvement-over-original-colbert" style="position: relative;"><a href="#jinas-improvement-over-original-colbert" title="Les améliorations de Jina par rapport au ColBERT original" id="anchor-jinas-improvement-over-original-colbert"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Les améliorations de Jina par rapport au ColBERT original</h3>
<p>La principale avancée de Jina-ColBERT est son architecture de base, <code>jina-bert-v2-base-en</code>, qui permet le traitement de contextes significativement plus longs (jusqu'à 8192 tokens) comparé au ColBERT original qui utilise <code>bert-base-uncased</code>. Cette capacité est cruciale pour gérer des documents avec un contenu extensif, fournissant des résultats de recherche plus détaillés et contextuels.</p>
<h3 id="jina-colbert-v1-en-performance-comparison-vs-colbertv2" style="position: relative;"><a href="#jina-colbert-v1-en-performance-comparison-vs-colbertv2" title="Comparaison des performances de jina-colbert-v1-en vs. ColBERTv2" id="anchor-jina-colbert-v1-en-performance-comparison-vs-colbertv2"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Comparaison des performances de <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> vs. ColBERTv2</h3>
<p>Nous avons évalué <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> sur les datasets BEIR et le nouveau benchmark LoCo qui favorise le contexte long, en le testant par rapport à l'implémentation originale de ColBERTv2 et aux modèles basés sur la non-interaction</p>modèle <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>.<p></p>

<table>
<thead>
<tr>
<th>Dataset</th>
<th>ColBERTv2</th>
<th><a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a></th>
<th><a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Arguana</td>
<td>46.5</td>
<td><strong>49.4</strong></td>
<td>44.0</td>
</tr>
<tr>
<td>Climate-Fever</td>
<td>18.1</td>
<td>19.6</td>
<td><strong>23.5</strong></td>
</tr>
<tr>
<td>DBPedia</td>
<td><strong>45.2</strong></td>
<td>41.3</td>
<td>35.1</td>
</tr>
<tr>
<td>FEVER</td>
<td>78.8</td>
<td><strong>79.5</strong></td>
<td>72.3</td>
</tr>
<tr>
<td>FiQA</td>
<td>35.4</td>
<td>36.8</td>
<td><strong>41.6</strong></td>
</tr>
<tr>
<td>HotpotQA</td>
<td><strong>67.5</strong></td>
<td>65.9</td>
<td>61.4</td>
</tr>
<tr>
<td>NFCorpus</td>
<td>33.7</td>
<td><strong>33.8</strong></td>
<td>32.5</td>
</tr>
<tr>
<td>NQ</td>
<td>56.1</td>
<td>54.9</td>
<td><strong>60.4</strong></td>
</tr>
<tr>
<td>Quora</td>
<td>85.5</td>
<td>82.3</td>
<td><strong>88.2</strong></td>
</tr>
<tr>
<td>SCIDOCS</td>
<td>15.4</td>
<td>16.9</td>
<td><strong>19.9</strong></td>
</tr>
<tr>
<td>SciFact</td>
<td>68.9</td>
<td><strong>70.1</strong></td>
<td>66.7</td>
</tr>
<tr>
<td>TREC-COVID</td>
<td>72.6</td>
<td><strong>75.0</strong></td>
<td>65.9</td>
</tr>
<tr>
<td>Webis-touch2020</td>
<td>26.0</td>
<td><strong>27.0</strong></td>
<td>26.2</td>
</tr>
<tr>
<td>LoCo</td>
<td>74.3</td>
<td>83.7</td>
<td><strong>85.4</strong></td>
</tr>
<tr>
<td>Moyenne</td>
<td>51.7</td>
<td><strong>52.6</strong></td>
<td>51.6</td>
</tr>
</tbody>
</table>

<p>Ce tableau démontre les performances supérieures de <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a>, particulièrement dans les scénarios nécessitant des longueurs de contexte plus importantes par rapport au ColBERTv2 original. Notez que <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> <a href="https://arxiv.org/abs/2310.19923">utilise plus de données d'entraînement</a>, tandis que <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> n'utilise que MSMARCO, ce qui peut justifier les bonnes performances de <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> sur certaines tâches.</p><h3 id="example-usage-of-jina-colbert-v1-en" style="position: relative;"><a href="#example-usage-of-jina-colbert-v1-en" title="Exemple d'utilisation de jina-colbert-v1-en" id="anchor-example-usage-of-jina-colbert-v1-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Exemple d'utilisation de <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a></h3><p>Cet extrait de code décrit le processus d'indexation avec Jina-ColBERT, mettant en évidence sa prise en charge des documents longs.</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> colbert <span class="hljs-keyword">import</span> Indexer
<span class="hljs-keyword">from</span> colbert.infra <span class="hljs-keyword">import</span> Run, RunConfig, ColBERTConfig

n_gpu: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>  <span class="hljs-comment"># Set your number of available GPUs</span>
experiment: <span class="hljs-built_in">str</span> = <span class="hljs-string">""</span>  <span class="hljs-comment"># Name of the folder where the logs and created indices will be stored</span>
index_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">""</span>  <span class="hljs-comment"># The name of your index, i.e. the name of your vector database</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">with</span> Run().context(RunConfig(nranks=n_gpu, experiment=experiment)):
        config = ColBERTConfig(
          doc_maxlen=<span class="hljs-number">8192</span>  <span class="hljs-comment"># Our model supports 8k context length for indexing long documents</span>
        )
        indexer = Indexer(
          checkpoint=<span class="hljs-string">"jinaai/jina-colbert-v1-en"</span>,
          config=config,
        )
        documents = [
          <span class="hljs-string">"ColBERT is an efficient and effective passage retrieval model."</span>,
          <span class="hljs-string">"Jina-ColBERT is a ColBERT-style model but based on JinaBERT so it can support both 8k context length."</span>,
          <span class="hljs-string">"JinaBERT is a BERT architecture that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length."</span>,
          <span class="hljs-string">"Jina-ColBERT model is trained on MSMARCO passage ranking dataset, following a very similar training procedure with ColBERTv2."</span>,
          <span class="hljs-string">"Jina-ColBERT achieves the competitive retrieval performance with ColBERTv2."</span>,
          <span class="hljs-string">"Jina is an easier way to build neural search systems."</span>,
          <span class="hljs-string">"You can use Jina-ColBERT to build neural search systems with ease."</span>,
          <span class="hljs-comment"># Add more documents here to ensure the clustering work correctly</span>
        ]
        indexer.index(name=index_name, collection=documents)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copier</button></div></pre><h3 id="use-jina-colbert-v1-en-in-ragatouille" style="position: relative;"><a href="#use-jina-colbert-v1-en-in-ragatouille" title="Utiliser jina-colbert-v1-en dans RAGatouille" id="anchor-use-jina-colbert-v1-en-in-ragatouille"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Utiliser <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> dans RAGatouille</h3><p>RAGatouille est une nouvelle bibliothèque Python qui facilite l'utilisation de méthodes de recherche avancées dans les pipelines RAG. Elle est conçue pour être modulaire et facilement intégrable, permettant aux utilisateurs d'exploiter la recherche de pointe de manière transparente. L'objectif principal de RAGatouille est de simplifier l'application de modèles complexes comme ColBERT dans les pipelines RAG, rendant accessibles ces méthodes aux développeurs sans nécessiter une expertise approfondie de la recherche sous-jacente. Grâce à <a href="https://twitter.com/bclavie">Benjamin Clavié</a>, vous pouvez maintenant utiliser facilement <a class="dynamic-model-name" href="/?sui&amp;model=jina-colbert-v1-en" target="_blank"><span class="dynamic-model-name-inner">jina-colbert-v1-en</span></a> :</p><pre class="hljs-copy-wrapper"><code class="language-python hljs"><span class="hljs-keyword">from</span> ragatouille <span class="hljs-keyword">import</span> RAGPretrainedModel

<span class="hljs-comment"># Get your model &amp; collection of big documents ready</span>
RAG = RAGPretrainedModel.from_pretrained(<span class="hljs-string">"jinaai/jina-colbert-v1-en"</span>)
my_documents = [
    <span class="hljs-string">"very long document1"</span>,
    <span class="hljs-string">"very long document2"</span>,
    <span class="hljs-comment"># ... more documents</span>
]

<span class="hljs-comment"># And create an index with them at full length!</span>
RAG.index(collection=my_documents,
          index_name=<span class="hljs-string">"the_biggest_index"</span>,
          max_document_length=<span class="hljs-number">8190</span>,)

<span class="hljs-comment"># or encode them in-memory with no truncation, up to your model's max length</span>
RAG.encode(my_documents)
</code><div class="hljs-copy-container" data-autohide="true" style="--hljs-theme-background:rgba(0, 0, 0, 0); --hljs-theme-color:rgb(204, 204, 204); --hljs-theme-padding:16px;"><button class="hljs-copy-button" data-copied="false">Copier</button></div></pre><p>Pour plus d'informations détaillées et pour explorer davantage Jina-ColBERT, vous pouvez visiter la <a href="https://huggingface.co/jinaai/jina-colbert-v1-en">page Hugging Face</a>.</p><h2 id="conclusion" style="position: relative;"><a href="#conclusion" title="Conclusion" id="anchor-conclusion"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Conclusion</h2><p>ColBERT représente une avancée significative dans le domaine de la recherche d'information. En permettant des longueurs de contexte plus importantes avec Jina-ColBERT et en maintenant la compatibilité avec l'approche d'interaction tardive de ColBERT, il offre une alternative puissante pour les développeurs cherchant à implémenter des fonctionnalités de recherche à la pointe de la technologie.</p><p>Associé à la bibliothèque RAGatouille, qui simplifie l'intégration de modèles de recherche complexes dans les pipelines RAG, les développeurs peuvent désormais exploiter facilement la puissance de la recherche avancée, rationalisant leurs flux de travail et améliorant leurs applications. La synergie entre Jina-ColBERT et RAGatouille illustre un progrès remarquable dans l'accessibilité et l'efficacité des modèles de recherche IA avancés pour une utilisation pratique.</p></section></article><div data-v-c36e4d4e="" class="row justify-between items-center q-py-md"><div data-v-c36e4d4e=""><span data-v-c36e4d4e="" class="text-weight-bold">Catégories:</span><span data-v-c36e4d4e="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><i class="q-icon notranslate material-symbols material-symbols-sharp q-chip__icon q-chip__icon--left" aria-hidden="true" role="presentation">star</i><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Mis en exergue</div></div></div><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog technique</div></div></div></span></div><div data-v-c36e4d4e=""><div data-v-c36e4d4e="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-c36e4d4e="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fwhat-is-colbert-and-late-interaction-and-why-they-matter-in-search%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-c36e4d4e="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div></div></div></div></div></main></div><div data-v-ce90450d="" class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div data-v-ce90450d="" class="col-sm-12 col-md"><div data-v-ce90450d="" class="q-list q-list--dark small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Des bureaux</div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Sunnyvale, Californie</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, États-Unis</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Berlin, Allemagne (siège social)</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Allemagne</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Pékin, Chine</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">Niveau 5, bâtiment 6, n° 48, rue Haidian Ouest, Pékin, Chine</div></div></div><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-ce90450d="" class="q-item__section column q-item__section--side q-item__section--top justify-start"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center"><div data-v-ce90450d="" class="q-item__label">Shenzhen, en Chine</div><div data-v-ce90450d="" class="q-item__label q-item__label--caption text-caption text-dim">402 étage 4, bâtiment technologique Fu'an, Shenzhen, Chine</div></div></div></div></div><div data-v-ce90450d="" class="col-sm-12 col-md row"><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fondation Recherche</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/deepsearch"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Recherche profonde</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Lecteur</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Intégrations</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Reclasseur</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Classificateur</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Segmenteur</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://docs.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Documentation de l'API</div></a><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Obtenir la clé API Jina</div></div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Limite de taux</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-pa-none"><svg data-v-ce90450d="" class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Statut de l'API</div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Entreprise</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">À propos de nous</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Contacter le service commercial</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Rédaction</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Programme de stage</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://app.dover.com/jobs/jinaai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Rejoignez-nous</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Télécharger le logo</div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i data-v-ce90450d="" class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div data-v-ce90450d="" class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Termes</div><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal#security-as-company-value"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Sécurité</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">termes et conditions</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Confidentialité</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--main justify-center">Gérer les cookies</div></a><a data-v-ce90450d="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div data-v-ce90450d="" class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div data-v-ce90450d="" class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div data-v-ce90450d="" class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div data-v-ce90450d="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a data-v-ce90450d="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div data-v-ce90450d="" class="row items-center justify-end q-gutter-x-sm col-12 col-md"><div class="text-caption text-dim"> Jina AI © 2020-2025. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div><script src="https://platform.twitter.com/widgets.js"></script><iframe scrolling="no" frameborder="0" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.2f70fb173b9000da126c79afe2098f02.html?origin=http%3A%2F%2F127.0.0.1%3A3000" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe"></iframe></body></html>
<!DOCTYPE html><html translate="no" dir="ltr" lang="fr"><head><title>Une analyse approfondie de la tokenisation</title><meta charset="utf-8"><meta name="title" content="Une analyse approfondie de la tokenisation"><meta name="description" content="La tokenisation, dans les LLMs, signifie découper les textes d'entrée en plus petites parties pour leur traitement. Alors pourquoi les embeddings sont-ils facturés au token ?"><meta property="og:type" content="website"><meta property="og:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="og:title" content="Une analyse approfondie de la tokenisation"><meta property="og:description" content="La tokenisation, dans les LLMs, signifie découper les textes d'entrée en plus petites parties pour leur traitement. Alors pourquoi les embeddings sont-ils facturés au token ?"><meta property="og:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta property="twitter:site" content="@JinaAI_"><meta name="twitter:creator" content="@JinaAI_"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://jina.ai/news/a-deep-dive-into-tokenization"><meta property="twitter:title" content="Une analyse approfondie de la tokenisation"><meta property="twitter:description" content="La tokenisation, dans les LLMs, signifie découper les textes d'entrée en plus petites parties pour leur traitement. Alors pourquoi les embeddings sont-ils facturés au token ?"><meta property="twitter:image" content="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"><meta name="format-detection" content="telephone=no"><meta name="msapplication-tap-highlight" content="no"><meta name="viewport" content="user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"><link rel="icon" type="image/png" sizes="128x128" href="/icons/favicon-128x128.png"><link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"><link rel="icon" type="image/ico" href="/favicon.ico"><link rel="apple-touch-startup-image" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1284x2778.png"><link rel="apple-touch-startup-image" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1170x2532.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-828x1792.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1125x2436.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2688.png"><link rel="apple-touch-startup-image" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-750x1334.png"><link rel="apple-touch-startup-image" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)" href="/icons/apple-launch-1242x2208.png"><link rel="apple-touch-startup-image" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1620x2160.png"><link rel="apple-touch-startup-image" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1536x2048.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2224.png"><link rel="apple-touch-startup-image" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-1668x2388.png"><link rel="apple-touch-startup-image" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" href="/icons/apple-launch-2048x2732.png"><style>body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
    }</style>  <script type="module" crossorigin="" src="/assets/index-BnRq6kR2.js"></script>
  <link rel="stylesheet" crossorigin="" href="/assets/index-DkC9LM0F.css">
<link rel="modulepreload" as="script" crossorigin="" href="/assets/i18n-D_2159PJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-BWPGmhJI.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/register-DQv5Gvrv.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTooltip-SvQWL-rt.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/position-engine-CJP0CMPg.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/copy-to-clipboard-81hPzHGG.js"><link rel="stylesheet" crossorigin="" href="/assets/prism-tomorrow-CHcPHExe.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/fr-DiR1v30g.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/index-sFLS0J54.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/en-B3at9lMY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/MainLayout-Cjtag0Nk.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QSpace-Bd8vKQ84.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnDropdown-DK8IZNdw.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QBtnGroup-BhStthui.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QMenu-MWdmCmbK.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SeFoComponent-38fIw6kX.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTabs-B97ArA7A.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QResizeObserver-D9Bvl1u3.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QLinearProgress-Bvqk2wby.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QItemLabel-DqwrbAsf.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/format-DyQxkAtJ.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/orderBy-CEX6MwS7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/finetune-DYDCs3HD.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/ClosePopup-BvIPEO70.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-meta-DS4vCOaO.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/embedding-BnOnJ-8f.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QTable-DFCb3Pd7.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/use-fullscreen-C2fTwmNp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/blogs-DAqcFNgp.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QForm-bws_vNtY.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QExpansionItem-DdKoWMbw.js"><link rel="stylesheet" crossorigin="" href="/assets/QForm-B0qXRYV5.css"><link rel="stylesheet" crossorigin="" href="/assets/SeFoComponent-F-ymhHEr.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/search-uufOv_iV.js"><link rel="stylesheet" crossorigin="" href="/assets/MainLayout-BMR_OewU.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsPage-HtiMjI9G.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/QPage-lSAdSzHS.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsBadge-Aib9uXKT.js"><link rel="modulepreload" as="script" crossorigin="" href="/assets/SXTooltip-NTqDSXMe.js"><link rel="stylesheet" crossorigin="" href="/assets/SXTooltip-vcpvmx2_.css"><link rel="modulepreload" as="script" crossorigin="" href="/assets/NewsVerticalCard-BoPPcTN2.js"><link rel="stylesheet" crossorigin="" href="/assets/NewsVerticalCard-BYg3bc8w.css"><link rel="stylesheet" crossorigin="" href="/assets/NewsPage-iSUijgjF.css"><meta property="twitter:label1" content="Written by"><meta property="twitter:data1" content="Scott Martens"><meta property="twitter:label2" content="Reading time"><meta property="twitter:data2" content="16 mins read"><meta property="article:published_time" content="2024-01-31T16:10:14.000+01:00"><meta property="article:modified_time" content="2024-08-14T11:38:01.000+02:00"><script type="application/ld+json" data-qmeta="ldJson">{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Une analyse approfondie de la tokenisation",
  "description": "La tokenisation, dans les LLMs, signifie découper les textes d'entrée en plus petites parties pour leur traitement. Alors pourquoi les embeddings sont-ils facturés au token ?",
  "image": [
    "https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png"
  ],
  "datePublished": "2024-01-31T16:10:14.000+01:00",
  "dateModified": "2024-08-14T11:38:01.000+02:00",
  "author": [
    {
      "@type": "Person",
      "name": "Scott Martens",
      "url": "https://jina-ai-gmbh.ghost.io/author/scott/"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Jina AI",
    "url": "https://jina.ai"
  }
}</script><script src="https://jina-ai-gmbh.ghost.io/public/cards.min.js" async=""></script><link prerender-ignore rel=preconnect href=//api.usercentrics.eu><link prerender-ignore rel=preconnect href=//privacy-proxy.usercentrics.eu><link prerender-ignore rel=preload href=//app.usercentrics.eu/browser-ui/latest/loader.js as=script><link prerender-ignore rel=preload href=//privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js as=script><script prerender-ignore id=usercentrics-cmp data-settings-id=w5v6v2pJsC3wdR src=https://app.usercentrics.eu/browser-ui/latest/loader.js async></script><script prerender-ignore src=https://privacy-proxy.usercentrics.eu/latest/uc-block.bundle.js async></script><script prerender-ignore src="https://www.googletagmanager.com/gtag/js?id=G-9T52NXDS9T" async></script><script prerender-ignore>window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'G-9T52NXDS9T');</script></head><body class="desktop no-touch body--dark"><div id="q-app" data-v-app class="hidden"><div class="q-layout q-layout--standard" tabindex="-1" style="min-height: 600px;"><header class="q-header q-layout__section--marginal fixed-top lock-blur bg-transparent print-hide"><div class="q-toolbar row no-wrap items-center q-px-none relative-position" role="toolbar"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--dense no-border-radius self-stretch q-px-md q-pa-none" tabindex="0" href="/" style="font-size: 2em;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/Jina - Dark.svg"></i></span></a><div class="q-space"></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle text- q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">search</i></span></button><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">reorder</i></span></button></div></header><div class="q-drawer-container"><div class="q-drawer__opener fixed-right" aria-hidden="true"></div><div class="fullscreen q-drawer__backdrop hidden" aria-hidden="true" style="background-color: rgba(0, 0, 0, 0);"></div><aside class="q-drawer q-drawer--right q-drawer--bordered q-drawer--dark q-dark q-layout--prevent-focus fixed q-drawer--on-top q-drawer--mobile q-drawer--top-padding" style="width: 300px; transform: translateX(300px);"><div class="q-drawer__content fit scroll column"><div class="q-scrollarea q-scrollarea--dark" style="flex-grow: 1;"><div class="q-scrollarea__container scroll relative-position fit hide-scrollbar"><div class="q-scrollarea__content absolute"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">notifications</i></div><div class="q-item__section column q-item__section--main justify-center">Nouvelles</div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_88796267-9fff-4734-93b3-7bc489ccefc9" aria-label="Développer &quot;Des produits&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled " aria-hidden="true" role="presentation">box</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Des produits</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_88796267-9fff-4734-93b3-7bc489ccefc9" style="display: none;"><div class="q-list q-list--dark" role="list" label="Des produits"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Pour les entreprises</span><div><div class="q-chip row inline no-wrap items-center q-chip--dense q-chip--outline q-chip--square q-chip--dark q-dark cursor-pointer" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">⇧1</div></div></div><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--actionable q-focusable q-hoverable q-btn--no-uppercase q-btn--dense" tabindex="0" type="button" style="font-size: 8px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><span class="block">Maximiser</span></span></button></div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/embedding-DzEuY8_E.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Intégrations</div><div class="q-item__label q-item__label--caption text-caption">Intégrations multimodales et multilingues de classe mondiale.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reranker-DudpN0Ck.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Reclasseur</div><div class="q-item__label q-item__label--caption text-caption">Récupérateur neuronal de classe mondiale pour maximiser la pertinence de la recherche.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/assets/reader-D06QTWF1.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Lecteur</div><div class="q-item__label q-item__label--caption text-caption">Lisez les URL et effectuez des recherches sur le Web pour de meilleurs LLM de base.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20width='240'%20height='240'%20viewBox='0%200%20240%20240'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20184.388L184.388%20152.304H152.304V184.388ZM146.922%20190.885V149.613C146.922%20148.127%20148.127%20146.922%20149.613%20146.922H190.886C193.283%20146.922%20194.484%20149.821%20192.789%20151.516L151.516%20192.788C149.821%20194.484%20146.922%20193.283%20146.922%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M152.304%20133.927L184.388%20101.843H152.304V133.927ZM146.922%20140.424V99.1521C146.922%2097.6657%20148.127%2096.4608%20149.613%2096.4608H190.886C193.283%2096.4608%20194.484%2099.3597%20192.789%20101.055L151.516%20142.327C149.821%20144.023%20146.922%20142.822%20146.922%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20184.806L83.4668%20152.722H51.3828V184.806ZM46.0003%20191.303V150.031C46.0003%20148.545%2047.2053%20147.34%2048.6916%20147.34H89.964C92.3616%20147.34%2093.5624%20150.239%2091.867%20151.934L50.5946%20193.206C48.8992%20194.902%2046.0003%20193.701%2046.0003%20191.303Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20184.388L133.927%20152.304H101.843V184.388ZM96.4608%20190.885V149.613C96.4608%20148.127%2097.6657%20146.922%2099.152%20146.922H140.424C142.822%20146.922%20144.023%20149.821%20142.327%20151.516L101.055%20192.788C99.3597%20194.484%2096.4608%20193.283%2096.4608%20190.885Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%20133.927L133.927%20101.843H101.843V133.927ZM96.4608%20140.424V99.1521C96.4608%2097.6657%2097.6657%2096.4608%2099.152%2096.4608H140.424C142.822%2096.4608%20144.023%2099.3597%20142.327%20101.055L101.055%20142.327C99.3597%20144.023%2096.4608%20142.822%2096.4608%20140.424Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M101.843%2083.4664L133.927%2051.3825H101.843V83.4664ZM96.4608%2089.9637V48.6913C96.4608%2047.2049%2097.6657%2046%2099.152%2046H140.424C142.822%2046%20144.023%2048.8989%20142.327%2050.5943L101.055%2091.8667C99.3597%2093.5621%2096.4608%2092.3613%2096.4608%2089.9637Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3828%20132.808L83.4668%20100.725H51.3828V132.808ZM46.0003%20139.306V98.0333C46.0003%2096.547%2047.2053%2095.3421%2048.6916%2095.3421H89.964C92.3616%2095.3421%2093.5624%2098.2409%2091.867%2099.9363L50.5946%20141.209C48.8992%20142.904%2046.0003%20141.703%2046.0003%20139.306Z'%20fill='white'/%3e%3cpath%20d='M190.891%2046H149.619C147.221%2046%20146.02%2048.8989%20147.716%2050.5943L188.988%2091.8667C190.683%2093.5621%20193.582%2092.3613%20193.582%2089.9637V48.6913C193.582%2047.2049%20192.377%2046%20190.891%2046Z'%20fill='white'/%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M51.3826%2083.4664L83.4665%2051.3825H51.3826V83.4664ZM46.0001%2089.9637V48.6913C46.0001%2047.2049%2047.205%2046%2048.6914%2046H89.9638C92.3614%2046%2093.5621%2048.8989%2091.8668%2050.5943L50.5944%2091.8667C48.899%2093.5621%2046.0001%2092.3613%2046.0001%2089.9637Z'%20fill='white'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Classificateur</div><div class="q-item__label q-item__label--caption text-caption">Classification à zéro plan et à quelques plans pour l'image et le texte.</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 100%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20xmlns:xlink='http://www.w3.org/1999/xlink'%20width='320'%20zoomAndPan='magnify'%20viewBox='0%200%20240%20239.999995'%20height='320'%20preserveAspectRatio='xMidYMid%20meet'%20version='1.0'%3e%3cpath%20fill='%23ffffff'%20d='M%20132.328125%2039%20L%20144.652344%2060.351562%20L%20132.328125%2081.699219%20L%20107.675781%2081.699219%20L%2095.347656%2060.351562%20L%20107.675781%2039%20Z%20M%20184.96875%2058.523438%20L%20202%2088.023438%20L%20184.96875%20117.527344%20L%20153.011719%20117.527344%20L%20138.085938%20143.375%20L%20154.066406%20171.050781%20L%20137.03125%20200.554688%20L%20102.964844%20200.554688%20L%2085.933594%20171.050781%20L%20101.910156%20143.375%20L%2086.988281%20117.527344%20L%2055.03125%20117.527344%20L%2038%2088.027344%20L%2055.03125%2058.523438%20L%2089.097656%2058.523438%20L%20105.074219%2086.199219%20L%20134.921875%2086.199219%20L%20150.902344%2058.523438%20Z%20M%2057.140625%20113.875%20L%2086.988281%20113.875%20L%20101.914062%2088.023438%20L%2086.988281%2062.175781%20L%2057.140625%2062.175781%20L%2042.21875%2088.027344%20Z%20M%20105.074219%20141.550781%20L%2090.152344%20115.703125%20L%20105.078125%2089.851562%20L%20134.921875%2089.851562%20L%20149.847656%20115.699219%20L%20134.925781%20141.550781%20Z%20M%20138.085938%2088.023438%20L%20153.011719%2062.175781%20L%20182.859375%2062.175781%20L%20197.78125%2088.023438%20L%20182.859375%20113.875%20L%20153.011719%20113.875%20Z%20M%20105.074219%20145.203125%20L%2090.152344%20171.050781%20L%20105.074219%20196.902344%20L%20134.921875%20196.902344%20L%20149.847656%20171.050781%20L%20134.921875%20145.203125%20Z%20M%2096.71875%20143.375%20L%2084.390625%20122.027344%20L%2059.738281%20122.027344%20L%2047.414062%20143.375%20L%2059.738281%20164.726562%20L%2084.390625%20164.726562%20Z%20M%20192.585938%20143.375%20L%20180.261719%20122.023438%20L%20155.605469%20122.023438%20L%20143.28125%20143.375%20L%20155.605469%20164.726562%20L%20180.261719%20164.726562%20Z%20M%20192.585938%20143.375%20'%20fill-opacity='1'%20fill-rule='evenodd'/%3e%3c/svg%3e" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Segmenteur</div><div class="q-item__label q-item__label--caption text-caption">Coupez un long texte en morceaux et effectuez la tokenisation.</div></div></a><hr class="q-separator q-separator--horizontal q-separator--dark" aria-orientation="horizontal"><div class="q-item__label q-item__label--header row justify-between items-center q-pa-sm"><span class="q-pl-sm">Pour les utilisateurs expérimentés</span></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://promptperfect.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://promptperfect.jina.ai/PromptPerfect-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">PromptPerfect</div><div class="q-item__label q-item__label--caption text-caption">Premier outil pour une ingénierie rapide</div></div></a><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_daea531c-091f-435a-b9ee-a60684c1ebc3" aria-label="Développer &quot;Plus d'outils pour les utilisateurs expérimentés&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Plus d'outils pour les utilisateurs expérimentés</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_daea531c-091f-435a-b9ee-a60684c1ebc3" style="display: none;"><div class="q-list q-list--dark" role="list"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://scenex.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://scenex.jina.ai/SceneX - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">SceneXplain</div><div class="q-item__label q-item__label--caption text-caption">Solution d'IA leader pour les légendes d'images et les résumés vidéo</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://bestbanner.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://bestbanner.jina.ai/bestbanner-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">BestBanner</div><div class="q-item__label q-item__label--caption text-caption">Du blog à la bannière, sans les invites&nbsp;!</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://chat.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://chat.jina.ai/JinaChat - Light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">JinaChat</div><div class="q-item__label q-item__label--caption text-caption">Plus de modalité, plus de mémoire, moins de coût</div></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://rationale.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://rationale.jina.ai/Rationale-light.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Rationale</div><div class="q-item__label q-item__label--caption text-caption">Outils d'aide à la décision IA ultimes</div></div></a></div></div></div></div></div></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div><div class="q-expansion-item q-item-type q-expansion-item--collapsed q-expansion-item--standard"><div class="q-expansion-item__container relative-position"><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="button" tabindex="0" aria-expanded="false" aria-controls="f_20dede52-a1a8-4a5e-90bd-5373a9686e22" aria-label="Développer &quot;Entreprise&quot;"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon" aria-hidden="true" role="presentation"><img src="/J.svg"></i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Entreprise</div></div><div class="q-item__section column q-item__section--side justify-center q-focusable relative-position cursor-pointer"><i class="q-icon notranslate material-symbols material-symbols-sharp q-expansion-item__toggle-icon" aria-hidden="true" role="presentation">keyboard_arrow_down</i></div></div><div class="q-expansion-item__content relative-position" id="f_20dede52-a1a8-4a5e-90bd-5373a9686e22" style="display: none;"><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">À propos de nous</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contacter le service commercial</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programme de stage</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Rejoignez-nous</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Télécharger le logo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-md" role="listitem" tabindex="0" href="/legal"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">termes et conditions</div></a></div><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--top absolute-top" aria-orientation="horizontal"><hr class="q-separator q-separator--horizontal q-separator--dark q-expansion-item__border q-expansion-item__border--bottom absolute-bottom" aria-orientation="horizontal"></div></div></div></div></div><div class="q-scrollarea__bar q-scrollarea__bar--v absolute-right q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__bar q-scrollarea__bar--h absolute-bottom q-scrollarea__bar--invisible" aria-hidden="true"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--v absolute-right q-scrollarea__thumb--invisible" aria-hidden="true" style="top: 0px; height: 600px; right: 0px;"></div><div class="q-scrollarea__thumb q-scrollarea__thumb--h absolute-bottom q-scrollarea__thumb--invisible" aria-hidden="true" style="opacity: 0; left: 0px; width: 299px; bottom: 0px;"></div></div></div></aside></div><div class="q-page-container" style="padding-top: 56px;"><main data-v-1d9869b0="" class="q-page" style="min-height: 100vh;"><div data-v-1d9869b0="" class="row full-width relative-position justify-end"><div data-v-1d9869b0="" class="fixed-left q-pl-md" style="width: 300px; top: 100px; z-index: 1; display: none;"><div data-v-1d9869b0="" class="q-list q-list--dark q-mx-sm" role="list"><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">tl;dr</div></div></div><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">Mots, Tokens, Nombres</div></div></div><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">Mapping du langage vers les nombres</div></div></div><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">Pourquoi tokeniser ? Et pourquoi de cette manière ?</div></div></div><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">Estimations empiriques des tailles de sortie des tokens</div></div></div><div data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable relative-position q-pa-md text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div data-v-1d9869b0="" class="q-item__section column q-item__section--main justify-center q-ml-md"><div data-v-1d9869b0="" class="q-item__label">Prendre les tokens au sérieux</div></div></div></div></div><div data-v-1d9869b0="" class="col-12 col-md-10 col-lg-12"><div data-v-1d9869b0="" class="row justify-center q-pt-xl q-mt-xl"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog technique</div></div></div></div><div data-v-1d9869b0="" class="row justify-center"><div data-v-1d9869b0="" class="col-11 col-sm-9 cold-md-7 col-lg-6 column items-center q-pt-md q-mt-md q-gutter-y-xl"><div data-v-1d9869b0="" class="q-item__label q-item__label--caption text-caption text-white q-mt-sm text-center q-pt-xl q-mt-xl">janvier 31, 2024</div><h1 data-v-1d9869b0="" class="text-weight-medium text-center q-px-md my-title">Une analyse approfondie de la tokenisation</h1><div data-v-1d9869b0="" class="col row justify-center"><div data-v-1d9869b0="" class="q-item__label q-item__label--caption text-caption col-8 col-sm-7 col-md-6 text-center text-dim" style="font-size: 1rem;">La tokenisation, dans les LLMs, signifie découper les textes d'entrée en plus petites parties pour leur traitement. Alors pourquoi les embeddings sont-ils facturés au token ?</div></div><div data-v-1d9869b0="" class="q-card q-card--dark q-dark q-card--flat no-shadow" style="width: 100%;"><div data-v-1d9869b0="" class="q-img q-img--menu" role="img" aria-label="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect."><div style="padding-bottom: 52.5%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Colorful speckled grid pattern with a mix of small multicolored dots on a black background, creating a mosaic effect." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Untitled-design--25-.png" style="object-fit: contain; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-1d9869b0="" class="row justify-center"><div data-v-1d9869b0="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-py-md"><div data-v-1d9869b0="" class="col row justify-start items-center q-gutter-sm text-overline"><div data-v-61d959b7="" data-v-1d9869b0="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Scott Martens"><div style="padding-bottom: 118.041%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" alt="Scott Martens" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/photo-of-me-cropped.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></div></div><div data-v-1d9869b0="" class="q-item__label">Scott Martens • 16 minutes lues</div></div></div></div><div data-v-1d9869b0="" class="row justify-center"><div data-v-1d9869b0="" class="col-10 col-sm-9 cold-md-7 col-lg-6 q-mb-xl q-pb-xl"><article data-v-1d9869b0="" class="article"><section data-v-1d9869b0="" class="gh-content"><p>Il existe de nombreux obstacles à la compréhension des modèles d'IA, certains étant assez importants, et ils peuvent entraver la mise en œuvre des processus d'IA. Mais le premier que beaucoup rencontrent est de comprendre ce que nous entendons par <strong>tokens</strong>.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://jina.ai/tokenizer"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Tokenizer API</div><div class="kg-bookmark-description">Free API to tokenize texts, count and get first/last-N tokens.</div><div class="kg-bookmark-metadata"><img loading="lazy" class="kg-bookmark-icon" src="https://jina.ai/icons/favicon-128x128.png" alt="" style="cursor: help;"></div></div><div class="kg-bookmark-thumbnail"><img loading="lazy" src="https://jina.ai/banner-tokenize-api.png" alt="" style="cursor: help;"></div></a></figure><p>L'un des paramètres pratiques les plus importants dans le choix d'un modèle de langage IA est la taille de sa fenêtre contextuelle — la taille maximale du texte d'entrée — qui est donnée en tokens, et non en mots ou en caractères ou toute autre unité automatiquement reconnaissable.</p><p>De plus, les services d'embedding sont généralement facturés « par token », ce qui signifie que les tokens sont importants pour comprendre votre facture.</p><p>Cela peut être très déroutant si vous ne savez pas clairement ce qu'est un token.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png" class="kg-image" alt="Grille tarifaire actuelle de Jina Embeddings (février 2024)." width="2000" height="1036" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2024/01/Screenshot-2024-01-31-at-15.13.41.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2024/01/Screenshot-2024-01-31-at-15.13.41.png 2000w" sizes="(min-width: 720px) 720px" style="cursor: help;"><figcaption><span style="white-space: pre-wrap;">Grille tarifaire actuelle de Jina Embeddings (février 2024). Notez que les prix sont indiqués par « 1M tokens ».</span></figcaption></figure><p>Mais de tous les aspects déroutants de l'IA moderne, les tokens sont probablement les moins compliqués. Cet article tentera de clarifier ce qu'est la tokenisation, ce qu'elle fait et pourquoi nous procédons ainsi.</p><h2 id="tldr" style="position: relative;"><a href="#tldr" title="tl;dr" id="anchor-tldr"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>tl;dr</h2><p>Pour ceux qui veulent ou ont besoin d'une réponse rapide pour calculer combien de tokens acheter auprès de Jina Embeddings ou estimer combien ils devront en acheter, voici les statistiques que vous recherchez.</p><h3 id="tokens-per-english-word" style="position: relative;"><a href="#tokens-per-english-word" title="Tokens par mot en anglais" id="anchor-tokens-per-english-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens par mot en anglais</h3><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Un appel à l'API Jina Embeddings v2 pour les modèles anglais utilisera <b><strong style="white-space: pre-wrap;">environ</strong></b> <b><strong style="white-space: pre-wrap;">10% de tokens en plus</strong></b> que le nombre de mots dans votre texte, <b><strong style="white-space: pre-wrap;">plus deux tokens par embedding</strong></b>.</div></div><p>Lors des tests empiriques, décrits plus loin dans cet article, divers textes anglais ont été convertis en tokens à un taux d'environ 10% de tokens de plus que de mots, en utilisant les modèles anglais uniquement de Jina Embeddings. Ce résultat était assez robuste.</p><p>Les modèles Jina Embeddings v2 ont une fenêtre contextuelle de 8192 tokens. Cela signifie que si vous passez à un modèle Jina un texte anglais de plus de 7 400 mots, il y a de fortes chances qu'il soit tronqué.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">La taille maximale d'entrée pour <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 pour l'anglais</strong></b> est d'environ <b><strong style="white-space: pre-wrap;">7 400 mots</strong></b>.</div></div><h3 id="tokens-per-chinese-character" style="position: relative;"><a href="#tokens-per-chinese-character" title="Tokens par caractère chinois" id="anchor-tokens-per-chinese-character"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens par caractère chinois</h3><p>Pour le chinois, les résultats sont plus variables. Selon le type de texte, les ratios varient de 0,6 à 0,75 tokens par caractère chinois (汉字). Les textes anglais donnés à Jina Embeddings v2 pour le chinois produisent environ le même nombre de tokens que Jina Embeddings v2 pour l'anglais : environ 10% de plus que le nombre de mots.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">La taille maximale d'entrée en chinois pour <b><strong style="white-space: pre-wrap;">Jina Embeddings v2 pour le chinois et l'anglais</strong></b> est d'environ <b><strong style="white-space: pre-wrap;">10 500 caractères</strong></b> (<b><strong style="white-space: pre-wrap;">字数</strong></b>), soit <b><strong style="white-space: pre-wrap;">0,6 à 0,75 tokens par caractère chinois, plus deux par embedding.</strong></b></div></div><h3 id="tokens-per-german-word" style="position: relative;"><a href="#tokens-per-german-word" title="Tokens par mot en allemand" id="anchor-tokens-per-german-word"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Tokens par mot en allemand</h3><p>Les ratios mots-tokens en allemand sont plus variables qu'en anglais mais moins qu'en chinois. Selon le genre du texte, j'ai obtenu en moyenne 20% à 30% de tokens de plus que de mots. Donner des textes anglais à Jina Embeddings v2 pour l'allemand et l'anglais utilise un peu plus de tokens que les modèles anglais uniquement et chinois/anglais : 12% à 15% de tokens de plus que de mots.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Jina Embeddings v2 pour l'allemand et l'anglais comptera <b><strong style="white-space: pre-wrap;">20% à 30% de tokens de plus que de mots, plus deux par embedding</strong></b>. La taille maximale du contexte d'entrée est d'environ <b><strong style="white-space: pre-wrap;">6 300 mots allemands</strong></b>.</div></div><h3 id="caution" style="position: relative;"><a href="#caution" title="Attention !" id="anchor-caution"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Attention !</h3><p>Ce sont des calculs simples, mais ils devraient être approximativement corrects pour la plupart des textes en langage naturel et la plupart des utilisateurs. En fin de compte, nous pouvons seulement promettre que le nombre de tokens sera toujours inférieur ou égal au nombre de caractères dans votre texte, plus deux. Il sera pratiquement toujours beaucoup moins que cela, mais nous ne pouvons pas promettre un compte spécifique à l'avance.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text"><b><strong style="white-space: pre-wrap;">Les résultats peuvent varier !</strong></b><br><br>Ce sont des estimations basées sur des calculs statistiquement naïfs. Nous ne garantissons pas le nombre de tokens que prendra une requête particulière.</div></div><p>Si vous avez seulement besoin de conseils sur le nombre de tokens à acheter pour Jina Embeddings, vous pouvez vous arrêter ici. D'autres modèles d'embedding, d'entreprises autres que Jina AI, peuvent ne pas avoir les mêmes ratios token-mot et token-caractère-chinois que les modèles Jina, mais ils ne seront généralement pas très différents dans l'ensemble.</p><p>Si vous voulez comprendre pourquoi, le reste de cet article est une plongée plus profonde dans la tokenisation pour les modèles de langage.</p><h2 id="words-tokens-numbers" style="position: relative;"><a href="#words-tokens-numbers" title="Mots, Tokens, Nombres" id="anchor-words-tokens-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mots, Tokens, Nombres</h2><p>La tokenisation fait partie du traitement du langage naturel depuis plus longtemps que l'existence des modèles d'IA modernes.</p><p>C'est un peu un cliché de dire que tout dans un ordinateur n'est que des nombres, mais c'est aussi majoritairement vrai. Le langage, cependant, n'est pas naturellement qu'un ensemble de nombres. Il peut s'agir de parole, composée d'ondes sonores, ou d'écriture, composée de marques sur du papier, ou même d'une image d'un texte imprimé ou d'une vidéo de quelqu'un utilisant la langue des signes. Mais la plupart du temps, quand nous parlons d'utiliser des ordinateurs pour traiter le langage naturel, nous parlons de textes composés de séquences de caractères : lettres (a, b, c, etc.), chiffres (0, 1, 2…), ponctuation et espaces, dans différentes langues et encodages textuels.</p><p>Les ingénieurs informatiques appellent cela des « chaînes de caractères ».</p><p>Les modèles de langage IA prennent des séquences de nombres comme entrée. Donc, vous pourriez écrire la phrase :</p><blockquote><em>What is today's weather in Berlin?</em></blockquote><p>Mais, après tokenisation, le modèle IA reçoit comme entrée :</p><pre><code class="language-python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>La tokenisation est le processus de conversion d'une chaîne d'entrée en une séquence spécifique de nombres que votre modèle IA peut comprendre.</p><p>Lorsque vous utilisez un modèle IA via une API web qui facture les utilisateurs par token, chaque requête est convertie en une séquence de nombres comme celle ci-dessus. Le nombre de tokens dans la requête est la longueur de cette séquence de nombres. Donc, demander à Jina Embeddings v2 pour l'anglais de vous donner un embedding pour "<em>What is today's weather in Berlin?</em>" vous coûtera 11 tokens car il a converti cette phrase en une séquence de 11 nombres avant de la passer au modèle IA.</p><p>Les modèles IA basés sur l'<a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">architecture Transformer</a> ont une <strong>fenêtre contextuelle</strong> de taille fixe dont la taille est mesurée en tokens. Parfois, on l'appelle « fenêtre d'entrée », « taille de contexte » ou « longueur de séquence » (particulièrement sur le <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard MTEB de Hugging Face</a>). Cela signifie la taille maximale de texte que le modèle peut voir à la fois.</p><p>Donc, si vous voulez utiliser un modèle d'embedding, c'est la taille d'entrée maximale autorisée.</p><p>Les modèles Jina Embeddings v2 ont tous une fenêtre contextuelle de 8 192 tokens. D'autres modèles auront des fenêtres contextuelles différentes (généralement plus petites). Cela signifie que quelle que soit la quantité de texte que vous y mettez, le tokenizer associé à ce modèle Jina Embeddings doit le convertir en pas plus de 8 192 tokens.</p><h2 id="mapping-language-to-numbers" style="position: relative;"><a href="#mapping-language-to-numbers" title="Mapping du langage vers les nombres" id="anchor-mapping-language-to-numbers"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Mapping du langage vers les nombres</h2><p>La façon la plus simple d'expliquer la logique des tokens est celle-ci :</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">Un token est un nombre qui représente une partie d'une chaîne de caractères.</div></div><p>Pour les modèles de langage naturel, la partie de la chaîne qu'un token représente est un mot, une partie d'un mot, ou un signe de ponctuation. Les espaces ne reçoivent généralement aucune représentation explicite dans la sortie du tokenizer.</p><p>La tokenisation fait partie d'un groupe de techniques de traitement du langage naturel appelées <a href="https://en.wikipedia.org/wiki/Text_segmentation"><em>segmentation de texte</em></a>, et le module qui effectue la tokenisation est appelé, très logiquement, un <strong>tokenizer</strong>.</p><p>Pour montrer comment fonctionne la tokenisation, nous allons tokeniser quelques phrases en utilisant le plus petit modèle Jina Embeddings v2 pour l'anglais : <code>jina-embeddings-v2-small-en</code>. L'autre modèle anglais uniquement de Jina Embeddings — <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a> — utilise le même tokenizer, donc il n'y a pas d'intérêt à télécharger des mégaoctets supplémentaires de modèle IA que nous n'utiliserons pas dans cet article.</p><p>Tout d'abord, installez le module <code>transformers</code> dans votre environnement Python ou notebook. Utilisez le</p>L'indicateur <code>-U</code> permet de s'assurer que vous passez à la dernière version, car ce modèle ne fonctionnera pas avec certaines versions plus anciennes :<p></p><pre><code class="language-bash hljs">pip install -U transformers
</code></pre><p>Ensuite, téléchargez <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noreferrer"><code>jina-embeddings-v2-small-en</code></a> en utilisant <code>AutoModel.from_pretrained</code> :</p><pre><code class="language-Python hljs"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-small-en'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
</code></pre><p>Pour tokeniser une chaîne de caractères, utilisez la méthode <code>encode</code> de l'objet membre <code>tokenizer</code> du modèle :</p><pre><code class="language-Python hljs">model.tokenizer.encode(<span class="hljs-string">"What is today's weather in Berlin?"</span>)
</code></pre><p>Le résultat est une liste de nombres :</p><pre><code class="language-Python hljs">[<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>]
</code></pre><p>Pour reconvertir ces nombres en chaînes de caractères, utilisez la méthode <code>convert_ids_to_tokens</code> de l'objet <code>tokenizer</code> :</p><pre><code class="language-Python hljs">model.tokenizer.convert_ids_to_tokens([<span class="hljs-number">101</span>, <span class="hljs-number">2054</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">2651</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">4633</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">4068</span>, <span class="hljs-number">1029</span>, <span class="hljs-number">102</span>])
</code></pre><p>Le résultat est une liste de chaînes :</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'what'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'today'</span>, <span class="hljs-string">"'"</span>, <span class="hljs-string">'s'</span>, <span class="hljs-string">'weather'</span>, <span class="hljs-string">'in'</span>,
 <span class="hljs-string">'berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Notez que le tokenizer du modèle a :</p><ol><li>Ajouté <code>[CLS]</code> au début et <code>[SEP]</code> à la fin. C'est nécessaire pour des raisons techniques et signifie que <strong>chaque demande d'embedding coûtera deux tokens supplémentaires</strong>, en plus du nombre de tokens que le texte nécessite.</li><li>Séparé la ponctuation des mots, transformant « <em>Berlin?</em> » en : <code>berlin</code> et <code>?</code>, et « <em>today's</em> » en <code>today</code>, <code>'</code>, et <code>s</code>.</li><li>Mis tout en minuscules. Tous les modèles ne le font pas, mais cela peut aider lors de l'entraînement en anglais. Cela peut être moins utile dans les langues où la capitalisation a une signification différente.</li></ol><p>Différents algorithmes de comptage de mots dans différents programmes peuvent compter les mots de cette phrase différemment. OpenOffice compte ceci comme six mots. L'algorithme de segmentation de texte Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) compte sept mots. D'autres logiciels peuvent arriver à d'autres nombres, selon la façon dont ils gèrent la ponctuation et les clitiques comme « 's ».</p><p>Le tokenizer de ce modèle produit neuf tokens pour ces six ou sept mots, plus les deux tokens supplémentaires nécessaires à chaque requête.</p><p>Maintenant, essayons avec un nom de lieu moins courant que Berlin :</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"I live in Kinshasa."</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>Le résultat :</p><pre><code class="language-Python hljs">[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'i'</span>, <span class="hljs-string">'live'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'kin'</span>, <span class="hljs-string">'##sha'</span>, <span class="hljs-string">'##sa'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Le nom « Kinshasa » est divisé en trois tokens : <code>kin</code>, <code>##sha</code>, et <code>##sa</code>. Le <code>##</code> indique que ce token n'est pas le début d'un mot.</p><p>Si nous donnons au tokenizer quelque chose de complètement étranger, le nombre de tokens par rapport au nombre de mots augmente encore plus :</p><pre><code class="language-Python hljs">token_ids = model.tokenizer.encode(<span class="hljs-string">"Klaatu barada nikto"</span>)
tokens = model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)

[<span class="hljs-string">'[CLS]'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'##la'</span>, <span class="hljs-string">'##at'</span>, <span class="hljs-string">'##u'</span>, <span class="hljs-string">'bar'</span>, <span class="hljs-string">'##ada'</span>, <span class="hljs-string">'nik'</span>, <span class="hljs-string">'##to'</span>, <span class="hljs-string">'[SEP]'</span>]
</code></pre><p>Trois mots deviennent huit tokens, plus les tokens <code>[CLS]</code> et <code>[SEP]</code>.</p><p>La tokenisation en allemand est similaire. Avec le modèle <a href="https://jina.ai/news/ich-bin-ein-berliner-german-english-bilingual-embeddings-with-8k-token-length/" rel="noreferrer">Jina Embeddings v2 pour l'allemand</a>, nous pouvons tokeniser une traduction de « What is today's weather in Berlin? » de la même manière qu'avec le modèle anglais.</p><pre><code class="language-Python hljs">german_model = AutoModel.from_pretrained(<span class="hljs-string">'jinaai/jina-embeddings-v2-base-de'</span>, trust_remote_code=<span class="hljs-literal">True</span>)
token_ids = german_model.tokenizer.encode(<span class="hljs-string">"Wie wird das Wetter heute in Berlin?"</span>)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>Le résultat :</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Wie'</span>, <span class="hljs-string">'wird'</span>, <span class="hljs-string">'das'</span>, <span class="hljs-string">'Wetter'</span>, <span class="hljs-string">'heute'</span>, <span class="hljs-string">'in'</span>, <span class="hljs-string">'Berlin'</span>, <span class="hljs-string">'?'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Ce tokenizer est légèrement différent de celui de l'anglais car <code>&lt;s&gt;</code> et <code>&lt;/s&gt;</code> remplacent <code>[CLS]</code> et <code>[SEP]</code> mais servent la même fonction. De plus, le texte n'est pas normalisé en minuscules — les majuscules et minuscules restent telles quelles — car la capitalisation a une signification différente en allemand par rapport à l'anglais.</p><p>(Pour simplifier cette présentation, j'ai supprimé un caractère spécial indiquant le début d'un mot.)</p><p>Maintenant, essayons une phrase plus complexe <a href="https://www.welt.de/politik/deutschland/plus249565102/Proteste-der-Landwirte-Die-Krux-mit-den-Foerdermitteln.html">tirée d'un texte de journal</a> :</p><blockquote>Ein Großteil der milliardenschweren Bauern-Subventionen bleibt liegen – zu genervt sind die Landwirte von bürokratischen Gängelungen und Regelwahn.</blockquote><pre><code class="hljs language-python">sentence = <span class="hljs-string">"""
Ein Großteil der milliardenschweren Bauern-Subventionen
bleibt liegen – zu genervt sind die Landwirte von 
bürokratischen Gängelungen und Regelwahn.
"""</span>
token_ids = german_model.tokenizer.encode(sentence)
tokens = german_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)</code></pre><p>Le résultat tokenisé :</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'Ein'</span>, <span class="hljs-string">'Großteil'</span>, <span class="hljs-string">'der'</span>, <span class="hljs-string">'mill'</span>, <span class="hljs-string">'iarden'</span>, <span class="hljs-string">'schwer'</span>, 
 <span class="hljs-string">'en'</span>, <span class="hljs-string">'Bauern'</span>, <span class="hljs-string">'-'</span>, <span class="hljs-string">'Sub'</span>, <span class="hljs-string">'ventionen'</span>, <span class="hljs-string">'bleibt'</span>, <span class="hljs-string">'liegen'</span>, 
 <span class="hljs-string">'–'</span>, <span class="hljs-string">'zu'</span>, <span class="hljs-string">'gen'</span>, <span class="hljs-string">'ervt'</span>, <span class="hljs-string">'sind'</span>, <span class="hljs-string">'die'</span>, <span class="hljs-string">'Landwirte'</span>, <span class="hljs-string">'von'</span>, 
 <span class="hljs-string">'büro'</span>, <span class="hljs-string">'krat'</span>, <span class="hljs-string">'ischen'</span>, <span class="hljs-string">'Gän'</span>, <span class="hljs-string">'gel'</span>, <span class="hljs-string">'ungen'</span>, <span class="hljs-string">'und'</span>, <span class="hljs-string">'Regel'</span>, 
 <span class="hljs-string">'wahn'</span>, <span class="hljs-string">'.'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Ici, vous voyez que de nombreux mots allemands ont été divisés en plus petits morceaux et pas nécessairement selon les règles grammaticales allemandes. En conséquence, un long mot allemand qui ne compterait que comme un seul mot pour un compteur de mots peut représenter un nombre quelconque de tokens pour le modèle AI de Jina.</p><p>Faisons de même en chinois, en traduisant « What is today's weather in Berlin? » par :</p><blockquote>柏林今天的天气怎么样？</blockquote><pre><code class="hljs language-makefile">chinese_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-zh', trust_remote_code=True)
token_ids = chinese_model.tokenizer.encode(<span class="hljs-string">"柏林今天的天气怎么样？"</span>)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
print(tokens)
</code></pre><p>Le résultat tokenisé :</p><pre><code class="language-Python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'柏林'</span>, <span class="hljs-string">'今天的'</span>, <span class="hljs-string">'天气'</span>, <span class="hljs-string">'怎么样'</span>, <span class="hljs-string">'？'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>En chinois, il n'y a généralement pas d'espaces entre les mots dans le texte écrit, mais le tokenizer de Jina Embeddings regroupe souvent plusieurs caractères chinois ensemble :</p>

<table>
<thead>
<tr>
<th>Token string</th>
<th>Pinyin</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏林</td>
<td>Bólín</td>
<td>Berlin</td>
</tr>
<tr>
<td>今天的</td>
<td>jīntiān de</td>
<td>today's</td>
</tr>
<tr>
<td>天气</td>
<td>tiānqì</td>
<td>weather</td>
</tr>
<tr>
<td>怎么样</td>
<td>zěnmeyàng</td>
<td>how</td>
</tr>
</tbody>
</table>

<p>Utilisons une phrase plus complexe <a href="https://news.mingpao.com/pns/%e6%b8%af%e8%81%9e/article/20240116/s00002/1705335848777/%e7%81%a3%e5%8d%80%e7%86%b1%e6%90%9c-%e7%a9%97%e5%9c%b0%e9%90%b5%e6%8e%a8%e6%89%8b%e6%a9%9f%e3%80%8c%e9%9d%9c%e9%9f%b3%e4%bb%a4%e3%80%8d-%e7%84%a1%e7%bd%b0%e5%89%87-%e5%b8%82%e6%b0%91%e6%9c%89%e7%a8%b1%e5%85%b7%e8%ad%a6%e7%a4%ba%e4%bd%9c%e7%94%a8-%e6%9c%89%e6%84%9f%e5%af%a6%e6%95%88%e4%b8%8d%e5%a4%a7">d'un journal de Hong Kong</a> :</p><pre><code class="language-Python hljs">sentence = <span class="hljs-string">"""
新規定執行首日，記者在下班高峰前的下午5時來到廣州地鐵3號線，
從繁忙的珠江新城站啟程，向機場北方向出發。
"""</span>
token_ids = chinese_model.tokenizer.encode(sentence)
tokens = chinese_model.tokenizer.convert_ids_to_tokens(token_ids)
<span class="hljs-built_in">print</span>(tokens)
</code></pre><p>(Traduction : <em>« Le premier jour de l'entrée en vigueur des nouvelles réglementations, ce journaliste est arrivé à la ligne 3 du métro de Guangzhou à 17h, pendant l'heure de pointe, en partant de la station Zhujiang New Town en direction de l'aéroport. »</em>)</p><p>Le résultat :</p><pre><code class="language-python hljs">[<span class="hljs-string">'&lt;s&gt;'</span>, <span class="hljs-string">'新'</span>, <span class="hljs-string">'規定'</span>, <span class="hljs-string">'執行'</span>, <span class="hljs-string">'首'</span>, <span class="hljs-string">'日'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'記者'</span>, <span class="hljs-string">'在下'</span>, <span class="hljs-string">'班'</span>, 
 <span class="hljs-string">'高峰'</span>, <span class="hljs-string">'前的'</span>, <span class="hljs-string">'下午'</span>, <span class="hljs-string">'5'</span>, <span class="hljs-string">'時'</span>, <span class="hljs-string">'來到'</span>, <span class="hljs-string">'廣州'</span>, <span class="hljs-string">'地'</span>, <span class="hljs-string">'鐵'</span>, <span class="hljs-string">'3'</span>, 
 <span class="hljs-string">'號'</span>, <span class="hljs-string">'線'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'從'</span>, <span class="hljs-string">'繁忙'</span>, <span class="hljs-string">'的'</span>, <span class="hljs-string">'珠江'</span>, <span class="hljs-string">'新城'</span>, <span class="hljs-string">'站'</span>, <span class="hljs-string">'啟'</span>, 
 <span class="hljs-string">'程'</span>, <span class="hljs-string">'，'</span>, <span class="hljs-string">'向'</span>, <span class="hljs-string">'機場'</span>, <span class="hljs-string">'北'</span>, <span class="hljs-string">'方向'</span>, <span class="hljs-string">'出發'</span>, <span class="hljs-string">'。'</span>, <span class="hljs-string">'&lt;/s&gt;'</span>]
</code></pre><p>Ces tokens ne correspondent à aucun dictionnaire spécifique de mots chinois (词典). Par exemple, "啟程" - <em>qǐchéng</em> (partir, se mettre en route) serait généralement catégorisé comme un seul mot mais est ici divisé en ses deux caractères constitutifs. De même, "在下班" serait normalement reconnu comme deux mots, avec la séparation entre "在" - <em>zài</em> (à, pendant) et "下班" - <em>xiàbān</em> (la fin de la journée de travail, heure de pointe), et non entre "在下" et "班" comme l'a fait le tokenizer ici.</p><p>Dans les trois langues, les endroits où le tokenizer découpe le texte ne sont pas directement liés aux endroits logiques où un lecteur humain les séparerait.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">💡</div><div class="kg-callout-text">L'algorithme de tokenization n'utilise pas un dictionnaire conventionnel tenant compte de la langue, donc son comportement ne correspond pas à la façon dont les humains comptent les mots.</div></div><p>Ce n'est pas une caractéristique spécifique des modèles Jina Embeddings. Cette approche de la tokenization est presque universelle dans le développement des modèles d'IA. Bien que deux modèles d'IA différents puissent ne pas avoir des tokenizers identiques, dans l'état actuel du développement, ils utiliseront pratiquement tous des tokenizers avec ce type de comportement.</p><p>La section suivante discutera de l'algorithme spécifique utilisé dans la tokenization et la logique qui le sous-tend.</p><h2 id="why-do-we-tokenize-and-why-this-way" style="position: relative;"><a href="#why-do-we-tokenize-and-why-this-way" title="Pourquoi tokeniser ? Et pourquoi de cette manière ?" id="anchor-why-do-we-tokenize-and-why-this-way"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Pourquoi tokeniser ? Et pourquoi de cette manière ?</h2><p>Les modèles de langage IA prennent en entrée des séquences de nombres qui représentent des séquences de texte, mais il se passe un peu plus de choses avant d'exécuter le réseau neuronal sous-jacent et de créer un embedding. Lorsqu'on lui présente une liste de nombres représentant de petites séquences de texte, le modèle recherche chaque nombre dans un dictionnaire interne qui stocke un vecteur unique pour chaque nombre. Il les combine ensuite, et cela devient l'entrée du réseau neuronal.</p><p>Cela signifie que le tokenizer <strong>doit</strong> être capable de convertir <strong><em>n'importe quel</em></strong> texte d'entrée que nous lui donnons en tokens qui apparaissent dans le dictionnaire de vecteurs de tokens du modèle. Si nous prenions nos tokens d'un dictionnaire conventionnel, la première fois que nous rencontrerions une faute d'orthographe ou un nom propre rare ou un mot étranger, le modèle entier s'arrêterait. Il ne pourrait pas traiter cette entrée.</p><p>En traitement du langage naturel, on appelle cela le problème du vocabulaire hors-vocabulaire (OOV), et il est omniprésent dans tous les types de textes et toutes les langues. Il existe quelques stratégies pour résoudre le problème OOV :</p><ol><li>L'ignorer. Remplacer tout ce qui n'est pas dans le dictionnaire par un token "inconnu".</li><li>Le contourner. Au lieu d'utiliser un dictionnaire qui associe des séquences de texte à des vecteurs, utiliser un qui associe des <em>caractères individuels</em> à des vecteurs. L'anglais n'utilise que 26 lettres la plupart du temps, donc cela doit être plus petit et plus robuste contre les problèmes OOV que n'importe quel dictionnaire.</li><li>Trouver des sous-séquences fréquentes dans le texte, les mettre dans le dictionnaire et utiliser des caractères (tokens d'une seule lettre) pour ce qui reste.</li></ol><p>La première stratégie signifie que beaucoup d'informations importantes sont perdues. Le modèle ne peut même pas apprendre des données qu'il a vues si elles prennent la forme de quelque chose qui n'est pas dans le dictionnaire. Beaucoup de choses dans le texte ordinaire ne sont tout simplement pas présentes dans les plus grands dictionnaires.</p><p>La deuxième stratégie est possible, et les chercheurs l'ont étudiée. Cependant, cela signifie que le modèle doit accepter beaucoup plus d'entrées et doit apprendre beaucoup plus. Cela signifie un modèle beaucoup plus grand et beaucoup plus de données d'entraînement pour un résultat qui ne s'est jamais avéré meilleur que la troisième stratégie.</p><p>Les modèles de langage IA mettent pratiquement tous en œuvre la troisième stratégie sous une forme ou une autre. La plupart utilisent une variante de l'<a href="https://huggingface.co/learn/nlp-course/chapter6/6">algorithme Wordpiece</a> <a href="https://ieeexplore.ieee.org/document/6289079">[Schuster et Nakajima 2012]</a> ou une technique similaire appelée <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-Pair Encoding</a> (BPE). [<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829">Gage 1994</a>, <a href="https://aclanthology.org/P16-1162/">Senrich et al. 2016</a>] Ces algorithmes sont <em>agnostiques au langage</em>. Cela signifie qu'ils fonctionnent de la même manière pour toutes les langues écrites sans aucune connaissance au-delà d'une liste complète des caractères possibles. Ils ont été conçus pour des modèles multilingues comme BERT de Google qui prennent n'importe quelle entrée du scraping d'Internet — des centaines de langues et des textes autres que le langage humain comme les programmes informatiques — afin qu'ils puissent être entraînés sans faire de linguistique compliquée.</p><p>Certaines recherches montrent des améliorations significatives en utilisant des tokenizers plus spécifiques et conscients de la langue. [<a href="https://aclanthology.org/2021.acl-long.243/">Rust et al. 2021</a>] Mais construire des tokenizers de cette manière prend du temps, de l'argent et de l'expertise. Mettre en œuvre une stratégie universelle comme BPE ou Wordpiece est beaucoup moins cher et plus facile.</p><p>Cependant, en conséquence, il n'y a aucun moyen de savoir combien de tokens représente un texte spécifique autre que de le faire passer par un tokenizer puis de compter le nombre de tokens qui en sortent. Parce que la plus petite sous-séquence possible d'un texte est une lettre, vous pouvez être sûr que le nombre de tokens ne sera pas plus grand que le nombre de caractères (moins les espaces) plus deux.</p><p>Pour obtenir une bonne estimation, nous devons soumettre beaucoup de texte à notre tokenizer et calculer empiriquement combien de tokens nous obtenons en moyenne, par rapport au nombre de mots ou de caractères que nous avons entrés. Dans la section suivante, nous ferons quelques mesures empiriques pas très systématiques pour tous les modèles Jina Embeddings v2 actuellement disponibles.</p><h2 id="empirical-estimates-of-token-output-sizes" style="position: relative;"><a href="#empirical-estimates-of-token-output-sizes" title="Estimations empiriques des tailles de sortie des tokens" id="anchor-empirical-estimates-of-token-output-sizes"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Estimations empiriques des tailles de sortie des tokens</h2><p>Pour l'anglais et l'allemand, j'ai utilisé l'algorithme de segmentation de texte Unicode (<a href="https://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>) pour obtenir le nombre de mots des textes. Cet algorithme est largement utilisé pour sélectionner des extraits de texte lorsque vous double-cliquez sur quelque chose. C'est ce qui se rapproche le plus d'un compteur de mots objectif universel.</p><p>J'ai installé la <a href="https://pypi.org/project/polyglot/">bibliothèque polyglot</a> en Python, qui implémente ce segmenteur de texte :</p><pre><code class="language-bash hljs">pip install -U polyglot
</code></pre><p>Pour obtenir le nombre de mots d'un texte, vous pouvez utiliser un code comme cet extrait :</p><pre><code class="language-python hljs"><span class="hljs-keyword">from</span> polyglot.text <span class="hljs-keyword">import</span> Text

txt = <span class="hljs-string">"What is today's weather in Berlin?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(Text(txt).words))
</code></pre><p>Le résultat devrait être <code>7</code>.</p><p>Pour obtenir un nombre de tokens, des segments du texte ont été passés aux tokenizers de divers modèles Jina Embeddings, comme décrit ci-dessous, et chaque fois, j'ai soustrait deux du nombre de tokens retournés.</p><div class="kg-card kg-callout-card kg-callout-card-blue"><div class="kg-callout-emoji">⚠️</div><div class="kg-callout-text">Les nombres de tokens listés ici <b><strong style="white-space: pre-wrap;">n'incluent pas</strong></b> les deux tokens supplémentaires au début et à la fin de chaque texte tokenisé.</div></div><h3 id="english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" style="position: relative;"><a href="#english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en" title="Anglais
(jina-embeddings-v2-small-en et jina-embeddings-v2-base-en)" id="anchor-english-jina-embeddings-v2-small-en-and-jina-embeddings-v2-base-en"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Anglais<br>(<code>jina-embeddings-v2-small-en</code> et <a class="dynamic-model-name" href="/?sui&amp;model=jina-embeddings-v2-base-en" target="_blank"><span class="dynamic-model-name-inner">jina-embeddings-v2-base-en</span></a>)</h3><p>Pour calculer les moyennes, j'ai téléchargé deux corpus de texte anglais depuis <a href="https://wortschatz.uni-leipzig.de/en" rel="noreferrer">Wortschatz Leipzig</a>, une collection de corpus librement téléchargeables dans plusieurs langues et configurations hébergée par l'Université de Leipzig :</p><ul><li>Un corpus d'un million de phrases de données d'actualités en anglais de 2020 (<code>eng_news_2020_1M</code>)</li><li>Un corpus d'un million de phrases de données de <a href="https://en.wikipedia.org/">Wikipédia en anglais</a> de 2016 (<code>eng_wikipedia_2016_1M</code>)</li></ul><p>Les deux peuvent être trouvés sur <a href="https://wortschatz.uni-leipzig.de/en/download/English">leur page de téléchargements en anglais</a>.</p><p>Pour la diversité, j'ai également téléchargé la <a href="https://www.gutenberg.org/ebooks/135">traduction Hapgood des <em>Misérables</em> de Victor Hugo</a> depuis Project Gutenberg, et une copie de la version King James de la Bible, traduite en anglais en 1611.</p><p>Pour chacun des quatre textes, j'ai compté les mots en utilisant le segmenteur Unicode implémenté dans <code>polyglot</code>, puis compté les tokens créés par <code>jina-embeddings-v2-small-en</code>, en soustrayant deux tokens pour chaque requête de tokenization. Les résultats sont les suivants :</p>

<table id="6f07d5d4-ca08-466e-92fc-e784a932e4d0" class="simple-table"><thead class="simple-table-header"><tr id="4b8c4003-8ef9-4ac5-8df3-ef7662ab4d3b"><th id="wvl`" class="simple-table-header-color simple-table-header">Texte</th><th id="|<X;" class="simple-table-header-color simple-table-header">Nombre de mots<br>(Segmenteur Unicode)<br></th><th id="GHal" class="simple-table-header-color simple-table-header">Nombre de tokens<br>(Jina Embeddings v2 <br>pour l'anglais)<br></th><th id="h]mu" class="simple-table-header-color simple-table-header">Ratio tokens/mots<br>(à 3 décimales)<br></th></tr></thead><tbody><tr id="7e9eda1b-54b6-40f3-be6f-b233f161e2b5"><td id="wvl`" class=""><code>eng_news_2020_1M</code></td><td id="|<X;" class="">22 825 712</td><td id="GHal" class="">25 270 581</td><td id="h]mu" class="">1,107</td></tr><tr id="a81dfe1d-9143-4306-9bf3-4891ca8fb019"><td id="wvl`" class=""><code>eng_wikipedia_2016_1M</code></td><td id="|<X;" class="">24 243 607</td><td id="GHal" class="">26 813 877</td><td id="h]mu" class="">1,106</td></tr><tr id="d2fff413-6e0d-4ab2-9626-4d618d99af91"><td id="wvl`" class=""><code>les_miserables_en</code></td><td id="|<X;" class="">688 911</td><td id="GHal" class="">764 121</td><td id="h]mu" class="">1,109</td></tr><tr id="eb304e43-4fd3-4e02-9993-13fb0307f544"><td id="wvl`" class=""><code>kjv_bible</code></td><td id="|<X;" class="">1 007 651</td><td id="GHal" class="">1 099 335</td><td id="h]mu" class="">1,091</td></tr></tbody></table>

<p>L'utilisation de chiffres précis ne signifie pas que c'est un résultat précis. Le fait que des documents de genres si différents aient tous entre 9 % et 11 % de tokens de plus que de mots indique que vous pouvez probablement vous attendre à environ 10 % de tokens de plus que de mots, selon le segmenteur Unicode. Les traitements de texte ne comptent souvent pas la ponctuation, alors que le segmenteur Unicode le fait, donc vous ne pouvez pas vous attendre à ce que les décomptes de mots des logiciels de bureautique correspondent nécessairement à ceci.</p><h3 id="german-jina-embeddings-v2-base-de" style="position: relative;"><a href="#german-jina-embeddings-v2-base-de" title="Allemand
(jina-embeddings-v2-base-de)" id="anchor-german-jina-embeddings-v2-base-de"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Allemand<br>(<code>jina-embeddings-v2-base-de</code>)</h3><p>Pour l'allemand, j'ai téléchargé trois corpus depuis la <a href="https://wortschatz.uni-leipzig.de/en/download/German">page allemande de Wortschatz Leipzig</a> :</p><ul><li><code>deu_mixed-typical_2011_1M</code> — Un million de phrases provenant d'un mélange équilibré de textes de différents genres, datant de 2011.</li><li><code>deu_newscrawl-public_2019_1M</code> — Un million de phrases de textes d'actualités de 2019.</li><li><code>deu_wikipedia_2021_1M</code> — Un million de phrases extraites de Wikipédia en allemand en 2021.</li></ul><p>Et pour la diversité, j'ai également téléchargé les <a href="https://deutschestextarchiv.de/search?q=Kapital&amp;in=metadata">trois volumes du <em>Capital</em> de Karl Marx</a> depuis les <a href="https://www.deutschestextarchiv.de/" rel="noreferrer">Archives de textes allemands</a>.</p><p>J'ai ensuite suivi la même procédure que pour l'anglais :</p>

<table id="ad695a91-f35b-4215-bd4d-5d1415bb9812" class="simple-table"><thead class="simple-table-header"><tr id="7786decb-f68d-433d-8f58-3861d0350027"><th id="UGp`" class="simple-table-header-color simple-table-header" style="width:234.2265625px">Texte</th><th id="|qln" class="simple-table-header-color simple-table-header">Nombre de mots<br>(Segmenteur Unicode)<br></th><th id="YXZX" class="simple-table-header-color simple-table-header">Nombre de tokens<br>(Jina Embeddings v2 <br>pour l'allemand et l'anglais)<br></th><th id="oEoQ" class="simple-table-header-color simple-table-header">Ratio tokens/mots<br>(à 3 décimales)<br></th></tr></thead><tbody><tr id="9cb48640-64db-4783-8bfe-c78412022a21"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_mixed-typical_2011_1M</code></td><td id="|qln" class="">7 924 024</td><td id="YXZX" class="">9 772 652</td><td id="oEoQ" class="">1,234</td></tr><tr id="32fee905-17dc-4c2c-a32d-5e6508b033bc"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_newscrawl-public_2019_1M</code></td><td id="|qln" class="">17 949 120</td><td id="YXZX" class="">21 711 555</td><td id="oEoQ" class="">1,210</td></tr><tr id="35d0c8c4-7912-4d61-829a-bb39b643aa1c"><td id="UGp`" class="" style="width:234.2265625px"><code>deu_wikipedia_2021_1M</code></td><td id="|qln" class="">17 999 482</td><td id="YXZX" class="">22 654 901</td><td id="oEoQ" class="">1,259</td></tr><tr id="19e10367-e070-4dcc-8cbe-cfc75c43e0f9"><td id="UGp`" class="" style="width:234.2265625px"><code>marx_kapital</code></td><td id="|qln" class="">784 336</td><td id="YXZX" class="">1 011 377</td><td id="oEoQ" class="">1,289</td></tr></tbody></table>

<p>Ces résultats ont une plus grande dispersion que le modèle anglais uniquement, mais suggèrent toujours que le texte allemand produira, en moyenne, 20 % à 30 % de tokens de plus que de mots.</p><p>Les textes anglais produisent plus de tokens avec le tokenizer allemand-anglais qu'avec celui uniquement anglais :</p>

<table id="c31b2079-e921-4e06-a24b-8ed60ae63d8d" class="simple-table"><thead class="simple-table-header"><tr id="fe722fdd-ab88-44b4-9f3b-43c62eb3ccb5"><th id="Nc<l" class="simple-table-header-color simple-table-header" style="width:187.78125px">Texte</th><th id="R@A^" class="simple-table-header-color simple-table-header">Nombre de mots<br>(Segmenteur Unicode)<br></th><th id="UUfl" class="simple-table-header-color simple-table-header">Nombre de tokens<br>(Jina Embeddings v2 <br>pour l'allemand et l'anglais)<br></th><th id="iTZS" class="simple-table-header-color simple-table-header">Ratio tokens/mots<br>(à 3 décimales)<br></th></tr></thead><tbody><tr id="3461fd8c-ca39-4670-8f0e-e38a4958464a"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_news_2020_1M</code></td><td id="R@A^" class="">24 243 607</td><td id="UUfl" class="">27 758 535</td><td id="iTZS" class="">1,145</td></tr><tr id="48770d4d-5855-4f5f-934f-5b2900aa56c3"><td id="Nc<l" class="" style="width:187.78125px"><code>eng_wikipedia_2016_1M</code></td><td id="R@A^" class="">22 825 712</td><td id="UUfl" class="">25 566 921</td><td id="iTZS" class="">1,120</td></tr></tbody></table>

<p>Vous devriez vous attendre à avoir besoin de 12 % à 15 % de tokens de plus que de mots pour intégrer des textes anglais avec le modèle bilingue allemand/anglais plutôt qu'avec celui uniquement anglais.</p><h3 id="chinese-jina-embeddings-v2-base-zh" style="position: relative;"><a href="#chinese-jina-embeddings-v2-base-zh" title="Chinois
(jina-embeddings-v2-base-zh)" id="anchor-chinese-jina-embeddings-v2-base-zh"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Chinois<br>(<code>jina-embeddings-v2-base-zh</code>)</h3><p>Le chinois est généralement écrit sans espaces et n'avait pas de notion traditionnelle de "mots" avant le XXe siècle. Par conséquent, la taille d'un texte chinois est généralement mesurée en caractères (<strong>字数</strong>). Donc, au lieu d'utiliser le segmenteur Unicode, j'ai mesuré la longueur des textes chinois en supprimant tous les espaces puis en obtenant simplement la longueur en caractères.</p><p>J'ai téléchargé trois corpus depuis la <a href="https://wortschatz.uni-leipzig.de/en/download/Chinese">page des corpus chinois de Wortschatz Leipzig</a> :</p><ul><li><code>zho_wikipedia_2018_1M</code> — Un million de phrases de Wikipédia en chinois, extraites en 2018.</li><li><code>zho_news_2007-2009_1M</code> — Un million de phrases de sources d'actualités chinoises, collectées de 2007 à 2009.</li><li><code>zho-trad_newscrawl_2011_1M</code> — Un million de phrases de sources d'actualités qui utilisent exclusivement des caractères chinois traditionnels (繁體字).</li></ul><p>De plus, pour plus de diversité, j'ai également utilisé <em>L'Histoire véritable d'Ah Q</em> (阿Q正傳), une nouvelle de Lu Xun (魯迅) écrite au début des années 1920. J'ai téléchargé la <a href="https://www.gutenberg.org/ebooks/25332">version en caractères traditionnels depuis Project Gutenberg</a>.</p>

<table id="dace0ca3-97c0-481e-98e2-d2724b7bbe66" class="simple-table"><thead class="simple-table-header"><tr id="adc6e6ff-8afd-4915-8884-0894546a13dc"><th id="bCvb" class="simple-table-header-color simple-table-header" style="width:223.6953125px">Texte</th><th id="CaUc" class="simple-table-header-color simple-table-header">Nombre de caractères<br>(字数)<br></th><th id="CQ{d" class="simple-table-header-color simple-table-header">Nombre de tokens<br>(Jina Embeddings v2 <br>pour le chinois et l'anglais)<br></th><th id="_};C" class="simple-table-header-color simple-table-header">Ratio tokens/caractères<br>(à 3 décimales)<br></th></tr></thead><tbody><tr id="e75154ce-a33e-4af1-a983-4c4213f93c0e"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_wikipedia_2018_1M</code></td><td id="CaUc" class="">45 116 182</td><td id="CQ{d" class="">29 193 028</td><td id="_};C" class="">0,647</td></tr><tr id="605560a8-5c77-4add-a3e4-4615779b571a"><td id="bCvb" class="" style="width:223.6953125px"><code>zho_news_2007-2009_1M</code></td><td id="CaUc" class="">44 295 314</td><td id="CQ{d" class="">28 108 090</td><td id="_};C" class="">0,635</td></tr><tr id="6e23944e-a480-4978-8550-a83404b218c4"><td id="bCvb" class="" style="width:223.6953125px"><code>zho-trad_newscrawl_2011_1M</code></td><td id="CaUc" class="">54 585 819</td><td id="CQ{d" class="">40 290 982</td><td id="_};C" class="">0,738</td></tr><tr id="50abbb96-06f7-4308-9c66-7c18f2a67721"><td id="bCvb" class="" style="width:223.6953125px"><code>Ah_Q</code></td><td id="CaUc" class="">41 268</td><td id="CQ{d" class="">25 346</td><td id="_};C" class="">0,614</td></tr></tbody></table>

<p>Cette variation dans les ratios tokens/caractères est inattendue, et particulièrement la valeur aberrante pour le corpus en caractères traditionnels mérite une enquête plus approfondie. Néanmoins, nous pouvons conclure que pour le chinois, vous devriez vous attendre à avoir besoin de <em>moins</em> de tokens qu'il n'y a de caractères dans votre texte. Selon votre contenu, vous pouvez vous attendre à avoir besoin de 25 % à 40 % de moins.</p><p>Les textes anglais dans Jina Embeddings v2 pour le chinois et l'anglais ont produit à peu près le même nombre de tokens que dans le modèle anglais uniquement :</p>

<table id="061e7c3f-d109-476d-85fb-db3b369e4f35" class="simple-table"><thead class="simple-table-header"><tr id="1200d074-3353-4815-ab66-a90e93ec349d"><th id="v\xv" class="simple-table-header-color simple-table-header" style="width:184.53125px">Text</th><th id="qlUV" class="simple-table-header-color simple-table-header" style="width:165.3125px">Word count<br>(Unicode Segmenter)<br></th><th id="=]?F" class="simple-table-header-color simple-table-header">Token count<br>(Jina Embeddings v2 for Chinese and English)<br></th><th id="<rlw" class="simple-table-header-color simple-table-header">Ratio of tokens to words<br>(to 3 decimal places)<br></th></tr></thead><tbody><tr id="2fe4e02d-94fd-4513-bfcb-7f85d66b6883"><td id="v\xv" class="" style="width:184.53125px"><code>eng_news_2020_1M</code></td><td id="qlUV" class="" style="width:165.3125px">24 243 607</td><td id="=]?F" class="">26 890 176</td><td id="<rlw" class="">1,109</td></tr><tr id="e7f937f4-b156-4f5d-9e0b-3041d07b1b20"><td id="v\xv" class="" style="width:184.53125px"><code>eng_wikipedia_2016_1M</code></td><td id="qlUV" class="" style="width:165.3125px">22 825 712</td><td id="=]?F" class="">25 060 352</td><td id="<rlw" class="">1,097</td></tr></tbody></table>

<h2 id="taking-tokens-seriously" style="position: relative;"><a href="#taking-tokens-seriously" title="Prendre les tokens au sérieux" id="anchor-taking-tokens-seriously"><i class="material-symbols-sharp header-hash" style="margin-right: 8px; cursor: pointer; font-size: 0.8em; opacity: 0; transition: opacity 0.2s ease 0s;">tag</i></a>Prendre les tokens au sérieux</h2><p>Les tokens sont une structure importante pour les modèles de langage d'IA, et la recherche se poursuit dans ce domaine.</p><p>L'un des domaines où les modèles d'IA se sont révélés révolutionnaires est la découverte qu'ils sont très robustes face aux données bruitées. Même si un modèle particulier n'utilise pas la stratégie de tokenisation optimale, si le réseau est assez grand, dispose de suffisamment de données et est correctement entraîné, il peut apprendre à faire la bonne chose à partir d'entrées imparfaites.</p><p>Par conséquent, on consacre beaucoup moins d'efforts à l'amélioration de la tokenisation que dans d'autres domaines, mais cela pourrait changer.</p><p>En tant qu'utilisateur d'embeddings, qui les achète via une <a href="https://jina.ai/embeddings/">API comme Jina Embeddings</a>, vous ne pouvez pas savoir précisément combien de tokens vous aurez besoin pour une tâche spécifique et devrez peut-être faire vos propres tests pour obtenir des chiffres solides. Mais les estimations fournies ici — environ 110 % du nombre de mots pour l'anglais, environ 125 % du nombre de mots pour l'allemand, et environ 70 % du nombre de caractères pour le chinois — devraient être suffisantes pour une budgétisation de base.</p></section></article><div data-v-1d9869b0="" class="row justify-between items-center q-py-md"><div data-v-1d9869b0=""><span data-v-1d9869b0="" class="text-weight-bold">Catégories:</span><span data-v-1d9869b0="" class="q-ml-md"><div class="q-chip row inline no-wrap items-center q-chip--outline q-chip--square q-chip--dark q-dark non-selectable no-border-radius" style="font-size: 10px;"><div class="q-chip__content col row no-wrap items-center q-anchor--skip"><div class="ellipsis">Blog technique</div></div></div></span></div><div data-v-1d9869b0=""><div data-v-1d9869b0="" class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square inline"><a data-v-1d9869b0="" href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with HackerNews. (opens in new window)"><button data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-hacker-news" aria-hidden="true" role="img"> </i></span></button></a><a data-v-1d9869b0="" href="https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with LinkedIn. (opens in new window)"><button data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></button></a><a data-v-1d9869b0="" href="https://twitter.com/intent/tweet?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Twitter. (opens in new window)"><button data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></button></a><a data-v-1d9869b0="" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Facebook. (opens in new window)"><button data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-facebook" aria-hidden="true" role="img"> </i></span></button></a><a data-v-1d9869b0="" href="https://reddit.com/submit?url=http%3A%2F%2F127.0.0.1%3A3000%2Ffr%2Fnews%2Fa-deep-dive-into-tokenization%2F" target="_blank" rel="nofollow noopener noreferrer" aria-label="Share this with Reddit. (opens in new window)"><button data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square text-white q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" type="button"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-reddit" aria-hidden="true" role="img"> </i></span></button></a><a data-v-1d9869b0="" class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable q-btn--square q-py-lg" tabindex="0" href="https://jina.ai/feed.rss" target="_blank"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="img">rss_feed</i></span></a></div></div></div><hr data-v-1d9869b0="" class="q-separator q-separator--horizontal q-separator--dark q-mt-xl" aria-orientation="horizontal"><div data-v-1d9869b0="" class="text-h5 q-my-xl">En savoir plus</div><a data-v-aa7e154f="" data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/watermarking-text-with-embedding-models-to-protect-against-content-theft"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">novembre 27, 2024 • 10 minutes lues</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Watermarking Text with Embedding Models to Protect Against Content Theft</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">You use our embedding models to do what? This might be the most "out-of-domain" applications of embeddings we learned at EMNLP 2024.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Han Xiao"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Han Xiao" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/10/Untitled-2.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/banner--1-.jpg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/meta-prompt-for-better-jina-api-integration-and-codegen"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">novembre 19, 2024 • 9 minutes lues</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Meta-Prompt for Better Jina API Integration and CodeGen</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Is Meta-Prompt the new norm for API specs? Feed it to LLMs and generate integration code that reliably integrates Jina's APIs, saving you from the usual trial-and-error process.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 26px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Digital transformation icons with arrows on a teal background indicate file conversion, with contrasting blue and grey accent" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/11/Heading--58-.png" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a><a data-v-aa7e154f="" data-v-1d9869b0="" class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable q-pa-none q-my-md" role="listitem" tabindex="0" href="/news/beyond-clip-how-jina-clip-advances-multimodal-search"><div class="q-focus-helper" tabindex="-1"></div><div data-v-aa7e154f="" class="q-card q-card--dark q-dark q-card--square no-border-radius q-card--flat no-shadow cursor-pointer q-hoverable non-selectable full-width card-details"><span data-v-aa7e154f="" class="q-focus-helper"></span><div data-v-aa7e154f="" class="q-card__section q-card__section--horiz row no-wrap row justify-between full-height q-pa-none"><div data-v-aa7e154f="" class="q-card__section q-card__section--vert col column justify-between q-px-sm"><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption">octobre 29, 2024 • 11 minutes lues</div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-aa7e154f="" class="q-item__section column q-item__section--main justify-center"><div data-v-aa7e154f="" class="q-item__label text-h6 text-weight-light q-mb-xl" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Beyond CLIP: How Jina-CLIP Advances Multimodal Search</div><div data-v-aa7e154f="" class="q-item__label q-item__label--caption text-caption text-dim" style="overflow: hidden; display: -webkit-box; -webkit-box-orient: vertical; -webkit-line-clamp: 2;">Learn how Jina-CLIP enhances OpenAI's CLIP with better retrieval accuracy and more diverse results through unified text-image embeddings.</div></div></div><div data-v-aa7e154f="" class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div data-v-61d959b7="" data-v-aa7e154f="" class="relative-position row items-center" style="height: 26px; width: 47px;"><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 0px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Bo Wang"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Bo Wang" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/11/4B483B29-E306-402B-8635-64866C458406.jpeg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div><div data-v-61d959b7="" class="q-avatar overlapping" style="font-size: 24px; left: 18px;"><div class="q-avatar__content row flex-center overflow-hidden"><div data-v-61d959b7="" class="q-img q-img--menu" role="img" aria-label="Alex C-G"><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Alex C-G" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2022/09/alex.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></div></div><div data-v-aa7e154f="" class="col-4 overflow-hidden"><div data-v-aa7e154f="" class="q-img q-img--menu hoverOnImage full-height" role="img" aria-label="Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere."><div style="padding-bottom: 56.2493%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--current" alt="Abstract digital landscape with wave-like green and pink dunes against a dark background, conveying a tranquil atmosphere." loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="https://jina-ai-gmbh.ghost.io/content/images/2024/10/clip.jpg" style="object-fit: cover; object-position: 50% 50%; cursor: help;"></div><div class="q-img__loading absolute-full flex flex-center"><svg class="q-spinner q-spinner-mat" width="1em" height="1em" viewBox="25 25 50 50"><circle class="path" cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="5" stroke-miterlimit="10"></circle></svg></div></div></div></div></div></a></div></div></div></div></main></div><div class="q-card q-card--dark q-dark q-card--flat no-shadow print-hide q-py-xl q-px-sm-sm q-px-xs-xs q-px-md-xl bg-dark-page q-gutter-y-xl q-mt-xl"><div class="q-card__section q-card__section--vert row q-gutter-y-xl q-pa-none"><div class="col-sm-12 col-md"><div class="q-list q-list--dark small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Des bureaux</div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Berlin, Allemagne (siège)</div><div class="q-item__label q-item__label--caption text-caption text-dim">Prinzessinnenstraße 19-20, 10969 Berlin, Allemagne</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Pékin, Chine</div><div class="q-item__label q-item__label--caption text-caption text-dim">Niveau 5, Bâtiment 6, No.48 Haidian West St. Pékin Haidian, Chine</div></div></div><div class="q-item q-item-type row no-wrap q-item--dark" role="listitem"><div class="q-item__section column q-item__section--side q-item__section--top justify-start"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 24px;">location_on</i></div><div class="q-item__section column q-item__section--main justify-center"><div class="q-item__label">Shenzhen, en Chine</div><div class="q-item__label q-item__label--caption text-caption text-dim">402, étage 4, Fu'an Technology Building, Shenzhen Nanshan, Chine</div></div></div></div></div><div class="col-sm-12 col-md row"><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Fondation Recherche</div><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/embeddings"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Intégrations</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reranker"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Reclasseur</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/reader"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Lecteur</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/classifier"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Classificateur</div></a><a class="q-item q-item-type row no-wrap q-item--dense q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/segmenter"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Segmenteur</div></a><div class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Obtenir la clé API Jina AI</div></div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales#rate-limit"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Limite de taux</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://status.jina.ai" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-pa-none"><svg class="q-spinner text-green-13 q-mr-xs" stroke="currentColor" width="1em" height="1em" viewBox="0 0 45 45" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd" transform="translate(1 1)" stroke-width="2"><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="1.5s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="1.5s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="1.5s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="6"><animate attributeName="r" begin="3s" dur="3s" values="6;22" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-opacity" begin="3s" dur="3s" values="1;0" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="stroke-width" begin="3s" dur="3s" values="2;0" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="22" cy="22" r="8"><animate attributeName="r" begin="0s" dur="1.5s" values="6;1;2;3;4;5;6" calcMode="linear" repeatCount="indefinite"></animate></circle></g></svg></div><div class="q-item__section column q-item__section--main justify-center">Statut de l'API</div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Entreprise</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/about-us"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">À propos de nous</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/contact-sales"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Contacter le service commercial</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/news"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Rédaction</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/internship"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Programme de stage</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="https://career.jina.ai/" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Rejoignez-nous</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/logo-Jina-1024.zip" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Télécharger le logo</div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><i class="q-icon notranslate material-symbols material-symbols-sharp" aria-hidden="true" role="presentation" style="font-size: 18px;">open_in_new</i></div></a></div><div class="q-list q-list--dense q-list--dark col small-font-on-mobile" role="list"><div class="q-item q-item-type row no-wrap q-item--dark text-uppercase" role="listitem">Termes</div><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/COMMERCIAL-LICENSE-TERMS.pdf" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Licence commerciale</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#security"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Sécurité</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#terms-and-conditions"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">termes et conditions</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="/legal/#privacy-policy"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Confidentialité</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable text-dim" role="listitem" tabindex="0" href="javascript:UC_UI.showSecondLayer();"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--main justify-center">Gérer les cookies</div></a><a class="q-item q-item-type row no-wrap q-item--dark q-item--clickable q-link cursor-pointer q-focusable q-hoverable" role="listitem" tabindex="0" href="https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva" target="_blank"><div class="q-focus-helper" tabindex="-1"></div><div class="q-item__section column q-item__section--side justify-center q-item__section--avatar"><div class="q-img q-img--menu soc-icon is-mobile" role="img"><div style="padding-bottom: 99.3377%;"></div><div class="q-img__container absolute-full"><img class="q-img__image q-img__image--with-transition q-img__image--loaded" loading="lazy" fetchpriority="auto" aria-hidden="true" draggable="false" src="/21972-312_SOC_NonCPA_Blk.svg" style="object-fit: cover; object-position: 50% 50%;"></div><div class="q-img__content absolute-full q-anchor--skip"></div></div></div></a></div></div></div><div class="q-card__section q-card__section--vert row q-gutter-y-xl items-center justify-center q-pa-none"><div class="q-btn-group row no-wrap q-btn-group--flat q-btn-group--square q-btn-group--stretch inline col-12 col-md"><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://x.com/jinaAI_" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-x-twitter" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://www.linkedin.com/company/jinaai/" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-linkedin" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://github.com/jina-ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-github" aria-hidden="true" role="img"> </i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://huggingface.co/jinaai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon" aria-hidden="true" role="img"><img src="/huggingface_logo.svg"></i></span></a><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="https://discord.jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-discord" aria-hidden="true" role="img"> </i></span></a><button class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" type="button" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon fab fa-weixin" aria-hidden="true" role="img"> </i></span></button><a class="q-btn q-btn-item non-selectable no-outline q-btn--flat q-btn--rectangle q-btn--square q-btn--actionable q-focusable q-hoverable no-border-radius self-stretch q-btn--square" tabindex="0" href="mailto:support@jina.ai" target="_blank" style="font-size: 10px;"><span class="q-focus-helper"></span><span class="q-btn__content text-center col items-center q-anchor--skip justify-center row"><i class="q-icon notranslate material-symbols material-symbols-sharp material-symbols-sharp-filled" aria-hidden="true" role="img">email</i></span></a></div><div class="row items-center justify-between q-gutter-x-sm col-12 col-md"><label class="q-field row no-wrap items-start q-field--outlined q-select q-field--auto-height q-select--without-input q-select--without-chips q-select--single q-field--square q-field--dense q-field--dark text-caption" for="f_8f265c78-fbcb-4a37-b789-191ead1f1166"><div class="q-field__inner relative-position col self-stretch"><div class="q-field__control relative-position row no-wrap" tabindex="-1"><div class="q-field__prepend q-field__marginal row no-wrap items-center"><i class="q-icon text-white notranslate material-symbols material-symbols-sharp q-px-sm q-py-none" aria-hidden="true" role="presentation" style="font-size: 18px;">language</i></div><div class="q-field__control-container col relative-position row no-wrap q-anchor--skip"><div class="q-field__native row items-center"><span></span><input class="q-select__focus-target" id="f_8f265c78-fbcb-4a37-b789-191ead1f1166" readonly="" tabindex="0" role="combobox" aria-readonly="false" aria-autocomplete="none" aria-expanded="false" aria-controls="f_8f265c78-fbcb-4a37-b789-191ead1f1166_lb" value=""></div></div><div class="q-field__append q-field__marginal row no-wrap items-center q-anchor--skip"><i class="q-icon notranslate material-symbols material-symbols-sharp q-select__dropdown-icon" aria-hidden="true" role="presentation">arrow_drop_down</i></div></div></div></label><div class="text-caption text-dim"> Jina AI GmbH © 2020-2024. </div></div></div></div></div></div><div id="q-notify" data-v-app=""><div class="q-notifications"><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-start"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-end"></div><div class="q-notifications__list q-notifications__list--top fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--bottom fixed column no-wrap items-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-start justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap items-end justify-center"></div><div class="q-notifications__list q-notifications__list--center fixed column no-wrap flex-center"></div></div></div></body></html>